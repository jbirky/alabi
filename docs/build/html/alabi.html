<!doctype html>
<html class="no-js" lang="en" data-content_root="./">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="prev" title="alabi" href="modules.html" />
        <link rel="canonical" href="https://alabi.jessicabirky.com/alabi.html" />

    <!-- Generated with Sphinx 7.3.7 and Furo 2025.07.19 -->
        <title>alabi package - alabi</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=a7f7f5d3" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --font-stack: Roboto Light, sans-serif;
  --font-stack--monospace: Courier, monospace;
  --color-background-secondary: #eff1f6;
  --color-inline-code-background: #eff1f6;
  --color-sidebar-item-background--hover: white;
  --color-brand-primary: #004080;
  --color-brand-content: #0059b3;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">alabi</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  <span class="sidebar-brand-text">alabi</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html#quickstart-example">Quickstart Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_reload.html">Saving and Reloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp_tutorial.html">GP Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc_tutorial.html">MCMC sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_bayesian_optimization.html">Bayesian Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="plot_demo_1d.html">Visualize Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_demo_2d.html">2D Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_line_fit.html">Fit a line to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_kl_divergence.html">KL Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_gaussian_nd.html">Test computational scaling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="modules.html">alabi</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of alabi</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">alabi package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/LICENSE">License</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/issues">Issues</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="_sources/alabi.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="alabi-package">
<h1>alabi package<a class="headerlink" href="#alabi-package" title="Link to this heading">¶</a></h1>
<section id="module-alabi.benchmarks">
<span id="alabi-benchmarks-module"></span><h2>alabi.benchmarks module<a class="headerlink" href="#module-alabi.benchmarks" title="Link to this heading">¶</a></h2>
<section id="benchmarks-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">benchmarks.py</span></code><a class="headerlink" href="#benchmarks-py" title="Link to this heading">¶</a></h3>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.benchmarks.random_gaussian_covariance">
<span class="sig-prename descclassname"><span class="pre">alabi.benchmarks.</span></span><span class="sig-name descname"><span class="pre">random_gaussian_covariance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_dims</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/benchmarks.html#random_gaussian_covariance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.benchmarks.random_gaussian_covariance" title="Link to this definition">¶</a></dt>
<dd><p>Generate a random positive definite covariance matrix.</p>
</dd></dl>

</section>
<section id="module-alabi.cache_utils">
<span id="alabi-cache-utils-module"></span><h2>alabi.cache_utils module<a class="headerlink" href="#module-alabi.cache_utils" title="Link to this heading">¶</a></h2>
<section id="cache-utils-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">cache_utils.py</span></code><a class="headerlink" href="#cache-utils-py" title="Link to this heading">¶</a></h3>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.cache_utils.load_model_cache">
<span class="sig-prename descclassname"><span class="pre">alabi.cache_utils.</span></span><span class="sig-name descname"><span class="pre">load_model_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">savedir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'surrogate_model.pkl'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/cache_utils.html#load_model_cache"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.cache_utils.load_model_cache" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.cache_utils.write_report_gp">
<span class="sig-prename descclassname"><span class="pre">alabi.cache_utils.</span></span><span class="sig-name descname"><span class="pre">write_report_gp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/cache_utils.html#write_report_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.cache_utils.write_report_gp" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.cache_utils.write_report_emcee">
<span class="sig-prename descclassname"><span class="pre">alabi.cache_utils.</span></span><span class="sig-name descname"><span class="pre">write_report_emcee</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/cache_utils.html#write_report_emcee"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.cache_utils.write_report_emcee" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.cache_utils.write_report_dynesty">
<span class="sig-prename descclassname"><span class="pre">alabi.cache_utils.</span></span><span class="sig-name descname"><span class="pre">write_report_dynesty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/cache_utils.html#write_report_dynesty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.cache_utils.write_report_dynesty" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-alabi.core">
<span id="alabi-core-module"></span><h2>alabi.core module<a class="headerlink" href="#module-alabi.core" title="Link to this heading">¶</a></h2>
<section id="core-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">core.py</span></code><a class="headerlink" href="#core-py" title="Link to this heading">¶</a></h3>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">alabi.core.</span></span><span class="sig-name descname"><span class="pre">SurrogateModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lnlike_fn=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta_scaler=MinMaxScaler()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_scaler=FunctionTransformer(func=&lt;function</span> <span class="pre">no_scale&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_func=&lt;function</span> <span class="pre">no_scale&gt;)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savedir='results/'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name='surrogate_model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ncore=128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_warnings=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Gaussian Process surrogate model for Bayesian inference and optimization.</p>
<p>A SurrogateModel uses a Gaussian Process to create a fast approximation of expensive
likelihood functions, enabling efficient Bayesian inference, parameter estimation,
and active learning. The model supports various active learning algorithms and 
scalers for handling different types of likelihood functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lnlike_fn</strong> – (<em>callable, required</em>)
Log-likelihood function that takes parameter array theta and returns scalar 
log-likelihood value. For Bayesian inference, this is your model’s log-likelihood.
Signature: lnlike_fn(theta) -&gt; float</p></li>
<li><p><strong>bounds</strong> – (<em>array-like, required</em>)
Prior bounds for each parameter. List/array of (min, max) tuples for each dimension.
Example: bounds = [(0, 1), (2, 3), (-1, 1)]</p></li>
<li><p><strong>param_names</strong> – (<em>array-like, optional</em>)
Names/labels for each parameter. If None, defaults to θ₀, θ₁, etc.
Length must match number of dimensions in bounds.</p></li>
<li><p><strong>theta_scaler</strong> – (<em>sklearn transformer, optional, default=MinMaxScaler()</em>)
Scaler for input parameters. Applied to theta values before GP training.
Common options: MinMaxScaler() (scale to [0,1]) or StandardScaler()</p></li>
<li><p><strong>y_scaler</strong> – (<em>sklearn transformer, optional, default=no_scaler</em>)
Scaler for output values (log-likelihoods). Options include:
- no_scaler: No scaling (default)
- minmax_scaler: Scale to [0,1] 
- nlog_scaler: Apply -log10(-y) transformation for negative log-likelihoods
- log_scaler: Apply log10(y) for positive values</p></li>
<li><p><strong>cache</strong> – (<em>bool, optional, default=True</em>)
Whether to cache the trained model to disk for reuse</p></li>
<li><p><strong>savedir</strong> – (<em>str, optional, default=”results/”</em>)
Directory for saving results, plots, and cached models</p></li>
<li><p><strong>model_name</strong> – (<em>str, optional, default=”surrogate_model”</em>)
Name prefix for cached model files</p></li>
<li><p><strong>verbose</strong> – (<em>bool, optional, default=True</em>)
Print progress information during training and inference</p></li>
<li><p><strong>ncore</strong> – (<em>int, optional, default=cpu_count()</em>)
Number of CPU cores to use for parallel computation</p></li>
<li><p><strong>ignore_warnings</strong> – (<em>bool, optional, default=True</em>)
Suppress sklearn and other package warnings</p></li>
<li><p><strong>random_seed</strong> – (<em>int, optional, default=None</em>)
Random seed for reproducible results</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.gp">
<span class="sig-name descname"><span class="pre">gp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">george.GP</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.gp" title="Link to this definition">¶</a></dt>
<dd><p>Trained Gaussian Process model</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.bounds" title="Link to this definition">¶</a></dt>
<dd><p>Original parameter bounds (unscaled)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel._bounds">
<span class="sig-name descname"><span class="pre">_bounds</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel._bounds" title="Link to this definition">¶</a></dt>
<dd><p>Scaled parameter bounds used for GP training</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel._theta">
<span class="sig-name descname"><span class="pre">_theta</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel._theta" title="Link to this definition">¶</a></dt>
<dd><p>Training parameter samples (scaled)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel._y">
<span class="sig-name descname"><span class="pre">_y</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel._y" title="Link to this definition">¶</a></dt>
<dd><p>Training likelihood values (scaled)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.ntrain">
<span class="sig-name descname"><span class="pre">ntrain</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.ntrain" title="Link to this definition">¶</a></dt>
<dd><p>Number of initial training samples</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.ndim">
<span class="sig-name descname"><span class="pre">ndim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.ndim" title="Link to this definition">¶</a></dt>
<dd><p>Number of parameters/dimensions</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.emcee_samples">
<span class="sig-name descname"><span class="pre">emcee_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.emcee_samples" title="Link to this definition">¶</a></dt>
<dd><p>MCMC samples from emcee (if run_emcee called)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.dynesty_samples">
<span class="sig-name descname"><span class="pre">dynesty_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">ndarray</span></em><a class="headerlink" href="#alabi.core.SurrogateModel.dynesty_samples" title="Link to this definition">¶</a></dt>
<dd><p>Nested sampling results from dynesty (if run_dynesty called)</p>
</dd></dl>

<p><strong>Examples</strong></p>
<p>Basic usage for Bayesian inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="c1"># Your model likelihood function</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>  <span class="c1"># 2D parameter space</span>
<span class="n">sm</span> <span class="o">=</span> <span class="n">SurrogateModel</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">init_samples</span><span class="p">(</span><span class="n">ntrain</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># Initial training data</span>
<span class="n">sm</span><span class="o">.</span><span class="n">init_gp</span><span class="p">()</span>  <span class="c1"># Initialize Gaussian Process</span>
<span class="n">sm</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>  <span class="c1"># Active learning</span>
<span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">()</span>  <span class="c1"># Bayesian inference</span>
</pre></div>
</div>
<p>For optimization problems:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sm</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;jones&quot;</span><span class="p">)</span>  <span class="c1"># Use Jones algorithm for optimization</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#alabi.core.SurrogateModel.init_samples" title="alabi.core.SurrogateModel.init_samples"><code class="xref py py-meth docutils literal notranslate"><span class="pre">init_samples()</span></code></a> : Initialize training data
<a class="reference internal" href="#alabi.core.SurrogateModel.init_gp" title="alabi.core.SurrogateModel.init_gp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">init_gp()</span></code></a> : Initialize Gaussian Process
<a class="reference internal" href="#alabi.core.SurrogateModel.active_train" title="alabi.core.SurrogateModel.active_train"><code class="xref py py-meth docutils literal notranslate"><span class="pre">active_train()</span></code></a> : Perform active learning
<a class="reference internal" href="#alabi.core.SurrogateModel.run_dynesty" title="alabi.core.SurrogateModel.run_dynesty"><code class="xref py py-meth docutils literal notranslate"><span class="pre">run_dynesty()</span></code></a> : Run nested sampling
<a class="reference internal" href="#alabi.core.SurrogateModel.run_emcee" title="alabi.core.SurrogateModel.run_emcee"><code class="xref py py-meth docutils literal notranslate"><span class="pre">run_emcee()</span></code></a> : Run MCMC sampling</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lnlike_fn=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta_scaler=MinMaxScaler()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_scaler=FunctionTransformer(func=&lt;function</span> <span class="pre">no_scale&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_func=&lt;function</span> <span class="pre">no_scale&gt;)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savedir='results/'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name='surrogate_model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ncore=128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_warnings=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.__init__" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.save" title="Link to this definition">¶</a></dt>
<dd><p>Pickle <code class="docutils literal notranslate"><span class="pre">SurrogateModel</span></code> object and write summary to a text file</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.theta">
<span class="sig-name descname"><span class="pre">theta</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.theta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.theta" title="Link to this definition">¶</a></dt>
<dd><p>Return unscaled training theta values</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.y">
<span class="sig-name descname"><span class="pre">y</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.y"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.y" title="Link to this definition">¶</a></dt>
<dd><p>Return unscaled training y values</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.init_train">
<span class="sig-name descname"><span class="pre">init_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'initial_training_sample.npz'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.init_train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.init_train" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nsample</strong> – (<em>int, optional</em>) 
Number of samples. Defaults to <code class="docutils literal notranslate"><span class="pre">nsample</span> <span class="pre">=</span> <span class="pre">50</span> <span class="pre">*</span> <span class="pre">self.ndim</span></code></p></li>
<li><p><strong>sampler</strong> – (<em>str, optional</em>) 
Sampling method. Defaults to <code class="docutils literal notranslate"><span class="pre">'sobol'</span></code>. 
See <code class="docutils literal notranslate"><span class="pre">utility.prior_sampler</span></code> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.load_train">
<span class="sig-name descname"><span class="pre">load_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.load_train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.load_train" title="Link to this definition">¶</a></dt>
<dd><p>Reload training samples from cache file and apply scalers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cache_file</strong> – (<em>str, required</em>) 
Name of cache file relative to savedir. Must be a .npz file containing ‘theta’ and ‘y’ arrays.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(<em>tuple</em>) 
Scaled training samples (_theta, _y) after loading from cache.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.init_samples">
<span class="sig-name descname"><span class="pre">init_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ntrain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ntest</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reload</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'initial_training_sample.npz'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'initial_test_sample.npz'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.init_samples"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.init_samples" title="Link to this definition">¶</a></dt>
<dd><p>Initialize training and test samples for the surrogate model.</p>
<p>Creates initial dataset by either loading cached samples or computing new ones
by evaluating the likelihood function at randomly sampled parameter values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ntrain</strong> – (<em>int, optional, default=100</em>)
Number of training samples to generate. Used only if not loading cached samples.</p></li>
<li><p><strong>ntest</strong> – (<em>int, optional, default=0</em>)
Number of test samples to generate. Currently unused.</p></li>
<li><p><strong>reload</strong> – (<em>bool, optional, default=False</em>)
Whether to attempt loading cached samples from previous runs.
If True, tries to load from default cache files first.</p></li>
<li><p><strong>sampler</strong> – (<em>str, optional, default=”uniform”</em>)
Sampling method for generating parameter values. Options:
- “uniform”: Uniform sampling within bounds (default)
- “sobol”: Low-discrepancy Sobol sequence sampling
- “lhs”: Latin hypercube sampling</p></li>
<li><p><strong>train_file</strong> – (<em>str, optional, default=”initial_training_sample.npz”</em>)
Filename for cached training samples relative to savedir.
Format: .npz file containing ‘theta’ and ‘y’ arrays.</p></li>
<li><p><strong>test_file</strong> – (<em>str, optional, default=”initial_test_sample.npz”</em>)
Filename for cached test samples relative to savedir. Currently unused.</p></li>
</ul>
</dd>
<dt class="field-even">Note<span class="colon">:</span></dt>
<dd class="field-even"><p></p></dd>
</dl>
<p>This method must be called before init_gp() to provide training data for the
Gaussian Process. The samples are automatically scaled using the configured
theta_scaler and y_scaler.</p>
<p>The method sets several important attributes:
- _theta, _y: Scaled training samples used by GP
- theta0, y0: Unscaled original training samples  
- ntrain: Number of training samples</p>
<dl class="field-list simple">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Basic usage with default uniform sampling:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_samples</span><span class="p">(</span><span class="n">ntrain</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
<p>Use Sobol sampling for better space coverage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_samples</span><span class="p">(</span><span class="n">ntrain</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;sobol&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Reload from cached file:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_samples</span><span class="p">(</span><span class="n">reload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.set_hyperparam_prior_bounds">
<span class="sig-name descname"><span class="pre">set_hyperparam_prior_bounds</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.set_hyperparam_prior_bounds"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.set_hyperparam_prior_bounds" title="Link to this definition">¶</a></dt>
<dd><p>Configure prior bounds for GP hyperparameters based on current training data.</p>
<dl class="simple">
<dt>By default ranges for parameters:</dt><dd><ul class="simple">
<li><p>mean: [mean(y) - std(y), mean(y) + std(y)]</p></li>
<li><p>amplitude: [0.1, 10]</p></li>
<li><p>white noise: [white_noise - 3, white_noise + 3]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.fit_gp">
<span class="sig-name descname"><span class="pre">fit_gp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">_theta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.fit_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.fit_gp" title="Link to this definition">¶</a></dt>
<dd><p>Fit Gaussian Process to training data with current hyperparameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>_theta</strong> – (<em>array, optional</em>) 
Scaled training parameter samples. If None, uses <code class="docutils literal notranslate"><span class="pre">self._theta</span></code>.</p></li>
<li><p><strong>_y</strong> – (<em>array, optional</em>) 
Scaled training output values. If None, uses <code class="docutils literal notranslate"><span class="pre">self._y</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(<em>tuple</em>) 
- gp: Fitted george.GP object
- timing: Time taken to fit the GP in seconds</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.opt_gp">
<span class="sig-name descname"><span class="pre">opt_gp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.opt_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.opt_gp" title="Link to this definition">¶</a></dt>
<dd><p>Optimize GP hyperparameters by maximizing the log marginal likelihood.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>(<em>tuple</em>) 
- op_gp: Optimized george.GP object with updated hyperparameters
- timing: Time taken for optimization in seconds</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.init_gp">
<span class="sig-name descname"><span class="pre">init_gp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ExpSquaredKernel'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_white_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">white_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_scale_rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[-2,</span> <span class="pre">2]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_opt_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'newton-cg'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_nopt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{'max_iter':</span> <span class="pre">50}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.init_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.init_gp" title="Link to this definition">¶</a></dt>
<dd><p>Initialize the Gaussian Process surrogate model with specified kernel and hyperparameters.</p>
<p>This function sets up a Gaussian Process (GP) using the george library with the specified 
kernel type and configuration. The GP is initialized with random scale lengths and then
fitted to the current training data.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> – <p>(<em>str or george kernel object, optional</em>) 
Kernel type for the Gaussian Process. Can be either a string specifying one of the 
built-in kernels or a george kernel object. Default is “ExpSquaredKernel”.</p>
<dl class="simple">
<dt>Built-in options:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'ExpSquaredKernel'</span></code>: Squared exponential (RBF) kernel, smooth functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'Matern32Kernel'</span></code>: Matérn kernel with ν=3/2, moderately smooth functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'Matern52Kernel'</span></code>: Matérn kernel with ν=5/2, smooth functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'RationalQuadraticKernel'</span></code>: Rational quadratic kernel, scale mixture of RBF kernels</p></li>
</ul>
</dd>
</dl>
<p>See <a class="reference external" href="https://george.readthedocs.io/en/latest/user/kernels/">https://george.readthedocs.io/en/latest/user/kernels/</a> for more details.</p>
</p></li>
<li><p><strong>fit_amp</strong> – (<em>bool, optional</em>) 
Whether to optimize the amplitude (overall scale) hyperparameter of the kernel.
If True, the GP will learn the optimal amplitude from data. Default is True.</p></li>
<li><p><strong>fit_mean</strong> – (<em>bool, optional</em>) 
Whether to optimize the mean function hyperparameter. If True, the GP will learn
a constant mean offset. If False, assumes zero mean. Default is True.</p></li>
<li><p><strong>fit_white_noise</strong> – (<em>bool, optional</em>) 
Whether to optimize the white noise (nugget) hyperparameter. If True, the GP will
learn the optimal noise level. If False, uses the fixed value from white_noise.
Default is True.</p></li>
<li><p><strong>white_noise</strong> – (<em>float, optional</em>) 
Log-scale white noise parameter. If fit_white_noise=False, this fixed value is used.
If fit_white_noise=True, this serves as the initial guess. Typical values are 
between -15 (very low noise) and -5 (high noise). Default is -12.</p></li>
<li><p><strong>gp_scale_rng</strong> – (<em>list of two floats, optional</em>) 
Log-scale bounds for the characteristic length scale parameters of the kernel.
Format: [log_min_scale, log_max_scale]. These bounds apply to all input dimensions.
Default is [-2, 2], corresponding to scales between ~0.14 and ~7.4 in original units.</p></li>
<li><p><strong>overwrite</strong> – (<em>bool, optional</em>) 
If True, allows reinitializing the GP even if one already exists. If False and a GP
already exists, raises an AssertionError. Default is False.</p></li>
<li><p><strong>gp_opt_method</strong> – (<em>str, optional</em>) 
Optimization method for GP hyperparameter optimization. Passed to scipy.optimize.minimize.
Common options: ‘newton-cg’, ‘l-bfgs-b’, ‘bfgs’, ‘cg’. Default is ‘newton-cg’.</p></li>
<li><p><strong>gp_nopt</strong> – (<em>int, optional</em>) 
Number of optimization restarts for GP hyperparameter optimization. Multiple restarts
help avoid local minima. Default is 3.</p></li>
<li><p><strong>optimizer_kwargs</strong> – (<em>dict, optional</em>) 
Additional keyword arguments passed to the scipy optimizer. Common options include
‘max_iter’ (maximum iterations) and convergence tolerances. 
Default is {“max_iter”: 50}.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AssertionError</strong> – If a GP already exists and overwrite=False.</p></li>
<li><p><strong>ValueError</strong> – If an invalid kernel name is provided.</p></li>
<li><p><strong>Exception</strong> – If GP initialization fails after multiple attempts with different scale lengths.</p></li>
</ul>
</dd>
<dt class="field-odd">Note<span class="colon">:</span></dt>
<dd class="field-odd"><p>This function must be called after init_samples() since it requires training data
to initialize the GP. The function will automatically retry initialization with
different random scale lengths if the initial attempt fails.</p>
</dd>
<dt class="field-even">Example<span class="colon">:</span></dt>
<dd class="field-even"><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Basic initialization with default settings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_gp</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Custom kernel with specific hyperparameter settings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_gp</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;Matern52Kernel&quot;</span><span class="p">,</span> 
<span class="gp">... </span>           <span class="n">fit_white_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
<span class="gp">... </span>           <span class="n">white_noise</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">gp_scale_rng</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># High-precision optimization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">init_gp</span><span class="p">(</span><span class="n">gp_opt_method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">gp_nopt</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;ftol&quot;</span><span class="p">:</span> <span class="mf">1e-9</span><span class="p">})</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.eval_gp_at_iteration">
<span class="sig-name descname"><span class="pre">eval_gp_at_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_var</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.eval_gp_at_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.eval_gp_at_iteration" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.surrogate_log_likelihood">
<span class="sig-name descname"><span class="pre">surrogate_log_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta_xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_var</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.surrogate_log_likelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.surrogate_log_likelihood" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate predictive mean of the GP at point(s) <code class="docutils literal notranslate"><span class="pre">theta_xs</span></code></p>
<p>This method is vectorized to handle both single parameter vectors and
arrays of parameter vectors efficiently.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta_xs</strong> (<em>array-like</em>) – Point(s) to evaluate GP mean at. Can be:
- 1D array of shape (ndim,) for single point
- 2D array of shape (npoints, ndim) for multiple points</p></li>
<li><p><strong>iter</strong> (<em>int, optional</em>) – Iteration number of GP to use. If -1, uses most recent GP.</p></li>
<li><p><strong>return_var</strong> (<em>bool, optional</em>) – Whether to also return variance predictions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ypred</strong> (<em>array</em>) – GP mean(s) evaluated at <code class="docutils literal notranslate"><span class="pre">theta_xs</span></code>. Shape matches input.</p></li>
<li><p><strong>varpred</strong> (<em>array, optional</em>) – GP variance(s) if return_var=True.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>array or tuple of arrays</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.surrogate_likelihood">
<span class="sig-name descname"><span class="pre">surrogate_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta_xs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.surrogate_likelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.surrogate_likelihood" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate predictive probability (not log-probability) of the GP at point(s) theta_xs</p>
<p>This method is vectorized to handle both single parameter vectors and
arrays of parameter vectors efficiently.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>theta_xs</strong> (<em>array-like</em>) – Point(s) to evaluate GP probability at. Can be:
- 1D array of shape (ndim,) for single point
- 2D array of shape (npoints, ndim) for multiple points</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>GP probability/probabilities evaluated at <code class="docutils literal notranslate"><span class="pre">theta_xs</span></code>. Shape matches input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>float or array</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.find_next_point">
<span class="sig-name descname"><span class="pre">find_next_point</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nopt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attempts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.find_next_point"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.find_next_point" title="Link to this definition">¶</a></dt>
<dd><p>Find next set of <code class="docutils literal notranslate"><span class="pre">(theta,</span> <span class="pre">y)</span></code> training points by maximizing the
active learning utility function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>nopt</strong> – (<em>int, optional</em>) 
Number of times to restart the objective function optimization. 
Defaults to 1. Increase to avoid converging to local maxima.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.active_train">
<span class="sig-name descname"><span class="pre">active_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">niter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bape'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_opt_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_opt_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l-bfgs-b'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nopt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attempts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_grad_opt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.active_train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.active_train" title="Link to this definition">¶</a></dt>
<dd><p>Perform active learning to iteratively improve the surrogate model.</p>
<p>Uses acquisition functions to intelligently select new training points that
will most improve the Gaussian Process model. Different algorithms balance
exploration (uncertainty reduction) vs exploitation (finding optima).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>niter</strong> – (<em>int, optional, default=100</em>)
Number of active learning iterations. Each iteration adds one new training point.</p></li>
<li><p><strong>algorithm</strong> – (<em>str, optional, default=”bape”</em>)
Active learning algorithm. Options:
- “bape”: Bayesian Active Parameter Estimation (exploration-focused)
- “jones”: Jones algorithm (exploitation-focused, good for optimization)
- “agp”: Augmented Gaussian Process (balanced)
- “alternate”: Alternates between exploration and exploitation</p></li>
<li><p><strong>gp_opt_freq</strong> – (<em>int, optional, default=20</em>)
Frequency of GP hyperparameter re-optimization. GP hyperparameters are
re-optimized every gp_opt_freq iterations. Lower values = more optimization.</p></li>
<li><p><strong>save_progress</strong> – (<em>bool, optional, default=False</em>)
Whether to save training progress data for later analysis.</p></li>
<li><p><strong>obj_opt_method</strong> – (<em>str, optional, default=”nelder-mead”</em>)
Optimization method for acquisition function. Options:
- “l-bfgs-b”: L-BFGS-B (good with gradients)
- “nelder-mead”: Nelder-Mead simplex (gradient-free)</p></li>
<li><p><strong>nopt</strong> – (<em>int, optional, default=1</em>)
Number of optimization restarts for acquisition function. Higher values
help avoid local minima but increase computation time.</p></li>
<li><p><strong>n_attempts</strong> – (<em>int, optional, default=5</em>)
Number of attempts for each optimization restart.</p></li>
<li><p><strong>use_grad_opt</strong> – (<em>bool, optional, default=True</em>)
Whether to use gradient information if available. Set False for
gradient-free optimization.</p></li>
<li><p><strong>optimizer_kwargs</strong> – (<em>dict, optional, default={}</em>)
Additional keyword arguments passed to the optimizer.</p></li>
<li><p><strong>show_progress</strong> – (<em>bool, optional, default=True</em>)
Whether to display progress bar during training.</p></li>
</ul>
</dd>
<dt class="field-even">Note<span class="colon">:</span></dt>
<dd class="field-even"><p></p></dd>
</dl>
<p>Active learning algorithms have different purposes:</p>
<ul class="simple">
<li><p><strong>BAPE</strong>: Best for uncertainty quantification and space-filling</p></li>
<li><p><strong>Jones</strong>: Best for finding likelihood maxima/minima (optimization)</p></li>
<li><p><strong>Alternate</strong>: Good balance for both exploration and exploitation</p></li>
<li><p><strong>AGP</strong>: Another balanced approach</p></li>
</ul>
<p>The method automatically handles GP re-training and hyperparameter optimization
based on the specified frequency. Training data is accumulated in _theta and _y
attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Basic active learning with BAPE:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Optimization-focused active learning:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;jones&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Balanced approach with frequent GP optimization:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;alternate&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.lnprob">
<span class="sig-name descname"><span class="pre">lnprob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.lnprob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.lnprob" title="Link to this definition">¶</a></dt>
<dd><p>Log probability function used for <code class="docutils literal notranslate"><span class="pre">emcee</span></code>, which sums the prior with the surrogate model likelihood</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\ln P(\theta | x) \propto \ln P(x | \theta) + \ln P(\theta)\]</div>
</div>
<p>where ln P(x | theta) is the surrogate likelihood function and ln P(theta) is the prior function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>theta</strong> – (<em>array, required</em>) 
Array of model input parameters to evaluate model probability at.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.find_map">
<span class="sig-name descname"><span class="pre">find_map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nelder-mead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nRestarts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.find_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.find_map" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.run_emcee">
<span class="sig-name descname"><span class="pre">run_emcee</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">like_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nwalkers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nsteps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_proc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_fn_comment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">burn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.run_emcee"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.run_emcee" title="Link to this definition">¶</a></dt>
<dd><p>Sample the posterior using the emcee affine-invariant MCMC algorithm.</p>
<p>This method uses the emcee package to perform Markov Chain Monte Carlo (MCMC) 
sampling on either the trained GP surrogate model or the true likelihood function.
The affine-invariant ensemble sampler is robust and works well for a wide variety
of posterior shapes without requiring manual tuning of step sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>like_fn</strong> – (<em>callable, str, or None, optional</em>)
Likelihood function to sample. Options:
- None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)
- “surrogate”, “gp”: Uses the GP surrogate model explicitly
- “true”: Uses the true likelihood function (self.true_log_likelihood)
- callable: Custom likelihood function with signature like_fn(theta)
Default is None.</p></li>
<li><p><strong>prior_fn</strong> – (<em>callable or None, optional</em>)
Log-prior function with signature prior_fn(theta). Should return log-probability
density. If None, uses uniform prior with bounds from self.bounds.
Default is None.</p></li>
<li><p><strong>nwalkers</strong> – (<em>int or None, optional</em>)
Number of MCMC walkers in the ensemble. Should be at least 2*ndim.
If None, defaults to 10*ndim. More walkers improve convergence but increase
computational cost. Default is None.</p></li>
<li><p><strong>nsteps</strong> – (<em>int, optional</em>)
Number of MCMC steps per walker. Total number of likelihood evaluations
will be nwalkers * nsteps. Default is 50000.</p></li>
<li><p><strong>sampler_kwargs</strong> – (<em>dict, optional</em>)
Additional keyword arguments passed to emcee.EnsembleSampler constructor.
Common options include:
- ‘a’: Stretch move scale parameter (default: 2.0)
- ‘moves’: Custom proposal moves
Default is {}.</p></li>
<li><p><strong>run_kwargs</strong> – (<em>dict, optional</em>)
Additional keyword arguments passed to the run_mcmc() method.
Common options include:
- ‘progress’: Show progress bar (default: True)
- ‘store’: Store chain in memory (default: True)
Default is {}.</p></li>
<li><p><strong>opt_init</strong> – (<em>bool, optional</em>)
Whether to initialize walkers near the maximum a posteriori (MAP) estimate.
If True, uses find_map() to locate starting point. If False, initializes
walkers randomly from the prior. Default is False.</p></li>
<li><p><strong>multi_proc</strong> – (<em>bool, optional</em>)
Whether to use multiprocessing with self.ncore processes. Generally
recommended for expensive likelihood evaluations. Default is True.</p></li>
<li><p><strong>prior_fn_comment</strong> – (<em>str or None, optional</em>)
Comment describing the prior function for logging purposes. If None
and prior_fn is provided, attempts to extract function name.
Default is None.</p></li>
<li><p><strong>burn</strong> – (<em>int or None, optional</em>)
Number of burn-in samples to discard from each walker. If None, 
automatically estimates burn-in using autocorrelation analysis.
Default is None.</p></li>
<li><p><strong>thin</strong> – (<em>int or None, optional</em>)
Thinning factor - keep every thin-th sample to reduce autocorrelation.
If None, automatically estimates based on autocorrelation time.
Default is None.</p></li>
</ul>
</dd>
</dl>
<section id="attributes-set">
<h3>Attributes Set<a class="headerlink" href="#attributes-set" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>sampler<span class="classifier">emcee.EnsembleSampler</span></dt><dd><p>The emcee sampler object containing full chain and metadata</p>
</dd>
<dt>emcee_samples<span class="classifier">ndarray of shape (nsamples_final, ndim)</span></dt><dd><p>Final MCMC samples after burn-in and thinning</p>
</dd>
<dt>emcee_samples_full<span class="classifier">ndarray of shape (nsteps, nwalkers, ndim)</span></dt><dd><p>Full MCMC chain before processing</p>
</dd>
<dt>emcee_samples_true<span class="classifier">ndarray of shape (nsamples_final, ndim)</span></dt><dd><p>Final samples when using true likelihood (like_fn=”true”)</p>
</dd>
<dt>emcee_samples_gp<span class="classifier">ndarray of shape (nsamples_final, ndim)</span></dt><dd><p>Final samples when using surrogate likelihood</p>
</dd>
<dt>emcee_run<span class="classifier">bool</span></dt><dd><p>Flag indicating emcee has been successfully run</p>
</dd>
<dt>emcee_runtime<span class="classifier">float</span></dt><dd><p>Wall-clock time taken for emcee sampling in seconds</p>
</dd>
<dt>nwalkers<span class="classifier">int</span></dt><dd><p>Number of walkers used</p>
</dd>
<dt>nsteps<span class="classifier">int</span></dt><dd><p>Number of steps per walker</p>
</dd>
<dt>burn<span class="classifier">int</span></dt><dd><p>Burn-in length used for final samples</p>
</dd>
<dt>thin<span class="classifier">int</span></dt><dd><p>Thinning factor used for final samples</p>
</dd>
<dt>iburn<span class="classifier">int</span></dt><dd><p>Automatically estimated burn-in length</p>
</dd>
<dt>ithin<span class="classifier">int</span></dt><dd><p>Automatically estimated thinning factor</p>
</dd>
<dt>acc_frac<span class="classifier">float</span></dt><dd><p>Mean acceptance fraction across all walkers</p>
</dd>
<dt>autcorr_time<span class="classifier">float</span></dt><dd><p>Mean autocorrelation time in steps</p>
</dd>
<dt>like_fn_name<span class="classifier">str</span></dt><dd><p>Name of likelihood function used (“true”, “surrogate”, or “likelihood”)</p>
</dd>
<dt>prior_fn_comment<span class="classifier">str</span></dt><dd><p>Description of prior function used</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">note<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>The emcee algorithm uses an affine-invariant ensemble sampler that:
- Does not require tuning of step sizes or proposal distributions
- Works well for correlated and multi-modal posteriors
- Scales well with parameter dimensionality
- Provides built-in convergence diagnostics</p>
<p>Burn-in and thinning are automatically estimated using autocorrelation
analysis if not provided. The acceptance fraction should typically be
between 0.2-0.5 for good performance.</p>
<dl class="field-list simple">
<dt class="field-odd">example<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Sample surrogate model with default settings:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_emcee</span><span class="p">()</span>
</pre></div>
</div>
<p>Sample true likelihood with specific settings:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_emcee</span><span class="p">(</span><span class="n">like_fn</span><span class="o">=</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="n">nwalkers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nsteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>Use custom prior and optimize initialization:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">log_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># Custom Gaussian prior</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_emcee</span><span class="p">(</span><span class="n">prior_fn</span><span class="o">=</span><span class="n">log_prior</span><span class="p">,</span> <span class="n">opt_init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Run with manual burn-in and thinning:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_emcee</span><span class="p">(</span><span class="n">nsteps</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h3>
<p>emcee documentation: <a class="reference external" href="https://emcee.readthedocs.io/">https://emcee.readthedocs.io/</a>
Foreman-Mackey et al. (2013): “emcee: The MCMC Hammer”, PASP, 125, 306-312</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.run_dynesty">
<span class="sig-name descname"><span class="pre">run_dynesty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">like_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'dynamic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_proc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_transform_comment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.run_dynesty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.run_dynesty" title="Link to this definition">¶</a></dt>
<dd><p>Sample the posterior using the dynesty nested sampling algorithm.</p>
<p>This method uses the dynesty package to perform nested sampling on either the 
trained GP surrogate model or the true likelihood function. Dynesty is particularly 
effective for estimating the Bayesian evidence and exploring multi-modal posteriors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>like_fn</strong> – (<em>callable, str, or None, optional</em>)
Likelihood function to sample. Options:
- None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)
- “surrogate”, “gp”: Uses the GP surrogate model explicitly
- “true”: Uses the true likelihood function (self.true_log_likelihood)  
- callable: Custom likelihood function with signature like_fn(theta)
Default is None.</p></li>
<li><p><strong>prior_transform</strong> – (<em>callable or None, optional</em>)
Prior transformation function that maps from unit hypercube [0,1]^ndim
to the parameter space. Should have signature prior_transform(u) where
u is array of shape (ndim,) with values in [0,1]. If None, uses uniform
prior with bounds from self.bounds. Default is None.</p></li>
<li><p><strong>mode</strong> – (<em>{“dynamic”, “static”}, optional</em>)
Dynesty sampling mode. “dynamic” uses DynamicNestedSampler which adaptively
allocates live points, while “static” uses fixed number of live points.
Dynamic mode is generally more efficient. Default is “dynamic”.</p></li>
<li><p><strong>sampler_kwargs</strong> – (<em>dict, optional</em>)
Additional keyword arguments passed to the dynesty sampler constructor.
Common options include:
- ‘nlive’: Number of live points (default: 50*ndim)
- ‘bound’: Bounding method (‘multi’, ‘single’, ‘none’)
- ‘sample’: Sampling method (‘auto’, ‘unif’, ‘rwalk’, ‘slice’, ‘rslice’, ‘hslice’)
Default is {}.</p></li>
<li><p><strong>run_kwargs</strong> – (<em>dict, optional</em>)
Additional keyword arguments passed to the run_nested() method.
Common options include:
- ‘dlogz’: Target evidence uncertainty (default: 0.5)
- ‘maxiter’: Maximum number of iterations (default: 50000)
- ‘wt_kwargs’: Weight function arguments (default: {‘pfrac’: 1.0})
- ‘stop_kwargs’: Stopping criterion arguments (default: {‘pfrac’: 1.0})
Default is {}.</p></li>
<li><p><strong>multi_proc</strong> – (<em>bool, optional</em>)
Whether to use multiprocessing. If True, uses self.ncore processes.
Note that multiprocessing can sometimes be slower due to overhead.
Default is False.</p></li>
<li><p><strong>save_iter</strong> – (<em>int or None, optional</em>)
If provided, saves the sampler state every save_iter iterations to allow
for checkpointing and resuming long runs. Saves to 
‘{savedir}/dynesty_sampler_{like_fn_name}.pkl’. Default is None.</p></li>
<li><p><strong>prior_transform_comment</strong> – (<em>str or None, optional</em>)
Comment describing the prior transform for logging purposes. If None
and prior_transform is provided, attempts to extract function name.
Default is None.</p></li>
</ul>
</dd>
</dl>
<section id="id1">
<h3>Attributes Set<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>res<span class="classifier">dynesty.results.Results</span></dt><dd><p>Complete dynesty results object containing samples, weights, evidence, etc.</p>
</dd>
<dt>dynesty_samples<span class="classifier">ndarray of shape (nsamples, ndim)</span></dt><dd><p>Resampled posterior samples with equal weights</p>
</dd>
<dt>dynesty_samples_true<span class="classifier">ndarray of shape (nsamples, ndim)</span></dt><dd><p>Posterior samples when using true likelihood (like_fn=”true”)</p>
</dd>
<dt>dynesty_samples_surrogate<span class="classifier">ndarray of shape (nsamples, ndim)  </span></dt><dd><p>Posterior samples when using surrogate likelihood</p>
</dd>
<dt>dynesty_run<span class="classifier">bool</span></dt><dd><p>Flag indicating dynesty has been successfully run</p>
</dd>
<dt>dynesty_runtime<span class="classifier">float</span></dt><dd><p>Wall-clock time taken for dynesty sampling in seconds</p>
</dd>
<dt>like_fn_name<span class="classifier">str</span></dt><dd><p>Name of likelihood function used (“true”, “surrogate”, or “custom”)</p>
</dd>
<dt>prior_transform_comment<span class="classifier">str</span></dt><dd><p>Description of prior transform used</p>
</dd>
</dl>
</section>
<section id="notes">
<h3>Notes<a class="headerlink" href="#notes" title="Link to this heading">¶</a></h3>
<p>Dynesty is particularly well-suited for:
- Computing Bayesian evidence for model comparison
- Exploring multi-modal posteriors
- Providing robust posterior sampling without tuning</p>
<p>The default settings prioritize posterior sampling over evidence estimation
by setting pfrac=1.0, which focuses computational effort on high-likelihood
regions rather than exploring the full prior volume.</p>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">¶</a></h3>
<p>Sample surrogate model with default settings:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">()</span>
</pre></div>
</div>
<p>Sample true likelihood with more live points:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">(</span><span class="n">like_fn</span><span class="o">=</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;nlive&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">})</span>
</pre></div>
</div>
<p>Use custom prior with bounds [-5, 5] for each parameter:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">my_prior</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="mi">10</span><span class="o">*</span><span class="n">u</span> <span class="o">-</span> <span class="mi">5</span>  <span class="c1"># maps [0,1] to [-5,5]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">(</span><span class="n">prior_transform</span><span class="o">=</span><span class="n">my_prior</span><span class="p">)</span>
</pre></div>
</div>
<p>Run with checkpointing every 1000 iterations:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">(</span><span class="n">save_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">50000</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>References<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>Dynesty documentation: <a class="reference external" href="https://dynesty.readthedocs.io/">https://dynesty.readthedocs.io/</a>
Speagle (2020): “dynesty: a dynamic nested sampling package for estimating
Bayesian posteriors and evidences”, MNRAS, 493, 3132-3158</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plots</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[None,</span> <span class="pre">None]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.plot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.plot" title="Link to this definition">¶</a></dt>
<dd><p>Generate diagnostic plots for training progress, GP performance, and MCMC results.</p>
<p>This method creates various diagnostic plots to assess the quality of the surrogate
model training, GP hyperparameter optimization, and MCMC sampling results. Plots
are automatically saved to the model’s save directory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>plots</strong> – <p>(<em>list of str, optional</em>)
List of plot types to generate. Each plot requires specific data to be available
(e.g., ‘emcee_corner’ requires run_emcee() to have been called first). If None,
no plots are generated. Available options:</p>
<p><strong>Training diagnostics:</strong>
- ‘test_mse’: Mean squared error vs training iteration
- ‘test_scaled_mse’: Scaled MSE vs training iteration  
- ‘test_log_mse’: Log-scale MSE vs training iteration
- ‘gp_hyperparameters’: GP hyperparameter evolution during training
- ‘gp_train_time’: GP training time vs iteration
- ‘gp_train_corner’: Corner plot of final training samples
- ‘gp_train_scatter’: Scatter plot of training samples vs predictions</p>
<p><strong>GP visualization (2D only):</strong>
- ‘gp_fit_2D’: 2D contour plot of GP surrogate surface</p>
<p><strong>MCMC diagnostics:</strong>
- ‘emcee_corner’: Corner plot of emcee posterior samples
- ‘emcee_walkers’: Walker trajectories for emcee chains
- ‘dynesty_corner’: Corner plot of dynesty posterior samples  
- ‘dynesty_corner_kde’: KDE version of dynesty corner plot
- ‘dynesty_traceplot’: Trace plot of dynesty sampling
- ‘dynesty_runplot’: Dynesty convergence diagnostics</p>
<p><strong>Comparison plots:</strong>
- ‘mcmc_comparison’: Compare emcee and dynesty posteriors</p>
<p><strong>Convenience options:</strong>
- ‘gp_all’: Generate all available GP training plots</p>
<p>Default is None.</p>
</p></li>
<li><p><strong>show</strong> – (<em>bool, optional</em>)
Whether to display plots interactively in addition to saving them.
If False, plots are only saved to disk. Default is False.</p></li>
<li><p><strong>cb_rng</strong> – (<em>list of [float, float], optional</em>)
Colorbar range for 2D contour plots as [vmin, vmax]. If [None, None],
uses automatic range determination. Only applies to plots with colorbars
like ‘gp_fit_2D’. Default is [None, None].</p></li>
<li><p><strong>log_scale</strong> – (<em>bool, optional</em>)
Whether to use logarithmic color scale for 2D contour plots. If True,
applies matplotlib.colors.LogNorm to the colorbar. Only applies to
plots with colorbars. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>None or matplotlib.figure.Figure</em>
Some individual plots may return figure objects for further customization.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>NameError</strong> – If required data for a requested plot is not available (e.g., requesting
‘emcee_corner’ before running run_emcee()).</p></li>
<li><p><strong>AttributeError</strong> – If the model has not been properly initialized or trained.</p></li>
</ul>
</dd>
</dl>
<section id="id3">
<h3>Notes<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>Plots are automatically saved to the model’s save directory (self.savedir)
with descriptive filenames. The save directory is created if it doesn’t exist.</p>
<p>Training diagnostic plots help assess:
- Convergence of active learning process
- Quality of GP hyperparameter optimization  
- Efficiency of training sample selection</p>
<p>MCMC diagnostic plots help assess:
- Posterior sampling convergence
- Chain mixing and autocorrelation
- Comparison between different sampling methods</p>
</section>
<section id="id4">
<h3>Examples<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>Generate all GP training plots:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gp_all&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Create MCMC comparison plots:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_emcee</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">run_dynesty</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;emcee_corner&#39;</span><span class="p">,</span> <span class="s1">&#39;dynesty_corner&#39;</span><span class="p">,</span> <span class="s1">&#39;mcmc_comparison&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Generate 2D GP visualization with custom colorbar:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gp_fit_2D&#39;</span><span class="p">],</span> <span class="n">cb_rng</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Show plots interactively:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sm</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;test_mse&#39;</span><span class="p">,</span> <span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">],</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.active_train_parallel">
<span class="sig-name descname"><span class="pre">active_train_parallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">niter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nchains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bape'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_opt_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_opt_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nelder-mead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nopt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attempts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_grad_opt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combine_frequency</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.active_train_parallel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.active_train_parallel" title="Link to this definition">¶</a></dt>
<dd><p>Run multiple active learning chains in parallel and combine their training samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>niter</strong> – (<em>int, optional</em>) 
Number of iterations per chain. Default 100.</p></li>
<li><p><strong>nchains</strong> – (<em>int, optional</em>) 
Number of parallel chains to run. Default 4.</p></li>
<li><p><strong>algorithm</strong> – (<em>str, optional</em>) 
Active learning algorithm. Default “bape”.</p></li>
<li><p><strong>gp_opt_freq</strong> – (<em>int, optional</em>)
Frequency of GP hyperparameter optimization. Default 20.</p></li>
<li><p><strong>save_progress</strong> – (<em>bool, optional</em>)
Whether to save progress during training. Default False.</p></li>
<li><p><strong>obj_opt_method</strong> – (<em>str, optional</em>)
Optimization method for acquisition function. Default “nelder-mead”.</p></li>
<li><p><strong>nopt</strong> – (<em>int, optional</em>)
Number of restarts for acquisition optimization. Default 1.</p></li>
<li><p><strong>n_attempts</strong> – (<em>int, optional</em>)
Number of attempts for optimization. Default 5.</p></li>
<li><p><strong>use_grad_opt</strong> – (<em>bool, optional</em>)
Whether to use gradient-based optimization. Default True.</p></li>
<li><p><strong>optimizer_kwargs</strong> – (<em>dict, optional</em>)
Additional optimizer kwargs. Default {}.</p></li>
<li><p><strong>combine_frequency</strong> – (<em>int, optional</em>)
How often to combine chains (in iterations). If None, only combines at the end.</p></li>
<li><p><strong>show_progress</strong> – (<em>bool, optional</em>) 
Whether to display progress bar during parallel chain execution. When True, shows
a progress bar tracking chain completion. Individual active_train progress bars
are disabled to avoid clutter. Default is True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.get_chain_diversity_metrics">
<span class="sig-name descname"><span class="pre">get_chain_diversity_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.get_chain_diversity_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.get_chain_diversity_metrics" title="Link to this definition">¶</a></dt>
<dd><p>Calculate diversity metrics for the combined training samples.
Useful for assessing the effectiveness of parallel chains.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="alabi.core.SurrogateModel.compare_parallel_vs_sequential">
<span class="sig-name descname"><span class="pre">compare_parallel_vs_sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">niter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nchains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bape'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_opt_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nelder-mead'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/core.html#SurrogateModel.compare_parallel_vs_sequential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.core.SurrogateModel.compare_parallel_vs_sequential" title="Link to this definition">¶</a></dt>
<dd><p>Compare parallel vs sequential active learning performance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>niter</strong> – Number of iterations for comparison</p></li>
<li><p><strong>nchains</strong> – Number of chains for parallel version</p></li>
<li><p><strong>algorithm</strong> – Active learning algorithm to use</p></li>
<li><p><strong>obj_opt_method</strong> – Optimization method</p></li>
<li><p><strong>kwargs</strong> – Additional arguments passed to active_train methods</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary with comparison results</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-alabi.gp_utils">
<span id="alabi-gp-utils-module"></span><h2>alabi.gp_utils module<a class="headerlink" href="#module-alabi.gp_utils" title="Link to this heading">¶</a></h2>
<section id="gp-utils-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">gp_utils.py</span></code><a class="headerlink" href="#gp-utils-py" title="Link to this heading">¶</a></h3>
<p>Gaussian process utility functions for initializing GPs and optimizing their
hyperparameters.</p>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.gp_utils.configure_gp">
<span class="sig-prename descclassname"><span class="pre">alabi.gp_utils.</span></span><span class="sig-name descname"><span class="pre">configure_gp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_white_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">white_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hyperparameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/gp_utils.html#configure_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.gp_utils.configure_gp" title="Link to this definition">¶</a></dt>
<dd><p>Configure and initialize a Gaussian Process with robust error handling.</p>
<p>Creates a george.GP object with the specified kernel and configuration options.
Includes automatic fixes for common numerical issues such as singular matrices,
duplicate points, and poor conditioning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – (<em>array-like, shape (n_samples, n_features)</em>)
Training input locations (parameters). Must contain finite values only.</p></li>
<li><p><strong>y</strong> – (<em>array-like, shape (n_samples,)</em>)
Training target values (function evaluations). Must contain finite values only.</p></li>
<li><p><strong>kernel</strong> – (<em>george kernel object</em>)
George kernel object defining the covariance function. Common options include
ExpSquaredKernel, Matern32Kernel, Matern52Kernel.</p></li>
<li><p><strong>fit_amp</strong> – (<em>bool, optional, default=True</em>)
Whether to fit the kernel amplitude. If True, scales the kernel by the 
variance of y to improve conditioning.</p></li>
<li><p><strong>fit_mean</strong> – (<em>bool, optional, default=True</em>)
Whether to fit a constant mean function. If True, initializes mean to 
median(y) and allows optimization.</p></li>
<li><p><strong>fit_white_noise</strong> – (<em>bool, optional, default=False</em>)
Whether to fit the white noise (nugget) parameter. If True, the noise
level will be optimized during hyperparameter tuning.</p></li>
<li><p><strong>white_noise</strong> – (<em>float, optional, default=-12</em>)
Log-scale white noise parameter. Acts as regularization to prevent
singular matrices. More negative values = less noise.</p></li>
<li><p><strong>hyperparameters</strong> – (<em>array-like, optional, default=None</em>)
Pre-specified hyperparameters to set. If provided, these values are used
instead of the kernel’s default initialization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>george.GP or None</em>
Configured and computed GP object ready for predictions, or None if
configuration failed despite all attempted fixes.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If theta or y contain non-finite values (NaN or inf).</p></li>
<li><p><strong>LinAlgError</strong> – If GP computation fails due to singular covariance matrix, automatically
attempts several fixes before giving up.</p></li>
</ul>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The function includes several automatic fixes for numerical issues:</p>
<ol class="arabic simple">
<li><p><strong>Jitter addition</strong>: Progressively increases white noise to regularize</p></li>
<li><p><strong>Duplicate removal</strong>: Detects and removes duplicate training points</p></li>
<li><p><strong>Error reporting</strong>: Provides diagnostic information for debugging</p></li>
</ol>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.gp_utils.optimize_gp">
<span class="sig-prename descclassname"><span class="pre">alabi.gp_utils.</span></span><span class="sig-name descname"><span class="pre">optimize_gp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp_hyper_prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l-bfgs-b'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/gp_utils.html#optimize_gp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.gp_utils.optimize_gp" title="Link to this definition">¶</a></dt>
<dd><p>Optimize Gaussian Process hyperparameters by maximizing marginal likelihood.</p>
<p>Performs hyperparameter optimization for a Gaussian Process using scipy’s
minimize function. Supports multiple optimization restarts and automatically
selects the result with highest marginal likelihood.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gp</strong> – (<em>george.GP</em>)
Configured Gaussian Process object. Should be computed with training data.</p></li>
<li><p><strong>theta</strong> – (<em>array-like, shape (n_samples, n_features)</em>)
Training input locations (parameters). Will be squeezed if 1D.</p></li>
<li><p><strong>y</strong> – (<em>array-like, shape (n_samples,)</em>)
Training target values (function evaluations). Will be squeezed if 1D.</p></li>
<li><p><strong>gp_hyper_prior</strong> – (<em>callable</em>)
Prior function for hyperparameters. Should return log-probability density
for given hyperparameter vector. Used to constrain optimization.</p></li>
<li><p><strong>p0</strong> – (<em>array-like, shape (n_restarts, n_hyperparams) or (n_hyperparams,)</em>)
Initial guesses for hyperparameter optimization. If 2D, performs multiple
restarts with different initializations.</p></li>
<li><p><strong>bounds</strong> – (<em>list of tuples, optional, default=None</em>)
Bounds for hyperparameter optimization as [(min, max), …]. Only used
for methods that support bounds (e.g., ‘l-bfgs-b’).</p></li>
<li><p><strong>method</strong> – <p>(<em>str, optional, default=”l-bfgs-b”</em>)
Scipy optimization method. Supported methods:</p>
<ul>
<li><p>’l-bfgs-b’: L-BFGS-B with bounds support (default)</p></li>
<li><p>’newton-cg’: Newton-CG with gradients</p></li>
<li><p>’bfgs’: BFGS (no bounds support)</p></li>
<li><p>’powell’: Powell method (derivative-free)</p></li>
</ul>
</p></li>
<li><p><strong>optimizer_kwargs</strong> – (<em>dict, optional, default=None</em>)
Additional keyword arguments passed to scipy.optimize.minimize.
If None, uses method-specific defaults optimized for GP optimization.</p></li>
<li><p><strong>max_iter</strong> – (<em>int, optional, default=50</em>)
Maximum number of iterations for optimization. Used as default in
optimizer_kwargs if not specified.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>george.GP or None</em>
GP object with optimized hyperparameters, or None if optimization failed.
The returned GP is recomputed with optimal hyperparameters.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>Multiple Restarts</strong>: If p0 is 2D, the function performs multiple optimization
restarts and selects the result with highest marginal log-likelihood. This helps
avoid local minima in the hyperparameter space.</p>
<p><strong>Gradient Usage</strong>: For methods ‘newton-cg’ and ‘l-bfgs-b’, analytical gradients
are used via the <cite>_grad_nll</cite> function for faster convergence.</p>
<p><strong>Error Handling</strong>: The function gracefully handles optimization failures and
falls back to initial hyperparameters when necessary.</p>
<p><strong>Examples</strong></p>
<p>Basic hyperparameter optimization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">uniform</span>

<span class="c1"># Define prior function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">prior</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="c1"># Uniform prior on log-scale parameters</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">params</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">params</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.0</span>  <span class="c1"># log(1.0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

<span class="c1"># Single optimization</span>
<span class="n">p0</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>  <span class="c1"># Current hyperparameters</span>
<span class="n">optimized_gp</span> <span class="o">=</span> <span class="n">optimize_gp</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span>
</pre></div>
</div>
<p>Multiple restarts for robustness:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple random initializations</span>
<span class="n">n_restarts</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">())</span>
<span class="n">p0_multi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_restarts</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>

<span class="c1"># Optimization with bounds</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">*</span> <span class="n">n_params</span>  <span class="c1"># Bound each parameter</span>

<span class="n">optimized_gp</span> <span class="o">=</span> <span class="n">optimize_gp</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">p0_multi</span><span class="p">,</span> 
                          <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;l-bfgs-b&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Custom optimization settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># High-precision optimization</span>
<span class="n">custom_opts</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
    <span class="s1">&#39;ftol&#39;</span><span class="p">:</span> <span class="mf">1e-12</span><span class="p">,</span>
    <span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span>
<span class="p">}</span>

<span class="n">optimized_gp</span> <span class="o">=</span> <span class="n">optimize_gp</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span>
                          <span class="n">method</span><span class="o">=</span><span class="s1">&#39;l-bfgs-b&#39;</span><span class="p">,</span>
                          <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">custom_opts</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.gp_utils.grad_gp_mean_prediction">
<span class="sig-prename descclassname"><span class="pre">alabi.gp_utils.</span></span><span class="sig-name descname"><span class="pre">grad_gp_mean_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/gp_utils.html#grad_gp_mean_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.gp_utils.grad_gp_mean_prediction" title="Link to this definition">¶</a></dt>
<dd><p>Compute the gradient of the GP mean prediction with respect to the input
locations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – (<em>array-like, required</em>)
Input locations to evaluate the gradient at.</p></li>
<li><p><strong>gp</strong> – (<em>george.GP, required</em>)
The computed Gaussian process object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(<em>array</em>)
The gradient of the GP mean prediction at the input locations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.gp_utils.grad_gp_var_prediction">
<span class="sig-prename descclassname"><span class="pre">alabi.gp_utils.</span></span><span class="sig-name descname"><span class="pre">grad_gp_var_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/gp_utils.html#grad_gp_var_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.gp_utils.grad_gp_var_prediction" title="Link to this definition">¶</a></dt>
<dd><p>Compute the gradient of the GP variance prediction with respect to the input
locations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – (<em>array-like, required</em>)
Input locations to evaluate the gradient at.</p></li>
<li><p><strong>gp</strong> – (<em>george.GP, required</em>)
The computed Gaussian process object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(<em>array</em>)
The gradient of the GP variance prediction at the input locations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.gp_utils.numerical_kernel_gradient">
<span class="sig-prename descclassname"><span class="pre">alabi.gp_utils.</span></span><span class="sig-name descname"><span class="pre">numerical_kernel_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/gp_utils.html#numerical_kernel_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.gp_utils.numerical_kernel_gradient" title="Link to this definition">¶</a></dt>
<dd><p>Compute numerical gradient of GP kernel k(xs, x_train) with respect to xs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – (<em>array_like, shape (d,) or (1, d)</em>)
Query point(s) to compute gradient at</p></li>
<li><p><strong>x_train</strong> – (<em>array_like, shape (n, d)</em>)
Training points</p></li>
<li><p><strong>gp</strong> – (<em>george.GP</em>)
Gaussian Process object with kernel</p></li>
<li><p><strong>h</strong> – (<em>float, default=1e-6</em>)
Step size for finite differences</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>ndarray, shape (n, d)</em>
Numerical gradient of kernel k(xs, x_train) w.r.t. xs
Each row corresponds to gradient w.r.t. one training point</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-alabi.mcmc_utils">
<span id="alabi-mcmc-utils-module"></span><h2>alabi.mcmc_utils module<a class="headerlink" href="#module-alabi.mcmc_utils" title="Link to this heading">¶</a></h2>
<section id="mcmc-utils-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">mcmc_utils.py</span></code><a class="headerlink" href="#mcmc-utils-py" title="Link to this heading">¶</a></h3>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.mcmc_utils.estimate_burnin">
<span class="sig-prename descclassname"><span class="pre">alabi.mcmc_utils.</span></span><span class="sig-name descname"><span class="pre">estimate_burnin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">est_burnin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thin_chains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/mcmc_utils.html#estimate_burnin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.mcmc_utils.estimate_burnin" title="Link to this definition">¶</a></dt>
<dd><p>Estimate the integrated autocorrelation length on the MCMC chain associated
with an emcee sampler object. With the integrated autocorrelation length,
we can then estimate the burn-in length for the MCMC chain. This procedure
follows the example outlined here:
<a class="reference external" href="https://emcee.readthedocs.io/en/stable/tutorials/autocorr/">https://emcee.readthedocs.io/en/stable/tutorials/autocorr/</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sampler</strong> – (<em>emcee.EnsembleSampler, optional</em>)
emcee MCMC sampler object/backend handler, given a complete chain</p></li>
<li><p><strong>est_burnin</strong> – (<em>bool, optional</em>)
Estimate burn-in time using integrated autocorrelation time
heuristic.  Defaults to True. In general, we recommend users
inspect the chains and calculate the burnin after the fact to ensure
convergence, but this function works pretty well.</p></li>
<li><p><strong>thin_chains</strong> – (<em>bool, optional</em>)
Whether or not to thin chains.  Useful if running long chains.
Defaults to True.  If true, estimates a thin cadence
via int(0.5*np.min(tau)) where tau is the intergrated autocorrelation
time.</p></li>
<li><p><strong>verbose</strong> – (<em>bool, optional</em>)
Output all the diagnostics? Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns iburn<span class="colon">:</span></dt>
<dd class="field-even"><p>(<em>int</em>)
burn-in index estimate.  If est_burnin == False, returns 0.</p>
</dd>
<dt class="field-odd">Returns ithin<span class="colon">:</span></dt>
<dd class="field-odd"><p>(<em>int</em>)
thin cadence estimate.  If thin_chains == False, returns 1.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-alabi.utility">
<span id="alabi-utility-module"></span><h2>alabi.utility module<a class="headerlink" href="#module-alabi.utility" title="Link to this heading">¶</a></h2>
<section id="utility-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">utility.py</span></code><a class="headerlink" href="#utility-py" title="Link to this heading">¶</a></h3>
<p>Utility functions in terms of usefulness, e.g. minimizing GP utility functions
or computing KL divergences, and the GP utility functions, e.g. the bape utility.</p>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.agp_utility">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">agp_utility</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#agp_utility"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.agp_utility" title="Link to this definition">¶</a></dt>
<dd><p>Compute the AGP (Adaptive Gaussian Process) utility function based on posterior entropy.</p>
<p>AGP is an information-theoretic acquisition function that measures the entropy
of the Gaussian Process posterior distribution. It balances the GP mean prediction
with the uncertainty (variance), preferring regions with high predicted values
and high uncertainty.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – (<em>array-like of shape (ndim,)</em>)
Parameter values at which to evaluate the utility function.</p></li>
<li><p><strong>y</strong> – (<em>array-like of shape (nsamples,)</em>)
Observed function values at training points, used to condition the GP.</p></li>
<li><p><strong>gp</strong> – (<em>george.GP</em>)
Trained Gaussian Process model. The GP will be computed if not already done.</p></li>
<li><p><strong>bounds</strong> – (<em>array-like of shape (ndim, 2)</em>)
Parameter bounds as [(min, max), …] for each dimension. Used to check
if theta is within the prior support.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>float</em>
Negative AGP utility value. The negative is used so that minimizing
this function is equivalent to maximizing the actual utility.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The AGP utility function is defined as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[u(\theta) = \mu(\theta) + \frac{1}{2} \ln(2\pi e \sigma^2(\theta))\]</div>
</div>
<p>This represents the entropy of the posterior predictive distribution at θ.
The utility encourages sampling where:
- The mean prediction μ(θ) is high (exploitation)
- The predictive variance σ²(θ) is high (exploration)</p>
<p>AGP provides a different balance compared to other acquisition functions:
- More exploitative than BAPE (focuses on high mean regions)
- Less optimization-focused than Expected Improvement
- Naturally balances exploration and exploitation through entropy</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the utility computation results in invalid values (e.g., negative variance).</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Evaluate AGP utility at a test point:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">utility</span> <span class="o">=</span> <span class="n">agp_utility</span><span class="p">(</span><span class="n">theta_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
</pre></div>
</div>
<p>Find next point by minimizing negative utility:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">agp_utility</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">),</span> <span class="n">x0</span><span class="p">)</span>
<span class="n">next_point</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Wang &amp; Li (2017): “Adaptive Gaussian Process Approximation for Bayesian 
Inference with Expensive Likelihood Functions”, Neural Computation, 30, 3072-3094.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.bape_utility">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">bape_utility</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#bape_utility"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.bape_utility" title="Link to this definition">¶</a></dt>
<dd><p>Compute the BAPE (Bayesian Active Posterior Estimation) utility function.</p>
<p>BAPE is an active learning acquisition function designed for posterior exploration
rather than optimization. It identifies regions where the GP variance is high
relative to the mean, promoting exploration of the parameter space. The utility
is computed in log-form for numerical stability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> (<em>array-like of shape (ndim,)</em>) – Parameter values at which to evaluate the utility function.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape (nsamples,)</em>) – Observed function values at training points, used to condition the GP.</p></li>
<li><p><strong>gp</strong> (<em>george.GP</em>) – Trained Gaussian Process model. Must have been computed (gp.computed=True).</p></li>
<li><p><strong>bounds</strong> (<em>array-like of shape (ndim, 2)</em>) – Parameter bounds as [(min, max), …] for each dimension. Used to check
if theta is within the prior support.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Negative log-BAPE utility value. The negative is used so that minimizing
this function is equivalent to maximizing the actual utility.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>float</em></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The BAPE utility function is defined as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[u(\theta) = e^{2\mu(\theta) + \sigma^2(\theta)} \left(e^{\sigma^2(\theta)} - 1 \right)\]</div>
</div>
<p>This function returns the negative logarithm of the utility for numerical stability:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[-\log u(\theta) = -\left(2\mu(\theta) + \sigma^2(\theta) + \log(e^{\sigma^2(\theta)} - 1)\right)\]</div>
</div>
<p>BAPE is particularly effective for:</p>
<ul class="simple">
<li><p>Exploring multi-modal posteriors</p></li>
<li><p>Reducing uncertainty in posterior estimates</p></li>
<li><p>Active learning when the goal is posterior characterization</p></li>
</ul>
<p>Unlike optimization-focused acquisitions (e.g., Expected Improvement), BAPE
prioritizes exploration over exploitation, making it less suitable for finding
global optima but excellent for posterior mapping.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If the utility computation results in invalid values (e.g., negative variance).</p></li>
<li><p><strong>RuntimeError</strong> – If the GP has not been computed before calling this function.</p></li>
</ul>
</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Evaluate BAPE utility at a test point:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">theta_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">utility</span> <span class="o">=</span> <span class="n">bape_utility</span><span class="p">(</span><span class="n">theta_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
</pre></div>
</div>
<p>Find next point by minimizing negative utility:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">bape_utility</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">),</span> <span class="n">x0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_point</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Kandasamy et al. (2015): “Query efficient posterior estimation in scientific 
experiments via Bayesian active learning”, Artificial Intelligence, 243, 45-56.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.grad_bape_utility">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">grad_bape_utility</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#grad_bape_utility"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.grad_bape_utility" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.jones_utility">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">jones_utility</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zeta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#jones_utility"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.jones_utility" title="Link to this definition">¶</a></dt>
<dd><p>Compute the Expected Improvement (EI) acquisition function.</p>
<p>This function implements the Expected Improvement criterion from Jones et al. (1998),
which balances exploitation (sampling where the mean is high) with exploration
(sampling where the uncertainty is high). Unlike BAPE, this acquisition function
is designed specifically for global optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> (<em>array-like of shape (ndim,)</em>) – Parameter values at which to evaluate the utility function.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape (nsamples,)</em>) – Observed function values at training points. The maximum value is used
as the current best (f_best).</p></li>
<li><p><strong>gp</strong> (<em>george.GP</em>) – Trained Gaussian Process model. Must have been computed (gp.computed=True).</p></li>
<li><p><strong>bounds</strong> (<em>array-like of shape (ndim, 2)</em>) – Parameter bounds as [(min, max), …] for each dimension. Used to check
if theta is within the prior support.</p></li>
<li><p><strong>zeta</strong> (<em>float, optional</em>) – <p>Exploration parameter controlling the trade-off between exploitation
and exploration. Larger values promote more exploration:</p>
<ul>
<li><p>zeta = 0: Pure exploitation (greedy)</p></li>
<li><p>zeta &gt; 0: Balanced exploration/exploitation</p></li>
<li><p>zeta &gt;&gt; 0: Pure exploration</p></li>
</ul>
<p>Default is 0.01.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Negative Expected Improvement value. The negative is used so that minimizing
this function is equivalent to maximizing the Expected Improvement.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>float</em></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Expected Improvement is defined as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[EI(\theta) = \mathbb{E}[\max(f(\theta) - f_{\text{best}} - \zeta, 0)]\]</div>
</div>
<p>where f_best is the current best observed value. For a Gaussian predictive
distribution with mean μ and variance σ², this becomes:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[EI(\theta) = (\mu - f_{\text{best}} - \zeta) \Phi(z) + \sigma \phi(z)\]</div>
</div>
<p>where z = (μ - f_best - ζ)/σ, Φ is the standard normal CDF, and φ is the
standard normal PDF.</p>
<p>Expected Improvement is particularly effective for:</p>
<ul class="simple">
<li><p>Global optimization problems</p></li>
<li><p>Finding the maximum of expensive functions</p></li>
<li><p>Balancing local and global search</p></li>
</ul>
<p>Compared to BAPE, Expected Improvement focuses on exploitation (finding optima)
rather than exploration (mapping the entire posterior).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If the utility computation results in invalid values.</p></li>
<li><p><strong>RuntimeError</strong> – If the GP has not been computed before calling this function.</p></li>
</ul>
</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Evaluate Expected Improvement at a test point:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">theta_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ei_value</span> <span class="o">=</span> <span class="n">jones_utility</span><span class="p">(</span><span class="n">theta_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
</pre></div>
</div>
<p>Find next point for optimization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jones_utility</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">zeta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">x0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_point</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
<p>Use higher exploration parameter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ei_explore</span> <span class="o">=</span> <span class="n">jones_utility</span><span class="p">(</span><span class="n">theta_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">zeta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jones et al. (1998): “Efficient global optimization of expensive black-box 
functions”, Journal of Global Optimization, 13, 455-492.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.assign_utility">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">assign_utility</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algorithm</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#assign_utility"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.assign_utility" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.minimize_objective">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">minimize_objective</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_obj_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nopt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l-bfgs-b'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ncore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attempts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#minimize_objective"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.minimize_objective" title="Link to this definition">¶</a></dt>
<dd><p>Find the global minimum of an acquisition function using multiple restarts.</p>
<p>This function optimizes acquisition functions (BAPE, Jones/EI, etc.) to select
the next point for active learning. Multiple optimization restarts with different
initial points help avoid local minima, which is crucial for acquisition function
optimization where the landscape can be highly multi-modal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obj_fn</strong> (<em>callable</em>) – Objective function to minimize. Should have signature obj_fn(theta) and
return a scalar value. Typically an acquisition function like BAPE or EI.</p></li>
<li><p><strong>grad_obj_fn</strong> (<em>callable or None</em>) – Gradient of the objective function. Should have signature grad_obj_fn(theta)
and return array of shape (ndim,). If None, uses finite differences.</p></li>
<li><p><strong>bounds</strong> (<em>array-like of shape (ndim, 2)</em>) – Parameter bounds as [(min, max), …] for each dimension. Used both
for optimization constraints and generating initial points.</p></li>
<li><p><strong>nopt</strong> (<em>int, optional</em>) – Number of optimization restarts with different initial points. More
restarts increase the chance of finding the global minimum but increase
computational cost. Default is 1.</p></li>
<li><p><strong>method</strong> (<em>str, optional</em>) – <p>Scipy optimization method to use. Common choices:</p>
<ul>
<li><p>”l-bfgs-b”: Quasi-Newton with bounds (default, works well with gradients)</p></li>
<li><p>”nelder-mead”: Simplex method (gradient-free, robust)</p></li>
<li><p>”tnc”: Truncated Newton with bounds</p></li>
<li><p>”slsqp”: Sequential Least Squares Programming</p></li>
</ul>
<p>Default is “l-bfgs-b”.</p>
</p></li>
<li><p><strong>ps</strong> (<em>callable or None, optional</em>) – Prior sampling function with signature ps(nsample=1). Used to generate
initial points for optimization restarts. If None, uses uniform sampling
within bounds. Default is None.</p></li>
<li><p><strong>options</strong> (<em>dict or None, optional</em>) – <p>Additional options passed to scipy.optimize.minimize. Common options:</p>
<ul>
<li><p>”max_iter”: Maximum iterations (default: 50)</p></li>
<li><p>”ftol”: Function tolerance for convergence</p></li>
<li><p>”gtol”: Gradient tolerance for convergence</p></li>
</ul>
<p>Default is {“max_iter”: 50}.</p>
</p></li>
<li><p><strong>ncore</strong> (<em>int, optional</em>) – Number of CPU cores to use for parallel optimization. If ncore &gt; 1,
runs multiple optimization restarts in parallel using multiprocessing.
Default is 1 (serial execution).</p></li>
<li><p><strong>n_attempts</strong> (<em>int, optional</em>) – Maximum number of retry attempts if an optimization fails (returns
infinite or out-of-bounds results). Default is 5.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>theta_best</strong> (<em>ndarray of shape (ndim,)</em>) – Parameter values that minimize the objective function.</p></li>
<li><p><strong>obj_best</strong> (<em>float</em>) – Minimum objective function value achieved.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>tuple</em></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If no valid solutions are found after all optimization attempts.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is critical for acquisition function optimization in active learning.
The key challenges addressed are:</p>
<ol class="arabic simple">
<li><p><strong>Multi-modality</strong>: Acquisition functions often have many local minima</p></li>
<li><p><strong>Numerical stability</strong>: Some points may yield infinite or invalid values</p></li>
<li><p><strong>Boundary constraints</strong>: Solutions must respect parameter bounds</p></li>
<li><p><strong>Computational efficiency</strong>: Parallel restarts when multiple cores available</p></li>
</ol>
<p>The function automatically retries failed optimizations with new initial points
and filters out invalid solutions (infinite values, out-of-bounds points).</p>
</div>
<p><strong>Examples</strong></p>
<p>Basic optimization of BAPE utility:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">theta_next</span><span class="p">,</span> <span class="n">obj_min</span> <span class="o">=</span> <span class="n">minimize_objective</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">obj_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">bape_utility</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">grad_obj_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">grad_bape_utility</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">nopt</span><span class="o">=</span><span class="mi">5</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Parallel optimization with multiple restarts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">theta_next</span><span class="p">,</span> <span class="n">obj_min</span> <span class="o">=</span> <span class="n">minimize_objective</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">obj_fn</span><span class="o">=</span><span class="n">acquisition_fn</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">grad_obj_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Use finite differences</span>
<span class="gp">... </span>    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">nopt</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ncore</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Custom optimization settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;ftol&quot;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">theta_next</span><span class="p">,</span> <span class="n">obj_min</span> <span class="o">=</span> <span class="n">minimize_objective</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">obj_fn</span><span class="o">=</span><span class="n">acquisition_fn</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">grad_obj_fn</span><span class="o">=</span><span class="n">grad_fn</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.prior_sampler">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">prior_sampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#prior_sampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.prior_sampler" title="Link to this definition">¶</a></dt>
<dd><p>Sample from parameter space using various quasi-random sampling methods.</p>
<p>This function generates samples within specified bounds using different sampling
strategies from the scikit-optimize library. It provides a unified interface to
various space-filling designs commonly used in Bayesian optimization and 
surrogate modeling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>bounds</strong> – (<em>array-like of shape (ndim, 2)</em>)
Array of (min, max) bounds for each parameter dimension. Each row specifies
the lower and upper bounds for one parameter.</p></li>
<li><p><strong>nsample</strong> – (<em>int, optional</em>)
Number of samples to generate. Default is 1.</p></li>
<li><p><strong>sampler</strong> – <p>(<em>{‘uniform’, ‘sobol’, ‘lhs’, ‘halton’, ‘hammersly’, ‘grid’}, optional</em>)
Sampling method to use:</p>
<ul>
<li><p>’uniform’: Pseudo-random uniform sampling</p></li>
<li><p>’sobol’: Sobol sequence (quasi-random, good space-filling)</p></li>
<li><p>’lhs’: Latin Hypercube Sampling (stratified sampling)</p></li>
<li><p>’halton’: Halton sequence (quasi-random, low discrepancy)</p></li>
<li><p>’hammersly’: Hammersley sequence (quasi-random)</p></li>
<li><p>’grid’: Regular grid sampling</p></li>
</ul>
<p>Default is ‘uniform’.</p>
</p></li>
<li><p><strong>random_state</strong> – (<em>int, RandomState instance or None, optional</em>)
Random state for reproducible sampling. If None, uses a different random
seed each time to avoid clustering. Default is None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>ndarray of shape (nsample, ndim)</em>
Array of parameter samples within the specified bounds.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If an invalid sampler method is specified.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Quasi-random samplers (sobol, halton, hammersley) provide better space-filling
properties than pseudo-random uniform sampling, which is beneficial for:
- Initial design of experiments
- Training surrogate models
- Global optimization</p>
<p>Latin Hypercube Sampling ensures each parameter dimension is stratified,
providing good marginal coverage even with small sample sizes.</p>
<p>For optimization starting points, consider using ‘sobol’ or ‘lhs’ instead of
‘uniform’ to avoid clustering issues when generating single samples repeatedly.</p>
<p><strong>Examples</strong></p>
<p>Generate uniform random samples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (5, 2)</span>
</pre></div>
</div>
<p>Use Sobol sequence for better space-filling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s1">&#39;sobol&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Latin Hypercube Sampling for stratified design:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s1">&#39;lhs&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>For more information on sampling methods, see:
<a class="reference external" href="https://scikit-optimize.github.io/stable/auto_examples/sampler/initial-sampling-method.html">https://scikit-optimize.github.io/stable/auto_examples/sampler/initial-sampling-method.html</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.prior_sampler_normal">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">prior_sampler_normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prior_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#prior_sampler_normal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.prior_sampler_normal" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.eval_fn">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">eval_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ncore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#eval_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.eval_fn" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate a function at multiple parameter points with optional parallelization.</p>
<p>This utility function provides a convenient interface for evaluating expensive
functions (like likelihood functions or forward models) at multiple parameter
values, with automatic parallelization when multiple cores are available.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> – (<em>callable</em>)
Function to evaluate at each parameter point. Should have signature
fn(theta_i) where theta_i is array of shape (ndim,) and return a scalar.</p></li>
<li><p><strong>theta</strong> – (<em>array-like of shape (npoints, ndim)</em>)
Array of parameter points at which to evaluate the function. Each row
represents one parameter vector.</p></li>
<li><p><strong>ncore</strong> – (<em>int, optional</em>)
Number of CPU cores to use for parallel evaluation. If ncore &lt;= 1,
uses serial evaluation with progress bar. If ncore &gt; 1, uses
multiprocessing.Pool for parallel evaluation. Default is all available cores.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>ndarray of shape (npoints,)</em>
Function values evaluated at each parameter point. Order matches the
input theta array.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>This function is particularly useful for:
- Initial sampling of expensive functions for surrogate model training
- Batch evaluation of likelihood functions
- Testing surrogate model accuracy on validation sets</p>
<p>The function automatically prints timing information and handles both serial
and parallel execution modes. For serial execution, a progress bar is shown
using tqdm.</p>
<p><strong>Examples</strong></p>
<p>Evaluate a simple function at multiple points:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">quadratic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">eval_fn</span><span class="p">(</span><span class="n">quadratic</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Parallel evaluation of expensive function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">expensive_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Simulate expensive computation</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">eval_fn</span><span class="p">(</span><span class="n">expensive_fn</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.lnprior_uniform">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">lnprior_uniform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#lnprior_uniform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.lnprior_uniform" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate log-probability density of uniform prior distribution.</p>
<p>This function computes the log-probability density for a uniform (flat)
prior distribution within specified bounds. Points outside the bounds
receive log-probability of negative infinity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – (<em>array-like of shape (ndim,) or float</em>)
Parameter values at which to evaluate the log-prior. For scalar input,
assumes 1D parameter space.</p></li>
<li><p><strong>bounds</strong> – (<em>array-like of shape (ndim, 2)</em>)
Parameter bounds as [(min, max), …] for each dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>float</em>
Log-probability density. Returns 0.0 if all parameters are within bounds,
-np.inf if any parameter is outside bounds.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For a uniform distribution on [a, b], the probability density is 1/(b-a)
and the log-probability density is -log(b-a). However, this function 
returns 0.0 for in-bounds points since constant offsets don’t affect
relative probabilities in MCMC sampling.</p>
<p><strong>Examples</strong></p>
<p>Check if point is within bounds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">lnp</span> <span class="o">=</span> <span class="n">lnprior_uniform</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>  <span class="c1"># Returns 0.0</span>
</pre></div>
</div>
<p>Point outside bounds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># First parameter out of bounds</span>
<span class="n">lnp</span> <span class="o">=</span> <span class="n">lnprior_uniform</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>  <span class="c1"># Returns -inf</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.prior_transform_uniform">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">prior_transform_uniform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#prior_transform_uniform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.prior_transform_uniform" title="Link to this definition">¶</a></dt>
<dd><p>Transform uniform random variables to parameter space with specified bounds.</p>
<p>This function implements the inverse CDF transformation for uniform distributions,
mapping from the unit hypercube [0,1]^ndim to the parameter space with given bounds.
It is commonly used in nested sampling algorithms like dynesty.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – (<em>array-like of shape (ndim,)</em>)
Random variables uniformly distributed on [0,1] for each dimension.</p></li>
<li><p><strong>bounds</strong> – (<em>array-like of shape (ndim, 2)</em>)
Parameter bounds as [(min, max), …] for each dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>ndarray of shape (ndim,)</em>
Transformed parameter values within the specified bounds.</p>
</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The transformation for each dimension i is:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\theta'_i = (b_{i,\text{max}} - b_{i,\text{min}}) \theta_i + b_{i,\text{min}}\]</div>
</div>
<p>where b_{i,min} and b_{i,max} are the bounds for dimension i.</p>
<p>This is the inverse of the uniform CDF, mapping uniform random variables
on [0,1] to uniform random variables on [a,b].</p>
<p><strong>Examples</strong></p>
<p>Transform unit cube samples to parameter bounds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">theta_unit</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>  <span class="c1"># Uniform on [0,1]</span>
<span class="n">theta_params</span> <span class="o">=</span> <span class="n">prior_transform_uniform</span><span class="p">(</span><span class="n">theta_unit</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta_params</span><span class="p">)</span>  <span class="c1"># [-1.0, 8.0]</span>
</pre></div>
</div>
<p>Use with nested sampling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">my_prior_transform</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">prior_transform_uniform</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>
<span class="c1"># Pass to dynesty sampler</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.lnprior_normal">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">lnprior_normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#lnprior_normal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.lnprior_normal" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.utility.prior_transform_normal">
<span class="sig-prename descclassname"><span class="pre">alabi.utility.</span></span><span class="sig-name descname"><span class="pre">prior_transform_normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/utility.html#prior_transform_normal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.utility.prior_transform_normal" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-alabi.visualization">
<span id="alabi-visualization-module"></span><h2>alabi.visualization module<a class="headerlink" href="#module-alabi.visualization" title="Link to this heading">¶</a></h2>
<section id="visualization-py">
<h3><code class="xref py py-mod docutils literal notranslate"><span class="pre">visualization.py</span></code><a class="headerlink" href="#visualization-py" title="Link to this heading">¶</a></h3>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_error_vs_iteration">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_error_vs_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iteration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_error</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_error</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GP</span> <span class="pre">fit'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savedir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_error_vs_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_error_vs_iteration" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_hyperparam_vs_iteration">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_hyperparam_vs_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GP</span> <span class="pre">fit'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_hyperparam_vs_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_hyperparam_vs_iteration" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_train_time_vs_iteration">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_train_time_vs_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GP</span> <span class="pre">fit'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_train_time_vs_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_train_time_vs_iteration" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_corner_lnp">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_corner_lnp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_corner_lnp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_corner_lnp" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_corner_scatter">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_corner_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_corner_scatter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_corner_scatter" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_gp_fit_1D">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_gp_fit_1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GP</span> <span class="pre">fit'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_gp_fit_1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_gp_fit_1D" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_gp_fit_2D">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_gp_fit_2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ngrid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GP</span> <span class="pre">fit'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Blues_r'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_gp_fit_2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_gp_fit_2D" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_contour_2D">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_contour_2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savedir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ngrid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Blues_r'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xlabel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ylabel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_contour_2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_contour_2D" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_true_fit_2D">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_true_fit_2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ngrid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_true_fit_2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_true_fit_2D" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_utility_2D">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_utility_2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ngrid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_utility_2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_utility_2D" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_dynesty_traceplot">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_dynesty_traceplot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_dynesty_traceplot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_dynesty_traceplot" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_dynesty_runplot">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_dynesty_runplot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_dynesty_runplot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_dynesty_runplot" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_mcmc_comparison">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_mcmc_comparison</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sampler</span> <span class="pre">1</span> <span class="pre">posterior'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sampler</span> <span class="pre">2</span> <span class="pre">posterior'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savedir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mcmc_comparison.png'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['orange',</span> <span class="pre">'royalblue']</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_mcmc_comparison"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_mcmc_comparison" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_emcee_dynesty_comparison">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_emcee_dynesty_comparison</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_emcee_dynesty_comparison"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_emcee_dynesty_comparison" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="alabi.visualization.plot_2D_panel4">
<span class="sig-prename descclassname"><span class="pre">alabi.visualization.</span></span><span class="sig-name descname"><span class="pre">plot_2D_panel4</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">savedir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/alabi/visualization.html#plot_2D_panel4"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#alabi.visualization.plot_2D_panel4" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          <a class="prev-page" href="modules.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">alabi</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Jess Birky
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">alabi package</a><ul>
<li><a class="reference internal" href="#module-alabi.benchmarks">alabi.benchmarks module</a><ul>
<li><a class="reference internal" href="#benchmarks-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">benchmarks.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.benchmarks.random_gaussian_covariance"><code class="docutils literal notranslate"><span class="pre">random_gaussian_covariance()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.cache_utils">alabi.cache_utils module</a><ul>
<li><a class="reference internal" href="#cache-utils-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">cache_utils.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.cache_utils.load_model_cache"><code class="docutils literal notranslate"><span class="pre">load_model_cache()</span></code></a></li>
<li><a class="reference internal" href="#alabi.cache_utils.write_report_gp"><code class="docutils literal notranslate"><span class="pre">write_report_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.cache_utils.write_report_emcee"><code class="docutils literal notranslate"><span class="pre">write_report_emcee()</span></code></a></li>
<li><a class="reference internal" href="#alabi.cache_utils.write_report_dynesty"><code class="docutils literal notranslate"><span class="pre">write_report_dynesty()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.core">alabi.core module</a><ul>
<li><a class="reference internal" href="#core-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">core.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel"><code class="docutils literal notranslate"><span class="pre">SurrogateModel</span></code></a><ul>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.gp"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.gp</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.bounds"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.bounds</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel._bounds"><code class="docutils literal notranslate"><span class="pre">SurrogateModel._bounds</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel._theta"><code class="docutils literal notranslate"><span class="pre">SurrogateModel._theta</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel._y"><code class="docutils literal notranslate"><span class="pre">SurrogateModel._y</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.ntrain"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.ntrain</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.ndim"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.ndim</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.emcee_samples"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.emcee_samples</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.dynesty_samples"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.dynesty_samples</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.__init__"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.__init__()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.save"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.save()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.theta"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.theta()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.y"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.y()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.init_train"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.init_train()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.load_train"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.load_train()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.init_samples"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.init_samples()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.set_hyperparam_prior_bounds"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.set_hyperparam_prior_bounds()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.fit_gp"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.fit_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.opt_gp"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.opt_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.init_gp"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.init_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.eval_gp_at_iteration"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.eval_gp_at_iteration()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.surrogate_log_likelihood"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.surrogate_log_likelihood()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.surrogate_likelihood"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.surrogate_likelihood()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.find_next_point"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.find_next_point()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.active_train"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.active_train()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.lnprob"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.lnprob()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.find_map"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.find_map()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.run_emcee"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.run_emcee()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.run_dynesty"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.run_dynesty()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.plot"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.plot()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.active_train_parallel"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.active_train_parallel()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.get_chain_diversity_metrics"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.get_chain_diversity_metrics()</span></code></a></li>
<li><a class="reference internal" href="#alabi.core.SurrogateModel.compare_parallel_vs_sequential"><code class="docutils literal notranslate"><span class="pre">SurrogateModel.compare_parallel_vs_sequential()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.gp_utils">alabi.gp_utils module</a><ul>
<li><a class="reference internal" href="#gp-utils-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gp_utils.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.gp_utils.configure_gp"><code class="docutils literal notranslate"><span class="pre">configure_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.gp_utils.optimize_gp"><code class="docutils literal notranslate"><span class="pre">optimize_gp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.gp_utils.grad_gp_mean_prediction"><code class="docutils literal notranslate"><span class="pre">grad_gp_mean_prediction()</span></code></a></li>
<li><a class="reference internal" href="#alabi.gp_utils.grad_gp_var_prediction"><code class="docutils literal notranslate"><span class="pre">grad_gp_var_prediction()</span></code></a></li>
<li><a class="reference internal" href="#alabi.gp_utils.numerical_kernel_gradient"><code class="docutils literal notranslate"><span class="pre">numerical_kernel_gradient()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.mcmc_utils">alabi.mcmc_utils module</a><ul>
<li><a class="reference internal" href="#mcmc-utils-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mcmc_utils.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.mcmc_utils.estimate_burnin"><code class="docutils literal notranslate"><span class="pre">estimate_burnin()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.utility">alabi.utility module</a><ul>
<li><a class="reference internal" href="#utility-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">utility.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.agp_utility"><code class="docutils literal notranslate"><span class="pre">agp_utility()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.bape_utility"><code class="docutils literal notranslate"><span class="pre">bape_utility()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.grad_bape_utility"><code class="docutils literal notranslate"><span class="pre">grad_bape_utility()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.jones_utility"><code class="docutils literal notranslate"><span class="pre">jones_utility()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.assign_utility"><code class="docutils literal notranslate"><span class="pre">assign_utility()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.minimize_objective"><code class="docutils literal notranslate"><span class="pre">minimize_objective()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.prior_sampler"><code class="docutils literal notranslate"><span class="pre">prior_sampler()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.prior_sampler_normal"><code class="docutils literal notranslate"><span class="pre">prior_sampler_normal()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.eval_fn"><code class="docutils literal notranslate"><span class="pre">eval_fn()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.lnprior_uniform"><code class="docutils literal notranslate"><span class="pre">lnprior_uniform()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.prior_transform_uniform"><code class="docutils literal notranslate"><span class="pre">prior_transform_uniform()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.lnprior_normal"><code class="docutils literal notranslate"><span class="pre">lnprior_normal()</span></code></a></li>
<li><a class="reference internal" href="#alabi.utility.prior_transform_normal"><code class="docutils literal notranslate"><span class="pre">prior_transform_normal()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-alabi.visualization">alabi.visualization module</a><ul>
<li><a class="reference internal" href="#visualization-py"><code class="xref py py-mod docutils literal notranslate"><span class="pre">visualization.py</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_error_vs_iteration"><code class="docutils literal notranslate"><span class="pre">plot_error_vs_iteration()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_hyperparam_vs_iteration"><code class="docutils literal notranslate"><span class="pre">plot_hyperparam_vs_iteration()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_train_time_vs_iteration"><code class="docutils literal notranslate"><span class="pre">plot_train_time_vs_iteration()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_corner_lnp"><code class="docutils literal notranslate"><span class="pre">plot_corner_lnp()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_corner_scatter"><code class="docutils literal notranslate"><span class="pre">plot_corner_scatter()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_gp_fit_1D"><code class="docutils literal notranslate"><span class="pre">plot_gp_fit_1D()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_gp_fit_2D"><code class="docutils literal notranslate"><span class="pre">plot_gp_fit_2D()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_contour_2D"><code class="docutils literal notranslate"><span class="pre">plot_contour_2D()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_true_fit_2D"><code class="docutils literal notranslate"><span class="pre">plot_true_fit_2D()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_utility_2D"><code class="docutils literal notranslate"><span class="pre">plot_utility_2D()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_dynesty_traceplot"><code class="docutils literal notranslate"><span class="pre">plot_dynesty_traceplot()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_dynesty_runplot"><code class="docutils literal notranslate"><span class="pre">plot_dynesty_runplot()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_mcmc_comparison"><code class="docutils literal notranslate"><span class="pre">plot_mcmc_comparison()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_emcee_dynesty_comparison"><code class="docutils literal notranslate"><span class="pre">plot_emcee_dynesty_comparison()</span></code></a></li>
<li><a class="reference internal" href="#alabi.visualization.plot_2D_panel4"><code class="docutils literal notranslate"><span class="pre">plot_2D_panel4()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="_static/documentation_options.js?v=d45e8c67"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/notebook-outputs.js?v=9bb603c3"></script>
    </body>
</html>