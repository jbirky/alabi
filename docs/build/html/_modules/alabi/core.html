<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html"><link rel="search" title="Search" href="../../search.html">
        <link rel="canonical" href="https://alabi.jessicabirky.com/_modules/alabi/core.html">

    <!-- Generated with Sphinx 9.0.4 and Furo 2025.12.19 -->
        <title>alabi.core - alabi</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0cf789f7" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --font-stack: Roboto Light, sans-serif;
  --font-stack--monospace: Courier, monospace;
  --color-background-secondary: #eff1f6;
  --color-inline-code-background: #eff1f6;
  --color-sidebar-item-background--hover: white;
  --color-brand-primary: #004080;
  --color-brand-content: #0059b3;
  --font-size--small: 0.875rem;
  --font-size--normal: 1rem;
  --font-size--large: 1.125rem;
  --font-size-h1: 2.2rem;
  --font-size-h2: 1.8rem;
  --font-size-h3: 1.5rem;
  --font-size-h4: 1.3rem;
  --font-size-h5: 1.1rem;
  --font-size-h6: 1rem;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">alabi</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  <span class="sidebar-brand-text">alabi</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html#quickstart-example">Quickstart Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../save_reload.html">Saving and Reloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gp_tutorial.html">GP Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mcmc_tutorial.html">MCMC sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_bayesian_optimization.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_hp_settings.html">Automated Hyperparameter Selection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_1d.html">Visualize Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_2d.html">Test 2D Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_line_fit.html">Fit a line to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_kl_divergence.html">KL Divergence: Gaussian</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_gaussian_nd.html">Test computational scaling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../trappist_stellar_evolution.html">Stellar Evolution</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../modules.html">alabi</a><input aria-label="Toggle navigation of alabi" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../alabi.html">alabi package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/LICENSE">License</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/issues">Issues</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for alabi.core</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">:py:mod:`core.py` </span>
<span class="sd">-------------------------------------</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">utility</span> <span class="k">as</span> <span class="n">ut</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">visualization</span> <span class="k">as</span> <span class="n">vis</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">gp_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">mcmc_utils</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">cache_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">parallel_utils</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">george</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">george</span><span class="w"> </span><span class="kn">import</span> <span class="n">kernels</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">multiprocess</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">op</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SurrogateModel&quot;</span><span class="p">,</span> <span class="s2">&quot;CachedSurrogateLikelihood&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="CachedSurrogateLikelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.CachedSurrogateLikelihood">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CachedSurrogateLikelihood</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A picklable cached surrogate likelihood function.</span>
<span class="sd">    </span>
<span class="sd">    This class creates a callable object that caches the GP computation</span>
<span class="sd">    and can be pickled for use with multiprocessing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="CachedSurrogateLikelihood.__init__">
<a class="viewcode-back" href="../../alabi.html#alabi.core.CachedSurrogateLikelihood.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gp_iter</span><span class="p">,</span> <span class="n">_y_cond</span><span class="p">,</span> <span class="n">theta_scaler</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the cached surrogate likelihood.</span>
<span class="sd">        </span>
<span class="sd">        :param gp_iter: Pre-computed GP object</span>
<span class="sd">        :param y_cond: Training target values</span>
<span class="sd">        :param theta_scaler: Parameter scaler object</span>
<span class="sd">        :param y_scaler: Target scaler object  </span>
<span class="sd">        :param ndim: Number of dimensions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_iter</span> <span class="o">=</span> <span class="n">gp_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_cond</span> <span class="o">=</span> <span class="n">_y_cond</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span> <span class="o">=</span> <span class="n">theta_scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span> <span class="o">=</span> <span class="n">y_scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="n">ndim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_var</span> <span class="o">=</span> <span class="n">return_var</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta_xs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the cached surrogate likelihood.</span>
<span class="sd">        </span>
<span class="sd">        :param theta_xs: Point(s) to evaluate. Same format as surrogate_log_likelihood.</span>
<span class="sd">        :param return_var: Whether to return variance as well.</span>
<span class="sd">        :returns: Same format as surrogate_log_likelihood.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert input to numpy array and handle dimensionality</span>
        <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Handle 1D input (single point)</span>
        <span class="k">if</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;theta_xs must be 1D or 2D array, got </span><span class="si">{</span><span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling transformation</span>
        <span class="n">_theta_xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        
        <span class="c1"># Ensure proper shape for george GP</span>
        <span class="n">_theta_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">_theta_xs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                <span class="n">_theta_xs</span> <span class="o">=</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">T</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">_theta_xs</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                <span class="n">_theta_xs</span> <span class="o">=</span> <span class="n">_theta_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Use the pre-computed GP (this is fast since gp.compute() was already called)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_var</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">_ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_iter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_cond</span><span class="p">,</span> <span class="n">_theta_xs</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="c1"># Return single value if input was 1D, otherwise return array</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_ypred</span><span class="p">,</span> <span class="n">_varpred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_iter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_cond</span><span class="p">,</span> <span class="n">_theta_xs</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="c1"># Variance transformation: variance scales as scale_factor² for linear transforms</span>
            <span class="c1"># For FunctionTransformer (like nlog_scaler), we need to handle this carefully</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">scale_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># For StandardScaler or similar: var_unscaled = scale_factor² × var_scaled  </span>
                <span class="n">var_scale_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">scale_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">varpred</span> <span class="o">=</span> <span class="n">_varpred</span> <span class="o">*</span> <span class="n">var_scale_factor</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># For FunctionTransformer or other scalers, compute numerical derivative</span>
                <span class="c1"># This is more accurate than using inverse_transform on variance</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Use a small epsilon to estimate the derivative</span>
                    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
                    <span class="n">test_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">eps</span><span class="p">]])</span>
                    <span class="n">transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">test_vals</span><span class="p">)</span>
                    <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">transformed</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">eps</span>
                    <span class="n">varpred</span> <span class="o">=</span> <span class="n">_varpred</span> <span class="o">*</span> <span class="p">(</span><span class="n">scale_factor</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="c1"># Fallback: use absolute value of incorrect transform to ensure positivity</span>
                    <span class="n">varpred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_varpred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

            <span class="c1"># Return single values if input was 1D, otherwise return arrays</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">varpred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">varpred</span></div>

            

<div class="viewcode-block" id="SurrogateModel">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SurrogateModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Process surrogate model for Bayesian inference and optimization.</span>
<span class="sd">    </span>
<span class="sd">    A SurrogateModel uses a Gaussian Process to create a fast approximation of expensive</span>
<span class="sd">    likelihood functions, enabling efficient Bayesian inference, parameter estimation,</span>
<span class="sd">    and active learning. The model supports various active learning algorithms and </span>
<span class="sd">    scalers for handling different types of likelihood functions.</span>

<span class="sd">    :param lnlike_fn: (*callable, required*)</span>
<span class="sd">        Log-likelihood function that takes parameter array theta and returns scalar </span>
<span class="sd">        log-likelihood value. For Bayesian inference, this is your model&#39;s log-likelihood.</span>
<span class="sd">        Signature: lnlike_fn(theta) -&gt; float</span>
<span class="sd">        </span>
<span class="sd">    :param bounds: (*array-like, required*)</span>
<span class="sd">        Prior bounds for each parameter. List/array of (min, max) tuples for each dimension.</span>
<span class="sd">        Example: bounds = [(0, 1), (2, 3), (-1, 1)]</span>
<span class="sd">        </span>
<span class="sd">    :param param_names: (*array-like, optional*)</span>
<span class="sd">        Names/labels for each parameter. If None, defaults to θ₀, θ₁, etc.</span>
<span class="sd">        Length must match number of dimensions in bounds.</span>
<span class="sd">        </span>
<span class="sd">    :param cache: (*bool, optional, default=True*)</span>
<span class="sd">        Whether to cache the trained model to disk for reuse</span>
<span class="sd">        </span>
<span class="sd">    :param savedir: (*str, optional, default=&quot;results/&quot;*)</span>
<span class="sd">        Directory for saving results, plots, and cached models</span>
<span class="sd">        </span>
<span class="sd">    :param model_name: (*str, optional, default=&quot;surrogate_model&quot;*)</span>
<span class="sd">        Name prefix for cached model files</span>
<span class="sd">        </span>
<span class="sd">    :param verbose: (*bool, optional, default=True*)</span>
<span class="sd">        Print progress information during training and inference</span>
<span class="sd">        </span>
<span class="sd">    :param ncore: (*int, optional, default=cpu_count()*)</span>
<span class="sd">        Number of CPU cores to use for parallel computation</span>
<span class="sd">        </span>
<span class="sd">    :param ignore_warnings: (*bool, optional, default=True*)</span>
<span class="sd">        Suppress sklearn and other package warnings</span>
<span class="sd">        </span>
<span class="sd">    :param random_state: (*int, optional, default=None*)</span>
<span class="sd">        Random seed for reproducible results</span>

<span class="sd">    .. attribute:: gp</span>
<span class="sd">        :type: george.GP</span>
<span class="sd">        </span>
<span class="sd">        Trained Gaussian Process model</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: bounds</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Original parameter bounds (unscaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _bounds</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Scaled parameter bounds used for GP training</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _theta</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Training parameter samples (scaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _y</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Training likelihood values (scaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: ntrain</span>
<span class="sd">        :type: int</span>
<span class="sd">        </span>
<span class="sd">        Number of initial training samples</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: ndim</span>
<span class="sd">        :type: int</span>
<span class="sd">        </span>
<span class="sd">        Number of parameters/dimensions</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: emcee_samples</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        MCMC samples from emcee (if run_emcee called)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: dynesty_samples</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Nested sampling results from dynesty (if run_dynesty called)</span>

<span class="sd">    **Examples**</span>
<span class="sd">    </span>
<span class="sd">    Basic usage for Bayesian inference:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        def log_likelihood(theta):</span>
<span class="sd">            # Your model likelihood function</span>
<span class="sd">            return -0.5 * np.sum((theta - 2)**2)</span>
<span class="sd">         </span>
<span class="sd">        bounds = [(0, 4), (0, 4)]  # 2D parameter space</span>
<span class="sd">        sm = SurrogateModel(log_likelihood, bounds)</span>
<span class="sd">        sm.init_samples(ntrain=100)  # Initial training data</span>
<span class="sd">        sm.init_gp()  # Initialize Gaussian Process</span>
<span class="sd">        sm.active_train(niter=50)  # Active learning</span>
<span class="sd">        sm.run_dynesty()  # Bayesian inference</span>
<span class="sd">    </span>
<span class="sd">    For optimization problems:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        sm.active_train(algorithm=&quot;jones&quot;)  # Use Jones algorithm for optimization</span>
<span class="sd">    </span>
<span class="sd">    .. seealso::</span>
<span class="sd">    </span>
<span class="sd">        :meth:`init_samples` : Initialize training data</span>
<span class="sd">        :meth:`init_gp` : Initialize Gaussian Process</span>
<span class="sd">        :meth:`active_train` : Perform active learning</span>
<span class="sd">        :meth:`run_dynesty` : Run nested sampling with dynesty</span>
<span class="sd">        :meth:`run_emcee` : Run MCMC sampling with emcee</span>
<span class="sd">        :meth:`run_ultranest` : Run nested sampling with UltraNest</span>
<span class="sd">        :meth:`run_pymultinest` : Run nested sampling with PyMultiNest</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SurrogateModel.__init__">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lnlike_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">param_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                 <span class="n">cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">savedir</span><span class="o">=</span><span class="s2">&quot;results/&quot;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;surrogate_model&quot;</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pool_method</span><span class="o">=</span><span class="s2">&quot;forkserver&quot;</span><span class="p">,</span> <span class="n">ignore_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="c1"># Check all required inputs are specified</span>
        <span class="k">if</span> <span class="n">lnlike_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must supply lnlike_fn to train GP surrogate model.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must supply prior bounds.&quot;</span><span class="p">)</span>

        <span class="c1"># Set random seed for reproducibility</span>
        <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use a time-based seed to avoid clustering when called repeatedly</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">*</span> <span class="mi">1000000</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

        <span class="c1"># Set function for training the GP, and initial training samples</span>
        <span class="c1"># For bayesian inference problem this would be your log likelihood function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lnlike_fn</span> <span class="o">=</span> <span class="n">lnlike_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span> <span class="o">=</span> <span class="n">lnlike_fn</span>

        <span class="c1"># unscaled bounds for theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bounds</span><span class="p">)</span>

        <span class="c1"># define prior sampler with unscaled bounds </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_sampler</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Determine dimensionality </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
        
        <span class="c1"># Set parameter names</span>
        <span class="k">if</span> <span class="n">param_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">bounds</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Length of param_names must match length of bounds.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names</span> <span class="o">=</span> <span class="n">param_names</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">param_names</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$\theta_</span><span class="si">%s</span><span class="s2">$&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;theta_</span><span class="si">{i}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>

        <span class="c1"># Cache surrogate model as pickle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="n">cache</span> 

        <span class="c1"># Directory to save results and plots; defaults to local dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">savedir</span> <span class="o">=</span> <span class="n">savedir</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">)</span>

        <span class="c1"># Name of model cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>

        <span class="c1"># Print progress statements</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        
        <span class="c1"># Ignore warnings</span>
        <span class="k">if</span> <span class="n">ignore_warnings</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>

        <span class="c1"># Number of cores alabi is allowed to use</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">=</span> <span class="n">pool_method</span>
        <span class="k">if</span> <span class="n">ncore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ncore</span><span class="p">,</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">())</span>
            
        <span class="c1"># Check if MPI is active</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mpi_is_active</span> <span class="o">=</span> <span class="n">parallel_utils</span><span class="o">.</span><span class="n">is_mpi_active</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mpi_is_active</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">mpi4py.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">MPIPoolExecutor</span>

        <span class="c1"># false if emcee, dynesty, and ultranest have not been run for this object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_run</span> <span class="o">=</span> <span class="kc">False</span></div>

        
    
    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="c1"># If self.gp is the problematic object, we need to handle it</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="s1">&#39;__dict__&#39;</span><span class="p">):</span>
            <span class="c1"># Try to find and remove unpickleable attributes from gp</span>
            <span class="n">gp_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">cleaned_gp_dict</span> <span class="o">=</span> <span class="p">{}</span>
            
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gp_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="n">cleaned_gp_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removing unpickleable attribute from gp: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_pool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">ncore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">ncore</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mpi_is_active</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">MPIPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">ncore</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pool</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span><span class="p">)</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">processes</span><span class="o">=</span><span class="n">ncore</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pool</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_close_pool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mpi_is_active</span><span class="p">:</span>
            <span class="n">pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pool</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="n">pool</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="SurrogateModel.save">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.save">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pickle ``SurrogateModel`` object and write summary to a text file.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>

        <span class="c1"># pickle surrogate model object</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Caching model to </span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        
        <span class="c1"># Create a temporary file first, then rename (atomic operation)</span>
        <span class="n">temp_file</span> <span class="o">=</span> <span class="n">file</span> <span class="o">+</span> <span class="s2">&quot;.pkl.tmp&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">temp_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>        
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
            <span class="c1"># Atomic rename to prevent corruption</span>
            <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">temp_file</span><span class="p">,</span> <span class="n">file</span> <span class="o">+</span> <span class="s2">&quot;.pkl&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Clean up temp file if something went wrong</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">temp_file</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">temp_file</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error writing GP report: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_emcee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_dynesty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span></div>


            
    <span class="k">def</span><span class="w"> </span><span class="nf">_lnlike_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_theta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal function to evaluate the model function ``lnlike_fn`` at scaled theta.</span>
<span class="sd">        This is used to avoid scaling the theta in the main function call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Unscale theta</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        
        <span class="c1"># Evaluate function</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Scale y - ensure y is a numpy array</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">_y</span>
            
<div class="viewcode-block" id="SurrogateModel.theta">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.theta">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">theta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return unscaled training theta values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SurrogateModel.y">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.y">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">y</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return unscaled training y values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span></div>

    
    
<div class="viewcode-block" id="SurrogateModel.refit_scalers">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.refit_scalers">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">refit_scalers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta_scaler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_scaler</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Refit the theta and y scalers using current training data.</span>
<span class="sd">        Useful if training data has changed significantly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">theta_scaler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span> <span class="o">=</span> <span class="n">theta_scaler</span>

        <span class="k">if</span> <span class="n">y_scaler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span> <span class="o">=</span> <span class="n">y_scaler</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        
        <span class="n">y_2d</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_2d</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_theta</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Refitted theta_scaler produced NaN values!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_theta</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Refitted theta_scaler produced Inf values!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_y</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Refitted y_scaler produced NaN values!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_y</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Refitted y_scaler produced Inf values!&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span></div>



<div class="viewcode-block" id="SurrogateModel.init_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;initial_training_sample.npz&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param nsample: (*int, optional*) </span>
<span class="sd">            Number of samples. Defaults to ``nsample = 50 * self.ndim``</span>

<span class="sd">        :param sampler: (*str, optional*) </span>
<span class="sd">            Sampling method. Defaults to ``&#39;sobol&#39;``. </span>
<span class="sd">            See ``utility.prior_sampler`` for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">nsample</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nsample</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>

        <span class="c1"># note: initial samples should be drawn uniformly in scaled space</span>
        <span class="c1"># if theta_scaler is a non-linear transform, then samples in real space will be non-uniform</span>
        <span class="c1"># _theta = self._prior_sampler(nsample=nsample, sampler=sampler, random_state=None)</span>
        <span class="c1"># theta = self.theta_scaler.inverse_transform(_theta)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">nsample</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> 
        
        <span class="c1"># create pool for parallel evaluation of likelihood function</span>
        <span class="n">pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span>  
        
        <span class="c1"># evaluate initial samples in parallel or sequential</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
                <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
                <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
            <span class="p">))</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span> <span class="k">for</span> <span class="n">tt</span> <span class="ow">in</span> <span class="n">theta</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="c1"># close init_train pool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_close_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
        
        <span class="c1"># replace any nan or inf values </span>
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">]):</span>
                <span class="n">ynan</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">while</span> <span class="n">ynan</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="c1"># resample theta</span>
                    <span class="n">new_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">theta</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_theta</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">])):</span>
                        <span class="n">ynan</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span></div>



<div class="viewcode-block" id="SurrogateModel.load_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.load_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache_file</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reload training samples from cache file and apply scalers.</span>
<span class="sd">        </span>
<span class="sd">        :param cache_file: (*str, required*) </span>
<span class="sd">            Name of cache file relative to savedir. Must be a .npz file containing &#39;theta&#39; and &#39;y&#39; arrays.</span>

<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            Scaled training samples (_theta, _y) after loading from cache.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">sims</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">sims</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dimension of bounds (n=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">) does not </span><span class="se">\</span>
<span class="s2">                              match dimension of training theta (n=</span><span class="si">{</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">y</span></div>



<div class="viewcode-block" id="SurrogateModel.init_samples">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_samples">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntrain</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ntest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">train_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize training and test samples for the surrogate model.</span>
<span class="sd">        </span>
<span class="sd">        Creates initial dataset by either loading cached samples or computing new ones</span>
<span class="sd">        by evaluating the likelihood function at randomly sampled parameter values.</span>
<span class="sd">        </span>
<span class="sd">        :param ntrain: (*int, optional, default=100*)</span>
<span class="sd">            Number of training samples to generate. Used only if not loading cached samples.</span>
<span class="sd">            </span>
<span class="sd">        :param ntest: (*int, optional, default=0*)</span>
<span class="sd">            Number of test samples to generate. Currently unused.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler: (*str, optional, default=&quot;uniform&quot;*)</span>
<span class="sd">            Sampling method for generating parameter values. Options:</span>
<span class="sd">            </span>
<span class="sd">            - &quot;uniform&quot;: Uniform sampling within bounds (default)</span>
<span class="sd">            - &quot;sobol&quot;: Low-discrepancy Sobol sequence sampling</span>
<span class="sd">            - &quot;lhs&quot;: Latin hypercube sampling</span>
<span class="sd">            </span>
<span class="sd">        :param train_file: (*str, optional, default=&quot;initial_training_sample.npz&quot;*)</span>
<span class="sd">            Filename for cached training samples relative to savedir.</span>
<span class="sd">            Format: .npz file containing &#39;theta&#39; and &#39;y&#39; arrays.</span>
<span class="sd">            </span>
<span class="sd">        :param test_file: (*str, optional, default=&quot;initial_test_sample.npz&quot;*)</span>
<span class="sd">            Filename for cached test samples relative to savedir. Currently unused.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Load or create training sample</span>
        <span class="k">if</span> <span class="n">train_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># check if file path exists</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">train_file</span><span class="p">):</span>
                <span class="n">cache_file</span> <span class="o">=</span> <span class="n">train_file</span>
            <span class="c1"># if not, check if it exists in the savedir</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">train_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
                <span class="n">cache_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">train_file</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">theta</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_train</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="si">}</span><span class="s2"> train samples from </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unable to reload </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2"> due to error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Computing new samples with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>
                <span class="n">theta</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntrain</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">train_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">train_file</span><span class="o">=</span><span class="s2">&quot;initial_train_file_sample.npz&quot;</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntrain</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">train_file</span><span class="p">)</span>
            
        <span class="c1"># --------------------------------------------------------</span>
        <span class="c1"># Load or create test sample</span>
        <span class="k">if</span> <span class="n">ntest</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">test_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># check if file path exists</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">test_file</span><span class="p">):</span>
                    <span class="n">cache_file</span> <span class="o">=</span> <span class="n">test_file</span>
                <span class="c1"># if not, check if it exists in the savedir</span>
                <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">test_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
                    <span class="n">cache_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">test_file</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">theta_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_train</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">theta_test</span><span class="p">)</span><span class="si">}</span><span class="s2"> test samples from </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unable to reload </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2"> due to error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Computing new samples with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>
                    <span class="n">theta_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntest</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">test_file</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">test_file</span><span class="o">=</span><span class="s2">&quot;initial_test_sample.npz&quot;</span>
                <span class="n">theta_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntest</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">test_file</span><span class="p">)</span>

            <span class="c1"># Save test dataset</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_test</span> <span class="o">=</span> <span class="n">theta_test</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntest</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_test</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntest</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Save initial training sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_train</span> <span class="o">=</span> <span class="n">theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>
        
        <span class="c1"># record number of training samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="mi">0</span></div>

            

<div class="viewcode-block" id="SurrogateModel.set_hyperparam_prior_bounds">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.set_hyperparam_prior_bounds">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_hyperparam_prior_bounds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure prior bounds for GP hyperparameters based on current training data.</span>
<span class="sd">        </span>
<span class="sd">        By default ranges for parameters:</span>
<span class="sd">            - mean: [mean(y) - std(y), mean(y) + std(y)]</span>
<span class="sd">            - amplitude: [0.1, 10]</span>
<span class="sd">            - white noise: [white_noise - 3, white_noise + 3]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Configure GP hyperparameter prior</span>
        <span class="c1"># hp_bounds = self.gp.get_parameter_bounds()</span>
        <span class="c1"># pnames = self.gp.get_parameter_names(include_frozen=False)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">pnames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pnames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span>
        <span class="n">hp_bounds</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pnames</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">:</span>
            <span class="n">mean_y</span><span class="p">,</span> <span class="n">std_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>
            <span class="n">mean_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean_y</span> <span class="o">-</span> <span class="n">std_y</span><span class="p">,</span> <span class="n">mean_y</span> <span class="o">+</span> <span class="n">std_y</span><span class="p">]</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;mean:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">mean_bounds</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">:</span>
            <span class="n">amp_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_amp_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_amp_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> 
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_amp_key</span><span class="si">}</span><span class="s2">:log_constant&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">amp_bounds</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">:</span>
            <span class="n">wn_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;white_noise:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">wn_bounds</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_scale_rng</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
                <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_scale_rng</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hp_bounds</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_hyper_prior</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">lnprior_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">)</span></div>

        
    
<div class="viewcode-block" id="SurrogateModel.expand_hyperparameter_vector">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.expand_hyperparameter_vector">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">expand_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimized_params</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">optimized_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;optimized_params cannot be None&quot;</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">optimized_params</span>
        
        <span class="n">full_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">:</span>
            <span class="n">full_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;mean:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">optimized_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;mean:value&quot;</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">:</span>
            <span class="n">full_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_amp_key</span><span class="si">}</span><span class="s2">:log_constant&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">optimized_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_amp_key</span><span class="si">}</span><span class="s2">:log_constant&quot;</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">:</span>
            <span class="n">full_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;white_noise:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">optimized_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;white_noise:value&quot;</span><span class="p">)]</span>
        
        <span class="c1"># if self.uniform_scales == True:  use same scale length for all dimensions</span>
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
            <span class="n">full_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">optimized_params</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M&quot;</span><span class="p">)]</span>
   
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">full_params</span><span class="p">)</span></div>

        
        
<div class="viewcode-block" id="SurrogateModel.set_hyperparameter_vector">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.set_hyperparameter_vector">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tmp_gp</span><span class="p">,</span> <span class="n">optimized_params</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">optimized_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;optimized_params cannot be None. Cannot set hyperparameters.&quot;</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">full_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">optimized_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_params</span> <span class="o">=</span> <span class="n">optimized_params</span>
                
        <span class="n">tmp_gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">full_params</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">tmp_gp</span></div>

    
    
<div class="viewcode-block" id="SurrogateModel.get_hyperparameter_dict">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.get_hyperparameter_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_hyperparameter_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gp</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get current GP hyperparameters as a dictionary.</span>
<span class="sd">        </span>
<span class="sd">        :returns: (*dict*) </span>
<span class="sd">            Dictionary of current GP hyperparameters with names as keys and values as values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">hp_dict</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_dict</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">hp_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hp_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M_0_0&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
                <span class="k">del</span> <span class="n">hp_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">hp_dict</span></div>

    
    
<div class="viewcode-block" id="SurrogateModel.get_hyperparameter_vector">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.get_hyperparameter_vector">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gp</span><span class="p">):</span>
        
        <span class="n">hp_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hyperparameter_dict</span><span class="p">(</span><span class="n">gp</span><span class="p">)</span>
        <span class="n">hp_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="n">hp_dict</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">hp_vector</span></div>


        
<div class="viewcode-block" id="SurrogateModel.init_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_gp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ExpSquaredKernel&quot;</span><span class="p">,</span>
                <span class="n">fit_amp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">fit_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">fit_white_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">white_noise</span><span class="o">=-</span><span class="mi">12</span><span class="p">,</span> 
                <span class="n">gp_scale_rng</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">gp_amp_rng</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">uniform_scales</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">theta_scaler</span><span class="o">=</span><span class="n">ut</span><span class="o">.</span><span class="n">no_scaler</span><span class="p">,</span>
                <span class="n">y_scaler</span><span class="o">=</span><span class="n">ut</span><span class="o">.</span><span class="n">no_scaler</span><span class="p">,</span>
                <span class="n">gp_opt_method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span> 
                <span class="n">gp_nopt</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;xatol&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="s2">&quot;fatol&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s2">&quot;adaptive&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
                <span class="n">hyperopt_method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
                <span class="n">regularize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">amp_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">mu_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">sigma_0</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                <span class="n">cv_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                <span class="n">cv_scoring</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
                <span class="n">cv_n_candidates</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">cv_stage2_candidates</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                <span class="n">cv_stage2_width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                <span class="n">cv_stage3_candidates</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                <span class="n">cv_stage3_width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                <span class="n">cv_weighted_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">multi_proc</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Gaussian Process surrogate model with specified kernel and hyperparameters.</span>

<span class="sd">        This function sets up a Gaussian Process (GP) using the george library with the specified </span>
<span class="sd">        kernel type and configuration. The GP is initialized with random scale lengths and then</span>
<span class="sd">        fitted to the current training data.</span>

<span class="sd">        :param kernel: (*str or george kernel object, optional*) </span>
<span class="sd">            Kernel type for the Gaussian Process. Can be either a string specifying one of the </span>
<span class="sd">            built-in kernels or a george kernel object. Default is &quot;ExpSquaredKernel&quot;.</span>
<span class="sd">            </span>
<span class="sd">            Built-in options:</span>
<span class="sd">                - ``&#39;ExpSquaredKernel&#39;``: Squared exponential (RBF) kernel, smooth functions</span>
<span class="sd">                - ``&#39;Matern32Kernel&#39;``: Matérn kernel with ν=3/2, moderately smooth functions  </span>
<span class="sd">                - ``&#39;Matern52Kernel&#39;``: Matérn kernel with ν=5/2, smooth functions</span>
<span class="sd">                - ``&#39;RationalQuadraticKernel&#39;``: Rational quadratic kernel, scale mixture of RBF kernels</span>
<span class="sd">                </span>
<span class="sd">            See https://george.readthedocs.io/en/latest/user/kernels/ for more details.</span>

<span class="sd">        :param fit_amp: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the amplitude (overall scale) hyperparameter of the kernel.</span>
<span class="sd">            If True, the GP will learn the optimal amplitude from data. Default is True.</span>

<span class="sd">        :param fit_mean: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the mean function hyperparameter. If True, the GP will learn</span>
<span class="sd">            a constant mean offset. If False, assumes zero mean. Default is True.</span>

<span class="sd">        :param fit_white_noise: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the white noise (nugget) hyperparameter. If True, the GP will</span>
<span class="sd">            learn the optimal noise level. If False, uses the fixed value from white_noise.</span>
<span class="sd">            Default is True.</span>

<span class="sd">        :param white_noise: (*float, optional*) </span>
<span class="sd">            Log-scale white noise parameter. If fit_white_noise=False, this fixed value is used.</span>
<span class="sd">            If fit_white_noise=True, this serves as the initial guess. Typical values are </span>
<span class="sd">            between -15 (very low noise) and -5 (high noise). Default is -12.</span>

<span class="sd">        :param gp_scale_rng: (*list of two floats, optional*) </span>
<span class="sd">            Log-scale bounds for the characteristic length scale parameters of the kernel.</span>
<span class="sd">            Format: [log_min_scale, log_max_scale]. These bounds apply to all input dimensions.</span>
<span class="sd">            Default is [-2, 2], corresponding to scales between ~0.14 and ~7.4 in original units.</span>
<span class="sd">            </span>
<span class="sd">        :param uniform_scales: (*bool, optional*) </span>
<span class="sd">            If True, the same scale length will be used for all input dimensions. If False, each dimension</span>
<span class="sd">            will have its own independent scale length. Default is False.</span>

<span class="sd">        :param overwrite: (*bool, optional*) </span>
<span class="sd">            If True, allows reinitializing the GP even if one already exists. If False and a GP</span>
<span class="sd">            already exists, raises an AssertionError. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param theta_scaler: (*sklearn transformer, optional, default=no_scaler*)</span>
<span class="sd">            Scaler for input parameters. Applied to theta values before GP training.</span>
<span class="sd">            Common options: MinMaxScaler() (scale to [0,1]) or StandardScaler()</span>
<span class="sd">            </span>
<span class="sd">        :param y_scaler: (*sklearn transformer, optional, default=no_scaler*)</span>
<span class="sd">            Scaler for output values (log-likelihoods). Options include:</span>
<span class="sd">            - no_scaler: No scaling (default)</span>
<span class="sd">            - minmax_scaler: Scale to [0,1] </span>
<span class="sd">            - nlog_scaler: Apply -log10(-y) transformation for negative log-likelihoods</span>
<span class="sd">            - log_scaler: Apply log10(y) for positive values</span>

<span class="sd">        :param gp_opt_method: (*str, optional*) </span>
<span class="sd">            Optimization method for GP hyperparameter optimization. Passed to scipy.optimize.minimize.</span>
<span class="sd">            Common options: &#39;l-bfgs-b&#39;, &#39;newton-cg&#39;, &#39;bfgs&#39;, &#39;cg&#39;. Default is &#39;l-bfgs-b&#39;.</span>

<span class="sd">        :param gp_nopt: (*int, optional*) </span>
<span class="sd">            Number of optimization restarts for GP hyperparameter optimization. Multiple restarts</span>
<span class="sd">            help avoid local minima. Default is 3.</span>

<span class="sd">        :param optimizer_kwargs: (*dict, optional*) </span>
<span class="sd">            Additional keyword arguments passed to the scipy optimizer. Common options include</span>
<span class="sd">            &#39;maxiter&#39; (maximum iterations) and convergence tolerances. </span>
<span class="sd">            Default is {&quot;maxiter&quot;: 50}.</span>

<span class="sd">        :param hyperopt_method: (*str, optional, default=&#39;ml&#39;*)</span>
<span class="sd">            Method for optimizing GP hyperparameters:</span>
<span class="sd">            </span>
<span class="sd">            - &#39;ml&#39;: Maximum marginal likelihood (fast, may overfit)</span>
<span class="sd">            - &#39;cv&#39;: k-fold cross-validation (slower, prevents overfitting)</span>

<span class="sd">        :param cv_folds: (*int, optional, default=5*)</span>
<span class="sd">            Number of folds for cross-validation (only used if hyperopt_method=&#39;cv&#39;).</span>

<span class="sd">        :param cv_scoring: (*str, optional, default=&#39;mse&#39;*)</span>
<span class="sd">            Scoring metric for cross-validation. Options: &#39;mse&#39;, &#39;mae&#39;, &#39;r2&#39;.</span>

<span class="sd">        :param cv_n_candidates: (*int, optional, default=20*)</span>
<span class="sd">            Number of hyperparameter candidates to evaluate for CV.</span>

<span class="sd">        :param cv_stage2_candidates: (*int, optional, default=20*)</span>
<span class="sd">            Number of candidates for stage 2 grid search. Only used when cv_two_stage=True.</span>

<span class="sd">        :param cv_stage2_width: (*float, optional, default=0.3*)</span>
<span class="sd">            Width factor for stage 2 search around best parameters from stage 1.</span>
<span class="sd">            Smaller values = tighter search. Only used when cv_two_stage=True.</span>

<span class="sd">        :param cv_stage3_candidates: (*int, optional, default=None*)</span>
<span class="sd">            Number of candidates for stage 3 ultra-fine search. If None, uses </span>
<span class="sd">            max(cv_stage2_candidates // 2, 3). Only used when cv_three_stage=True.</span>

<span class="sd">        :param cv_stage3_width: (*float, optional, default=0.2*)</span>
<span class="sd">            Width factor for stage 3 search around best parameters from stage 2.</span>
<span class="sd">            Should be smaller than cv_stage2_width for finer refinement.</span>
<span class="sd">            Only used when cv_three_stage=True.</span>

<span class="sd">        :raises AssertionError: </span>
<span class="sd">            If a GP already exists and overwrite=False.</span>
<span class="sd">        :raises ValueError: </span>
<span class="sd">            If an invalid kernel name is provided.</span>
<span class="sd">        :raises Exception: </span>
<span class="sd">            If GP initialization fails after multiple attempts with different scale lengths.</span>

<span class="sd">        .. note:: </span>
<span class="sd">        </span>
<span class="sd">            This function must be called after init_samples() since it requires training data</span>
<span class="sd">            to initialize the GP. The function will automatically retry initialization with</span>
<span class="sd">            different random scale lengths if the initial attempt fails.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            &gt;&gt;&gt; # Basic initialization with default settings</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp()</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # Custom kernel with specific hyperparameter settings</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp(kernel=&quot;Matern52Kernel&quot;, </span>
<span class="sd">            ...            fit_white_noise=False, </span>
<span class="sd">            ...            white_noise=-10,</span>
<span class="sd">            ...            gp_scale_rng=[-1, 1])</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # High-precision optimization</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp(gp_opt_method=&quot;l-bfgs-b&quot;,</span>
<span class="sd">            ...            gp_nopt=5,</span>
<span class="sd">            ...            optimizer_kwargs={&quot;maxiter&quot;: 100, &quot;ftol&quot;: 1e-9})</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">overwrite</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;GP kernel already assigned. Use overwrite=True to re-assign the kernel.&quot;</span><span class="p">)</span>
            
        <span class="c1"># optional hyperparameter choices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span> <span class="o">=</span> <span class="n">fit_amp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span> <span class="o">=</span> <span class="n">fit_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span> <span class="o">=</span> <span class="n">fit_white_noise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">=</span> <span class="n">white_noise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span> <span class="o">=</span> <span class="n">uniform_scales</span>

        <span class="c1"># GP hyperparameter optimization method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_method</span> <span class="o">=</span> <span class="n">gp_opt_method</span>

        <span class="c1"># GP hyperparameter number of opt restarts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_nopt</span> <span class="o">=</span> <span class="n">gp_nopt</span>

        <span class="c1"># Save all kwargs needed for opt_gp function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;hyperopt_method&quot;</span><span class="p">:</span> <span class="n">hyperopt_method</span><span class="p">,</span>
                              <span class="s2">&quot;regularize&quot;</span><span class="p">:</span> <span class="n">regularize</span><span class="p">,</span>
                              <span class="s2">&quot;amp_0&quot;</span><span class="p">:</span> <span class="n">amp_0</span><span class="p">,</span>
                              <span class="s2">&quot;mu_0&quot;</span><span class="p">:</span> <span class="n">mu_0</span><span class="p">,</span>
                              <span class="s2">&quot;sigma_0&quot;</span><span class="p">:</span> <span class="n">sigma_0</span><span class="p">,</span>
                              <span class="s2">&quot;optimizer_kwargs&quot;</span><span class="p">:</span> <span class="n">optimizer_kwargs</span><span class="p">,</span>
                              <span class="s2">&quot;cv_folds&quot;</span><span class="p">:</span> <span class="n">cv_folds</span><span class="p">,</span>
                              <span class="s2">&quot;cv_scoring&quot;</span><span class="p">:</span> <span class="n">cv_scoring</span><span class="p">,</span>
                              <span class="s2">&quot;cv_n_candidates&quot;</span><span class="p">:</span> <span class="n">cv_n_candidates</span><span class="p">,</span>
                              <span class="s2">&quot;cv_stage2_candidates&quot;</span><span class="p">:</span> <span class="n">cv_stage2_candidates</span><span class="p">,</span>
                              <span class="s2">&quot;cv_stage2_width&quot;</span><span class="p">:</span> <span class="n">cv_stage2_width</span><span class="p">,</span>
                              <span class="s2">&quot;cv_stage3_candidates&quot;</span><span class="p">:</span> <span class="n">cv_stage3_candidates</span><span class="p">,</span>
                              <span class="s2">&quot;cv_stage3_width&quot;</span><span class="p">:</span> <span class="n">cv_stage3_width</span><span class="p">,</span>
                              <span class="s2">&quot;cv_weighted_factor&quot;</span><span class="p">:</span> <span class="n">cv_weighted_factor</span><span class="p">,</span>
                              <span class="s2">&quot;multi_proc&quot;</span><span class="p">:</span> <span class="n">multi_proc</span><span class="p">,</span>
                              <span class="p">}</span>
        
        <span class="c1"># -------------------------------------------------------------------------</span>
        <span class="c1"># define scaling functions</span>
        <span class="c1"># Scale inputs between 0 and 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span> <span class="o">=</span> <span class="n">theta_scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Scale bounds to [0, 1] for training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Output scaling function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span> <span class="o">=</span> <span class="n">y_scaler</span>
        
        <span class="c1"># Fit scalers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit_scalers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
        
        <span class="c1"># Save scaled training data for GP fitting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_test</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                 <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;acquisition_optimizer_niter&quot;</span> <span class="p">:</span> <span class="p">[]</span>
                                 <span class="p">}</span>
        
        <span class="c1"># -------------------------------------------------------------------------</span>
        <span class="c1"># set the bounds for scale length parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_scale_rng</span> <span class="o">=</span> <span class="n">gp_scale_rng</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_amp_rng</span> <span class="o">=</span> <span class="n">gp_amp_rng</span>
        
        <span class="c1"># metric_bounds expects log-scale bounds</span>
        <span class="n">log_metric_bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">metric_bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>

        <span class="n">valid_scales</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">max_attempts</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Prevent infinite loops</span>
        <span class="n">attempt</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">while</span> <span class="n">valid_scales</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_attempts</span><span class="p">:</span>
            <span class="n">attempt</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Generate initial scale length in linear scale (metric parameter expects linear scale)</span>
            <span class="c1"># gp_scale_rng is in log scale, so convert to linear scale for initial guess</span>
            <span class="n">log_initial_lscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
            <span class="n">initial_lscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_initial_lscale</span><span class="p">)</span>
            
            <span class="c1"># Note: metric is linear scale, but metric_bounds are log scale!</span>
            <span class="c1"># https://github.com/dfm/george/issues/150</span>
            
            <span class="c1"># Initialize GP kernel</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Stationary kernels</span>
                <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;ExpSquaredKernel&quot;</span><span class="p">:</span>
                    <span class="c1"># Guess initial metric, or scale length of the covariances (must be &gt; 0)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSquaredKernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with squared exponential kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;RationalQuadraticKernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">RationalQuadraticKernel</span><span class="p">(</span><span class="n">log_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with rational quadratic kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;Matern32Kernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern32Kernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with Matérn-3/2 kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;Matern52Kernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern52Kernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with Matérn-5/2 kernel.&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; is not a valid option. Valid options: ExpSquaredKernel, Matern32Kernel, Matern52Kernel, RationalQuadraticKernel&quot;</span><span class="p">)</span>
                
                <span class="c1"># create GP first time </span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">configure_gp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                                <span class="n">fit_amp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">,</span> 
                                                <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span>
                                                <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">,</span>
                                                <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: configure_gp returned None.&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data shape: theta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, y=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scale lengths: </span><span class="si">{</span><span class="n">initial_lscale</span><span class="si">}</span><span class="s2"> (log: </span><span class="si">{</span><span class="n">log_initial_lscale</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scale bounds: </span><span class="si">{</span><span class="n">metric_bounds</span><span class="si">}</span><span class="s2"> (log: </span><span class="si">{</span><span class="n">log_metric_bounds</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Retrying with new initial scale length...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">valid_scales</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully initialized GP on attempt </span><span class="si">{</span><span class="n">attempt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception during GP initialization on attempt </span><span class="si">{</span><span class="n">attempt</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Kernel: </span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Initial scales: </span><span class="si">{</span><span class="n">initial_lscale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Scale bounds: </span><span class="si">{</span><span class="n">log_metric_bounds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Data shape: theta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, y=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exception: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Retrying with new initial scale length...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">continue</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_scales</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize GP after </span><span class="si">{</span><span class="n">max_attempts</span><span class="si">}</span><span class="s2"> attempts. &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;Check your data, kernel choice, and scale bounds. &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;Current settings: kernel=</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">, gp_scale_rng=</span><span class="si">{</span><span class="n">gp_scale_rng</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
        <span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">(</span><span class="n">include_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;metric:log_M&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:metric:log_M&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">fit_mean</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;mean:value&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fit_amp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_amp_key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;log_constant&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:log_constant&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_amp_key</span><span class="si">}</span><span class="s2">:log_constant&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fit_white_noise</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;white_noise:value&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
        <span class="c1"># Infer lengthscale indices (used if regularization is enabled)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hp_other_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_full</span><span class="p">):</span>
            <span class="c1"># Common patterns for lengthscale parameters in george kernels</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">pattern</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;metric:log_m&quot;</span><span class="p">]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hp_other_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
            <span class="c1"># Only one lengthscale parameter when uniform scales are used</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_index</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_names_optimized</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_scale_key</span><span class="si">}</span><span class="s2">:metric:log_M&quot;</span><span class="p">)]</span>
            
        <span class="c1"># record initial hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_gp_hyperparameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">)</span>
        
        <span class="c1"># Optimize GP hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_gp</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta_test&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y_test&quot;</span><span class="p">):</span>
            <span class="n">_ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ytest</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">ytest_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ytest_true</span> <span class="o">-</span> <span class="n">ytest</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">test_mse</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div>

        
        
    <span class="k">def</span><span class="w"> </span><span class="nf">_fit_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_theta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit Gaussian Process to training data with current hyperparameters.</span>
<span class="sd">        </span>
<span class="sd">        :param _theta: (*array, optional*) </span>
<span class="sd">            Scaled training parameter samples. If None, uses ``self._theta``.</span>
<span class="sd">            </span>
<span class="sd">        :param _y: (*array, optional*) </span>
<span class="sd">            Scaled training output values. If None, uses ``self._y``.</span>
<span class="sd">            </span>
<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            - gp: Fitted george.GP object</span>
<span class="sd">            - timing: Time taken to fit the GP in seconds</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">_theta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>

        <span class="k">if</span> <span class="n">_y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparam_prior_bounds</span><span class="p">()</span>

        <span class="c1"># Validate input data</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_theta</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_theta contains NaN or Inf values&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_y</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_y contains NaN or Inf values: </span><span class="si">{</span><span class="n">_y</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_y</span><span class="p">)]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">y_median</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span>
        <span class="n">y_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">y_median</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;median(_y) is not finite: </span><span class="si">{</span><span class="n">y_median</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span> <span class="ow">or</span> <span class="n">y_var</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;var(_y) is not finite or zero: </span><span class="si">{</span><span class="n">y_var</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">*</span> <span class="n">y_var</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span>

        <span class="n">gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">y_median</span><span class="p">,</span>
                    <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">,</span> <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">)</span>

        <span class="c1"># Check if hyperparameters contain NaN or Inf</span>
        <span class="k">if</span> <span class="n">hyperparameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hyperparameters_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">hyperparameters</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">hyperparameters_array</span><span class="p">)):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Hyperparameters contain NaN or Inf: </span><span class="si">{</span><span class="n">hyperparameters_array</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reoptimizing hyperparameters from scratch...&quot;</span><span class="p">)</span>
                <span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_gp</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="p">,</span> <span class="n">_theta</span><span class="o">=</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="n">_y</span><span class="p">)</span>
                <span class="c1"># Validate the reoptimized GP</span>
                <span class="n">reopt_params</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">reopt_params</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reoptimized GP still has invalid parameters: </span><span class="si">{</span><span class="n">reopt_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">gp</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
        
        <span class="n">gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">)</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">gp</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_opt_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperopt_method</span><span class="o">=</span><span class="s2">&quot;ml&quot;</span><span class="p">,</span> <span class="n">regularize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
               <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;xatol&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="s2">&quot;fatol&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s2">&quot;adaptive&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
               <span class="n">cv_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cv_scoring</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">cv_n_candidates</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">multi_proc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
               <span class="n">cv_stage2_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cv_stage2_width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cv_stage3_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cv_stage3_width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
               <span class="n">cv_weighted_mse_method</span><span class="o">=</span><span class="s2">&quot;exponential&quot;</span><span class="p">,</span> <span class="n">cv_weighted_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">_theta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">theta_scaler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_scaler</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize GP hyperparameters using marginal likelihood or cross-validation.</span>
<span class="sd">        </span>
<span class="sd">        :param method: (*str, optional, default=&quot;ml&quot;*)</span>
<span class="sd">            Optimization method to use:</span>
<span class="sd">            </span>
<span class="sd">            - &quot;ml&quot;: Maximum marginal likelihood (default, fast)</span>
<span class="sd">            - &quot;cv&quot;: k-fold cross-validation (slower but prevents overfitting)</span>
<span class="sd">            </span>
<span class="sd">        :param cv_folds: (*int, optional, default=5*)</span>
<span class="sd">            Number of folds for cross-validation (only used if method=&quot;cv&quot;)</span>
<span class="sd">            </span>
<span class="sd">        :param cv_scoring: (*str, optional, default=&#39;mse&#39;*)</span>
<span class="sd">            Scoring metric for cross-validation. Options: &#39;mse&#39;, &#39;mae&#39;, &#39;r2&#39;</span>
<span class="sd">            </span>
<span class="sd">        :param cv_n_candidates: (*int, optional, default=20*)</span>
<span class="sd">            Number of hyperparameter candidates to evaluate for CV</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional, default=True*)</span>
<span class="sd">            Whether to use multiprocessing for CV evaluation.</span>
<span class="sd">            Only used when method=&quot;cv&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_two_stage: (*bool, optional, default=False*)</span>
<span class="sd">            Whether to use two-stage CV optimization (explore-exploit strategy).</span>
<span class="sd">            Only used when method=&quot;cv&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_stage2_candidates: (*int, optional, default=None*)</span>
<span class="sd">            Number of candidates for stage 2 grid search. If None, uses</span>
<span class="sd">            cv_n_candidates // 2.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_stage2_width: (*float, optional, default=0.5*)</span>
<span class="sd">            Width factor for stage 2 search around best parameters.</span>
<span class="sd">            Smaller values = tighter search around best from stage 1.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_three_stage: (*bool, optional, default=False*)</span>
<span class="sd">            Whether to use three-stage CV optimization (explore-exploit-refine).</span>
<span class="sd">            Requires cv_two_stage=True. Only used when method=&quot;cv&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_stage3_candidates: (*int, optional, default=None*)</span>
<span class="sd">            Number of candidates for stage 3 ultra-fine search. If None, uses</span>
<span class="sd">            max(cv_stage2_candidates // 2, 3). Only used when cv_three_stage=True.</span>
<span class="sd">            </span>
<span class="sd">        :param cv_stage3_width: (*float, optional, default=0.2*)</span>
<span class="sd">            Width factor for stage 3 search around stage 2 best parameters.</span>
<span class="sd">            Should be smaller than cv_stage2_width for finer refinement.</span>
<span class="sd">            </span>
<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            - op_gp: Optimized george.GP object with updated hyperparameters</span>
<span class="sd">            - timing: Time taken for optimization in seconds</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="c1"># Use provided _theta and _y, or fall back to self._theta and self._y</span>
        <span class="k">if</span> <span class="n">_theta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
        <span class="k">if</span> <span class="n">_y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
        
        <span class="k">if</span> <span class="n">hyperopt_method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ml&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">]:</span>
            <span class="n">hyperopt_method</span> <span class="o">=</span> <span class="s2">&quot;ml&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid method &#39;</span><span class="si">{</span><span class="n">hyperopt_method</span><span class="si">}</span><span class="s2">&#39;. Must be &#39;ml&#39; or &#39;cv&#39;. Defaulting to &#39;ml&#39;.&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">,</span> <span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">]:</span>
            <span class="n">use_gradient</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparam_prior_bounds</span><span class="p">()</span>
            
        <span class="k">if</span> <span class="n">hyperopt_method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;ml&quot;</span><span class="p">:</span>
            <span class="n">current_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span>
            <span class="n">current_gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>

            <span class="c1"># Define the objective function (negative log-likelihood in this case).</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">nll</span><span class="p">(</span><span class="n">p_opt</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">p_opt</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">p_opt</span>
                <span class="n">tmp_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="n">current_gp</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                <span class="n">ll</span> <span class="o">=</span> <span class="o">-</span><span class="n">tmp_gp</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">regularize</span><span class="p">:</span>
                    <span class="n">reg</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">regularization_term</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>
                    <span class="n">ll</span> <span class="o">+=</span> <span class="n">reg</span>
                <span class="k">return</span> <span class="n">ll</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span> <span class="k">else</span> <span class="mf">1e25</span>

            <span class="c1"># And the gradient of the objective function.</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">grad_nll</span><span class="p">(</span><span class="n">p_opt</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">p_opt</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">p_opt</span>
                <span class="n">tmp_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="n">current_gp</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                <span class="n">grad_lnlike</span> <span class="o">=</span> <span class="o">-</span><span class="n">tmp_gp</span><span class="o">.</span><span class="n">grad_log_likelihood</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
                    <span class="n">gll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p_opt</span><span class="p">))</span>
                    <span class="n">gll</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_length_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">grad_lnlike</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">])</span>
                    <span class="n">gll</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_other_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_lnlike</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_other_indices</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">gll</span> <span class="o">=</span> <span class="n">grad_lnlike</span>
                        
                <span class="k">if</span> <span class="n">regularize</span><span class="p">:</span>
                    <span class="n">reg_grad</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">regularization_gradient</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>           
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
                        <span class="n">reg_grad_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">reg_grad</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">])</span>
                        <span class="n">gll</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_length_index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg_grad_avg</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">gll</span> <span class="o">+=</span> <span class="n">reg_grad</span>

                <span class="k">return</span> <span class="n">gll</span>
            
            <span class="k">if</span> <span class="n">use_gradient</span><span class="p">:</span>
                <span class="n">jac</span> <span class="o">=</span> <span class="n">grad_nll</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">jac</span> <span class="o">=</span> <span class="kc">None</span>
            
            <span class="k">def</span><span class="w"> </span><span class="nf">_optimize_fn</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">nll</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_method</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>

            <span class="n">current_hp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hyperparameter_vector</span><span class="p">(</span><span class="n">current_gp</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_nopt</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">results</span> <span class="o">=</span> <span class="n">_optimize_fn</span><span class="p">(</span><span class="n">current_hp</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">p0</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_nopt</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;lhs&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
                <span class="n">p0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_hp</span>
                
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Sequential with progress bar</span>
                    <span class="n">opt_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">_optimize_fn</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Optimizing GP&quot;</span><span class="p">)]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span>
                    <span class="c1"># Parallel with progress bar using imap</span>
                    <span class="n">opt_results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
                        <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="n">_optimize_fn</span><span class="p">,</span> <span class="n">p0</span><span class="p">),</span>
                        <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">p0</span><span class="p">),</span>
                        <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Optimizing GP (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="si">}</span><span class="s2"> cores)&quot;</span>
                    <span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_close_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">get_fun_value</span><span class="p">(</span><span class="n">res</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">fun</span>
                <span class="n">results</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">opt_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">get_fun_value</span><span class="p">)</span>
            
            <span class="n">op_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="n">current_gp</span><span class="p">,</span> <span class="n">results</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
            <span class="n">op_gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">nll_0</span> <span class="o">=</span> <span class="n">nll</span><span class="p">(</span><span class="n">current_hp</span><span class="p">)</span>
                <span class="n">nll_fit</span> <span class="o">=</span> <span class="n">nll</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">regularize</span><span class="p">:</span>
                    <span class="n">reg_0</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">regularization_term</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">current_hp</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>
                    <span class="n">reg_fit</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">regularization_term</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hp_length_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial -logL: </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">nll_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Regularization: </span><span class="si">{</span><span class="n">reg_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Total: </span><span class="si">{</span><span class="n">nll_0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">reg_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final -logL: </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">nll_fit</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Regularization: </span><span class="si">{</span><span class="n">reg_fit</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Total: </span><span class="si">{</span><span class="n">nll_fit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">reg_fit</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial -logL: </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">nll_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Regularization: None&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final -logL: </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">nll_fit</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | </span><span class="se">\t</span><span class="s2"> Regularization: None &quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">nit</span><span class="si">}</span><span class="s2"> iterations | Success: </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">success</span><span class="si">}</span><span class="s2"> | Message: </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">message</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hyperopt_method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;cv&quot;</span><span class="p">:</span>
            <span class="c1"># Cross-validation hyperparameter optimization    </span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Optimizing GP hyperparameters using </span><span class="si">{</span><span class="n">cv_folds</span><span class="si">}</span><span class="s2">-fold cross-validation...&quot;</span><span class="p">)</span>
            
            <span class="k">try</span><span class="p">:</span>                         
                <span class="n">candidates</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="n">cv_n_candidates</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;lhs&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

                <span class="c1"># Add current hyperparameters as a candidate if GP exists</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
                    <span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">)</span>
                
                <span class="c1"># Expand hyperparameters if using uniform scales</span>
                <span class="c1"># CV function expects full parameter vectors that can be set directly on GP</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">uniform_scales</span><span class="p">:</span>
                    <span class="n">candidates_expanded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">expand_hyperparameter_vector</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">candidates_expanded</span> <span class="o">=</span> <span class="n">candidates</span>
                     
                <span class="c1"># suppress outputs if running parallel chains   </span>
                <span class="k">if</span> <span class="n">multi_proc</span><span class="p">:</span>
                    <span class="n">verbose_cv</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Suppress for parallel chains</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">verbose_cv</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Always show CV diagnostics</span>
                
                <span class="c1"># Optimize using cross-validation</span>
                <span class="n">pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span> <span class="k">if</span> <span class="n">multi_proc</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">op_gp</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">optimize_gp_kfold_cv</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span>
                    <span class="n">candidates_expanded</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="p">,</span>
                    <span class="n">k_folds</span><span class="o">=</span><span class="n">cv_folds</span><span class="p">,</span>
                    <span class="n">scoring</span><span class="o">=</span><span class="n">cv_scoring</span><span class="p">,</span>
                    <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
                    <span class="n">stage2_candidates</span><span class="o">=</span><span class="n">cv_stage2_candidates</span><span class="p">,</span>
                    <span class="n">stage2_width</span><span class="o">=</span><span class="n">cv_stage2_width</span><span class="p">,</span>
                    <span class="n">stage3_candidates</span><span class="o">=</span><span class="n">cv_stage3_candidates</span><span class="p">,</span>
                    <span class="n">stage3_width</span><span class="o">=</span><span class="n">cv_stage3_width</span><span class="p">,</span>
                    <span class="n">weighted_mse_method</span><span class="o">=</span><span class="n">cv_weighted_mse_method</span><span class="p">,</span>
                    <span class="n">weighted_mse_factor</span><span class="o">=</span><span class="n">cv_weighted_factor</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="n">verbose_cv</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_close_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
                
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
                <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()</span>
                <span class="n">tb_line</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_tb</span><span class="p">(</span><span class="n">exc_traceback</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: CV hyperparameter optimization failed: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error at line </span><span class="si">{</span><span class="n">tb_line</span><span class="o">.</span><span class="n">lineno</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">tb_line</span><span class="o">.</span><span class="n">filename</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">tb_line</span><span class="o">.</span><span class="n">line</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Falling back to maximum likelihood optimization...&quot;</span><span class="p">)</span>
                
                <span class="c1"># Fall back to ML optimization if CV fails</span>
                <span class="n">op_gp</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># If op_gp is not set (CV failed or wasn&#39;t used), use current GP or initialize one</span>
        <span class="k">if</span> <span class="s1">&#39;op_gp&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">or</span> <span class="n">op_gp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">):</span>
                <span class="n">op_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Create a basic GP with default hyperparameters</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">:</span>
                    <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span>
                <span class="n">op_gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">_y</span><span class="p">),</span>
                                <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">,</span> <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">)</span>
                <span class="n">op_gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>

        <span class="n">tf</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">timing</span> <span class="o">=</span> <span class="n">tf</span> <span class="o">-</span> <span class="n">t0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparam_opt_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timing</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">op_gp</span><span class="p">,</span> <span class="n">timing</span>
        
    
<div class="viewcode-block" id="SurrogateModel.eval_gp_at_iteration">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.eval_gp_at_iteration">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eval_gp_at_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        
        <span class="c1"># gp_iter = self.gp </span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">]</span>
            <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">gp_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">gp_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_gp_hyperparameters</span><span class="p">)</span>
        <span class="c1"># Handle iter=-1 case to use all data instead of excluding last element</span>
        <span class="k">elif</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])):</span>
            <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
            <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
            <span class="c1"># Use the latest parameters (last in the list)</span>
            <span class="n">gp_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])):</span>
            <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">+</span> <span class="nb">iter</span><span class="p">]</span>
            <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">+</span> <span class="nb">iter</span><span class="p">]</span>
            <span class="n">last_hp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span>
            <span class="n">gp_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">last_hp</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="nb">iter</span><span class="si">}</span><span class="s2"> exceeds available training iterations (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span>
            
        <span class="n">gp_iter</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta_cond</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">gp_predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="c1"># Ensure x is properly shaped for george GP</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                <span class="c1"># If we have a single point but wrong shape, transpose</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
                <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">gp_iter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">_y_cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">gp_predict</span></div>



<div class="viewcode-block" id="SurrogateModel.surrogate_log_likelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.surrogate_log_likelihood">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">surrogate_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta_xs</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate predictive mean of the GP at point(s) ``theta_xs``</span>
<span class="sd">        </span>
<span class="sd">        This method is vectorized to handle both single parameter vectors and</span>
<span class="sd">        arrays of parameter vectors efficiently.</span>

<span class="sd">        :param theta_xs: Point(s) to evaluate GP mean at. Can be:</span>
<span class="sd">            - 1D array of shape (ndim,) for single point</span>
<span class="sd">            - 2D array of shape (npoints, ndim) for multiple points</span>
<span class="sd">        :type theta_xs: *array-like*</span>
<span class="sd">        :param iter: Iteration number of GP to use. If -1, uses most recent GP.</span>
<span class="sd">        :type iter: *int, optional*</span>
<span class="sd">        :param return_var: Whether to also return variance predictions.</span>
<span class="sd">        :type return_var: *bool, optional*</span>

<span class="sd">        :returns: </span>
<span class="sd">            - **ypred** (*array*) -- GP mean(s) evaluated at ``theta_xs``. Shape matches input.</span>
<span class="sd">            - **varpred** (*array, optional*) -- GP variance(s) if return_var=True.</span>
<span class="sd">        :rtype: *array or tuple of arrays*</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Convert input to numpy array and handle dimensionality</span>
        <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Handle 1D input (single point)</span>
        <span class="k">if</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;theta_xs must be 1D or 2D array, got </span><span class="si">{</span><span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling transformation</span>
        <span class="n">_theta_xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        
        <span class="c1"># Get GP at specified iteration</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">gp_ii</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_gp_at_iteration</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gp_ii</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Apply the GP and handle return values</span>
        <span class="k">if</span> <span class="n">return_var</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">_ypred</span> <span class="o">=</span> <span class="n">gp_ii</span><span class="p">(</span><span class="n">_theta_xs</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="c1"># Return single value if input was 1D, otherwise return array</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_ypred</span><span class="p">,</span> <span class="n">_varpred</span> <span class="o">=</span> <span class="n">gp_ii</span><span class="p">(</span><span class="n">_theta_xs</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">varpred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_varpred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="c1"># Return single values if input was 1D, otherwise return arrays</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">varpred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">varpred</span></div>

    
    
<div class="viewcode-block" id="SurrogateModel.surrogate_likelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.surrogate_likelihood">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">surrogate_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta_xs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate predictive probability (not log-probability) of the GP at point(s) theta_xs</span>
<span class="sd">        </span>
<span class="sd">        This method is vectorized to handle both single parameter vectors and</span>
<span class="sd">        arrays of parameter vectors efficiently.</span>
<span class="sd">        </span>
<span class="sd">        :param theta_xs: Point(s) to evaluate GP probability at. Can be:</span>
<span class="sd">            - 1D array of shape (ndim,) for single point</span>
<span class="sd">            - 2D array of shape (npoints, ndim) for multiple points</span>
<span class="sd">        :type theta_xs: *array-like*</span>
<span class="sd">        </span>
<span class="sd">        :returns: GP probability/probabilities evaluated at ``theta_xs``. Shape matches input.</span>
<span class="sd">        :rtype: *float or array*</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get log-probability from GP (already vectorized)</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        
        <span class="c1"># Convert to probability (works element-wise for arrays)</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">prob</span></div>



<div class="viewcode-block" id="SurrogateModel.create_cached_surrogate_likelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.create_cached_surrogate_likelihood">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_cached_surrogate_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a cached surrogate likelihood function that computes the GP once</span>
<span class="sd">        and can be evaluated multiple times without recomputing the GP.</span>
<span class="sd">        </span>
<span class="sd">        This is useful when you need to evaluate the surrogate likelihood at many</span>
<span class="sd">        different points with the same GP configuration, as it avoids the expensive</span>
<span class="sd">        GP computation (gp.compute()) on each call.</span>
<span class="sd">        </span>
<span class="sd">        :param iter: Iteration number of GP to use. If -1, uses most recent GP.</span>
<span class="sd">        :type iter: *int, optional*</span>
<span class="sd">        </span>
<span class="sd">        :returns: A cached likelihood function that can be called with theta_xs</span>
<span class="sd">        :rtype: *callable*</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Determine training data and hyperparameters for the specified iteration</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Handle iter=-1 case to use all data</span>
            <span class="k">if</span> <span class="nb">iter</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
                <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
                <span class="n">hyperparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">+</span> <span class="nb">iter</span><span class="p">]</span>
                <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">+</span> <span class="nb">iter</span><span class="p">]</span>
                <span class="n">hyperparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
            <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
            <span class="n">hyperparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
        
        <span class="c1"># Create a fresh GP instance with the same kernel</span>
        <span class="n">gp_iter</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">configure_gp</span><span class="p">(</span><span class="n">_theta_cond</span><span class="p">,</span> <span class="n">_y_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                        <span class="n">fit_amp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">,</span> 
                                        <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span>
                                        <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">,</span>
                                        <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">,</span>
                                        <span class="n">hyperparameters</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">)</span>
        
        <span class="c1"># Compute the GP and save so that it doesn&#39;t need to be recomputed later</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">gp_iter</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta_cond</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;create_cached_surrogate_likelihood: Error computing GP at iteration </span><span class="si">{</span><span class="nb">iter</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span>
        
        <span class="c1"># Return a picklable cached surrogate likelihood object</span>
        <span class="k">return</span> <span class="n">CachedSurrogateLikelihood</span><span class="p">(</span><span class="n">gp_iter</span><span class="p">,</span> <span class="n">_y_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="p">,</span> 
                                       <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.find_next_point">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.find_next_point">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">find_next_point</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{}):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find next set of ``(theta, y)`` training points by maximizing the</span>
<span class="sd">        active learning utility function.</span>

<span class="sd">        :param nopt: (*int, optional*) </span>
<span class="sd">            Number of times to restart the objective function optimization. </span>
<span class="sd">            Defaults to 1. Increase to avoid converging to local minima.</span>
<span class="sd">        :param optimizer_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to scipy optimizer. Default is {}.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">opt_timing_0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="n">predict_gp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_theta_xs</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">_theta_xs</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Create objective function with appropriate parameters for different algorithms</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;jones&quot;</span><span class="p">:</span>
            <span class="c1"># Jones (Expected Improvement) requires y_best parameter</span>
            <span class="n">y_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>  <span class="c1"># Current best observed value</span>
            <span class="n">obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">utility</span><span class="p">,</span> <span class="n">predict_gp</span><span class="o">=</span><span class="n">predict_gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span> <span class="n">y_best</span><span class="o">=</span><span class="n">y_best</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Other algorithms (BAPE, AGP, etc.) don&#39;t need y_best</span>
            <span class="n">obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">utility</span><span class="p">,</span> <span class="n">predict_gp</span><span class="o">=</span><span class="n">predict_gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span>
        
        <span class="c1"># Get gradient function if available</span>
        <span class="n">grad_obj_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;grad_utility&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;jones&quot;</span><span class="p">:</span>
                <span class="n">grad_obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span> <span class="n">y_best</span><span class="o">=</span><span class="n">y_best</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad_obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span>

        <span class="c1"># Always use serial execution for acquisition function optimization</span>
        <span class="n">_thetaN</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">minimize_objective</span><span class="p">(</span><span class="n">obj_fn</span><span class="p">,</span> 
                                        <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span>
                                        <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span>
                                        <span class="n">ps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span><span class="p">,</span>
                                        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_opt_method</span><span class="p">,</span>
                                        <span class="n">options</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span>
                                        <span class="n">grad_obj_fn</span><span class="o">=</span><span class="n">grad_obj_fn</span><span class="p">,</span>
                                        <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="n">opt_timing</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">opt_timing_0</span>
        <span class="c1"># self.training_results[&quot;acquisition_optimizer_niter&quot;].append(opt_result.nit)</span>
        
        <span class="c1"># Validate optimization result</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_thetaN</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Acquisition function optimization failed. Falling back to random sampling.&quot;</span><span class="p">)</span>
            <span class="c1"># Fall back to random sampling from prior</span>
            <span class="n">_thetaN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        
        <span class="n">thetaN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_thetaN</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">yN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">(</span><span class="n">thetaN</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># Validate new training point</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">thetaN</span><span class="o">.</span><span class="n">flatten</span><span class="p">())):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;New theta contains NaN or Inf: </span><span class="si">{</span><span class="n">thetaN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">opt_timing</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">yN</span><span class="o">.</span><span class="n">flatten</span><span class="p">())):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;New y value is NaN or Inf: </span><span class="si">{</span><span class="n">yN</span><span class="si">}</span><span class="s2">. Check your likelihood function at theta=</span><span class="si">{</span><span class="n">thetaN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">opt_timing</span>
        
        <span class="c1"># add theta and y to training samples</span>
        <span class="n">theta_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">(),</span> <span class="n">thetaN</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">(),</span> <span class="n">yN</span><span class="p">)</span>
        
        <span class="c1"># refit scaling functions including new point</span>
        <span class="n">_theta_prop</span><span class="p">,</span> <span class="n">_y_prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit_scalers</span><span class="p">(</span><span class="n">theta_prop</span><span class="p">,</span> <span class="n">y_prop</span><span class="p">)</span>
                
        <span class="k">if</span> <span class="n">_theta_prop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">_y_prop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">opt_timing</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_theta_prop</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;_theta_prop contains NaN or Inf after scaling. Check training data and scalers.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">opt_timing</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">_y_prop</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;_y_prop contains NaN or Inf after scaling. Check training data and scalers.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">opt_timing</span>

        <span class="k">return</span> <span class="n">_theta_prop</span><span class="p">,</span> <span class="n">_y_prop</span><span class="p">,</span> <span class="n">opt_timing</span></div>



<div class="viewcode-block" id="SurrogateModel.active_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.active_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">active_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_opt_multiproc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_attempts</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span> 
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform active learning to iteratively improve the surrogate model.</span>
<span class="sd">        </span>
<span class="sd">        Uses acquisition functions to intelligently select new training points that</span>
<span class="sd">        will most improve the Gaussian Process model. Different algorithms balance</span>
<span class="sd">        exploration (uncertainty reduction) vs exploitation (finding optima).</span>
<span class="sd">        </span>
<span class="sd">        :param niter: (*int, optional, default=100*)</span>
<span class="sd">            Number of active learning iterations. Each iteration adds one new training point.</span>
<span class="sd">            </span>
<span class="sd">        :param algorithm: (*str, optional, default=&quot;bape&quot;*)</span>
<span class="sd">            Active learning algorithm. Options:</span>
<span class="sd">            - &quot;bape&quot;: Bayesian Active Parameter Estimation (exploration-focused)</span>
<span class="sd">            - &quot;jones&quot;: Jones algorithm (exploitation-focused, good for optimization)</span>
<span class="sd">            - &quot;agp&quot;: Augmented Gaussian Process (balanced)</span>
<span class="sd">            - &quot;alternate&quot;: Alternates between exploration and exploitation</span>
<span class="sd">            </span>
<span class="sd">        :param gp_opt_freq: (*int, optional, default=20*)</span>
<span class="sd">            Frequency of GP hyperparameter re-optimization. GP hyperparameters are</span>
<span class="sd">            re-optimized every gp_opt_freq iterations. Lower values = more optimization.</span>
<span class="sd">            </span>
<span class="sd">        :param save_progress: (*bool, optional, default=False*)</span>
<span class="sd">            Whether to save training progress data for later analysis.</span>
<span class="sd">            </span>
<span class="sd">        :param obj_opt_method: (*str, optional, default=&quot;nelder-mead&quot;*)</span>
<span class="sd">            Optimization method for acquisition function. Options:</span>
<span class="sd">            - &quot;l-bfgs-b&quot;: L-BFGS-B (good with gradients)</span>
<span class="sd">            - &quot;nelder-mead&quot;: Nelder-Mead simplex (gradient-free)</span>
<span class="sd">            </span>
<span class="sd">        :param nopt: (*int, optional, default=1*)</span>
<span class="sd">            Number of optimization restarts for acquisition function. Higher values</span>
<span class="sd">            help avoid local minima but increase computation time.</span>
<span class="sd">            </span>
<span class="sd">        :param use_grad_opt: (*bool, optional, default=True*)</span>
<span class="sd">            Whether to use gradient information if available. Set False for</span>
<span class="sd">            gradient-free optimization.</span>
<span class="sd">            </span>
<span class="sd">        :param optimizer_kwargs: (*dict, optional, default={}*)</span>
<span class="sd">            Additional keyword arguments passed to the optimizer.</span>
<span class="sd">            </span>
<span class="sd">        :param show_progress: (*bool, optional, default=True*)</span>
<span class="sd">            Whether to display progress bar during training.</span>
<span class="sd">            </span>
<span class="sd">        .. note::</span>
<span class="sd">        </span>
<span class="sd">            Active learning algorithms have different purposes:</span>
<span class="sd">            </span>
<span class="sd">            - **BAPE**: Best for uncertainty quantification and space-filling</span>
<span class="sd">            - **Jones**: Best for finding likelihood maxima/minima (optimization)  </span>
<span class="sd">            - **Alternate**: Good balance for both exploration and exploitation</span>
<span class="sd">            - **AGP**: Another balanced approach</span>
<span class="sd">            </span>
<span class="sd">            The method automatically handles GP re-training and hyperparameter optimization</span>
<span class="sd">            based on the specified frequency. Training data is accumulated in _theta and _y</span>
<span class="sd">            attributes.</span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">        Basic active learning with BAPE:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=50, algorithm=&quot;bape&quot;)</span>
<span class="sd">        </span>
<span class="sd">        Optimization-focused active learning:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=30, algorithm=&quot;jones&quot;, gp_opt_freq=10)</span>
<span class="sd">        </span>
<span class="sd">        Balanced approach with frequent GP optimization:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=40, algorithm=&quot;alternate&quot;, gp_opt_freq=5)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Set algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">utility</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">assign_utility</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_grad_opt</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># GP hyperparameter optimization frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">=</span> <span class="n">gp_opt_freq</span>

        <span class="c1"># Objective function optimization method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obj_opt_method</span> <span class="o">=</span> <span class="n">obj_opt_method</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">first_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">first_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">niter</span><span class="si">}</span><span class="s2"> active learning iterations using </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

        <span class="c1"># Create iterator with or without progress bar based on show_progress parameter</span>
        <span class="n">iterator</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">niter</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="n">show_progress</span> <span class="k">else</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">niter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>

            <span class="n">attempts</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">success</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">success</span> <span class="ow">and</span> <span class="n">attempts</span> <span class="o">&lt;</span> <span class="n">max_attempts</span><span class="p">:</span>

                <span class="c1"># Find next training point! (always single-threaded)</span>
                <span class="n">_theta_prop</span><span class="p">,</span> <span class="n">_y_prop</span><span class="p">,</span> <span class="n">opt_timing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_next_point</span><span class="p">(</span><span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">_theta_prop</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">_y_prop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">attempts</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Fit GP with new training point</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">fit_gp_timing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="n">_theta_prop</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="n">_y_prop</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">())</span>
                    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="k">if</span> <span class="n">attempts</span> <span class="o">&gt;=</span> <span class="n">max_attempts</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to find a valid training point after </span><span class="si">{</span><span class="n">max_attempts</span><span class="si">}</span><span class="s2"> attempts. </span><span class="se">\</span>
<span class="s2">                                        Check your likelihood function and training data for issues or increase max_attempts.&quot;</span><span class="p">)</span>
                    
            <span class="c1"># If proposed (theta, y) did not cause fitting issues, save to surrogate model obj</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">_theta_prop</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">_y_prop</span>
            
            <span class="c1"># Optimize GP?</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

                <span class="n">reopt_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">reopt_kwargs</span><span class="p">[</span><span class="s2">&quot;multi_proc&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">allow_opt_multiproc</span>
                <span class="c1"># re-optimize hyperparamters</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_gp</span><span class="p">(</span><span class="o">**</span><span class="n">reopt_kwargs</span><span class="p">)</span>
                
                <span class="c1"># record which iteration hyperparameters were optimized</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="p">(</span><span class="n">save_progress</span> <span class="o">==</span> <span class="kc">True</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ii</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_error&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_hyperparam&quot;</span><span class="p">])</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_fit_2D&quot;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_train_scatter&quot;</span><span class="p">])</span>
            
            <span class="c1"># evaluate gp training error (scaled)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">_ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">_y_prop</span><span class="p">,</span> <span class="n">_theta_prop</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="n">training_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">()</span> <span class="o">-</span> <span class="n">ypred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">training_scaled_mse</span> <span class="o">=</span> <span class="n">training_mse</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">())</span>
                
                <span class="c1"># if hyperparameters were reoptimized, report train error</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train MSE:&quot;</span><span class="p">,</span> <span class="n">training_mse</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Error evaluating GP training error at iteration </span><span class="si">{</span><span class="n">ii</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">first_iter</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">training_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                <span class="n">training_scaled_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># evaluate gp test error (scaled)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_theta_test&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_y_test&#39;</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">_ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ytest</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                    <span class="n">ytest_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                    <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ytest_true</span> <span class="o">-</span> <span class="n">ytest</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">test_mse</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">())</span>
                    
                    <span class="c1"># if hyperparameters were reoptimized, report test error</span>
                    <span class="k">if</span> <span class="p">((</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test MSE:&quot;</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Error evaluating GP test error at iteration </span><span class="si">{</span><span class="n">ii</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">first_iter</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                    <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># evaluate convergence criteria</span>
            <span class="n">gp_kl_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># save results to a dictionary</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_scaled_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_scaled_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_kl_divergence&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gp_kl_divergence</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_train_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_gp_timing</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;obj_fn_opt_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">opt_timing</span><span class="p">)</span>

            <span class="c1"># record total number of training samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
            <span class="c1"># number of active training samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span></div>

            
            
<div class="viewcode-block" id="SurrogateModel.active_train_parallel">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.active_train_parallel">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">active_train_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nchains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                                   <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                   <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{},</span> 
                                   <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run multiple active learning chains in parallel.</span>
<span class="sd">        </span>
<span class="sd">        :param niter: (*int, optional*) </span>
<span class="sd">            Number of iterations per chain. Default 100.</span>
<span class="sd">            </span>
<span class="sd">        :param nchains: (*int, optional*) </span>
<span class="sd">            Number of parallel chains to run. Default 4.</span>
<span class="sd">            </span>
<span class="sd">        :param algorithm: (*str, optional*) </span>
<span class="sd">            Active learning algorithm. Default &quot;bape&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param gp_opt_freq: (*int, optional*)</span>
<span class="sd">            Frequency of GP hyperparameter optimization. Default 20.</span>
<span class="sd">            </span>
<span class="sd">        :param obj_opt_method: (*str, optional*)</span>
<span class="sd">            Optimization method for acquisition function. Default &quot;nelder-mead&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param nopt: (*int, optional*)</span>
<span class="sd">            Number of restarts for acquisition optimization. Default 1.</span>
<span class="sd">            </span>
<span class="sd">        :param use_grad_opt: (*bool, optional*)</span>
<span class="sd">            Whether to use gradient-based optimization. Default True.</span>
<span class="sd">            </span>
<span class="sd">        :param optimizer_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional optimizer kwargs. Default {}.</span>
<span class="sd">            </span>
<span class="sd">        :param show_progress: (*bool, optional*) </span>
<span class="sd">            Whether to display progress bar during parallel chain execution. Default is True.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function uses multiprocessing.Pool instead of threading, which can provide</span>
<span class="sd">        better performance for CPU-intensive tasks and avoids GIL limitations. However:</span>
<span class="sd">        </span>
<span class="sd">        - All model data must be pickleable (which it should be for SurrogateModel)</span>
<span class="sd">        - Each process runs in separate memory space (higher memory usage)</span>
<span class="sd">        - Process startup overhead is higher than threading</span>
<span class="sd">        - Better isolation between chains (one chain failure won&#39;t affect others)</span>
<span class="sd">        - Can achieve true parallelism on multi-core systems</span>
<span class="sd">        </span>
<span class="sd">        The function automatically respects the ncore limit and won&#39;t create more processes</span>
<span class="sd">        than specified in self.ncore.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        
        <span class="c1"># Set algorithm attribute to avoid save issues</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        
        <span class="c1"># Initialize training results if not present</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;training_results&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                   <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                   <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;acquisition_optimizer_niter&quot;</span><span class="p">:</span> <span class="p">[]}</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Running </span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> parallel active learning chains for </span><span class="si">{</span><span class="n">niter</span><span class="si">}</span><span class="s2"> iterations each...&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Algorithm: </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2">, Method: </span><span class="si">{</span><span class="n">obj_opt_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using multiprocessing with max </span><span class="si">{</span><span class="nb">min</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>
        
        <span class="c1"># Store original training data</span>
        <span class="n">original_ntrain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span>
        
        <span class="c1"># Track results from all chains</span>
        <span class="n">all_new_theta</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_new_y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">chain_results</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Determine number of processes to use (respect ncore limit)</span>
        <span class="n">max_processes</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span>
        <span class="n">use_multiprocessing</span> <span class="o">=</span> <span class="p">(</span><span class="n">nchains</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">use_multiprocessing</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running with </span><span class="si">{</span><span class="n">max_processes</span><span class="si">}</span><span class="s2"> processes (limited by ncore=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_multiprocessing</span><span class="p">:</span>
                <span class="c1"># Prepare arguments for each chain</span>
                <span class="n">chain_args</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                    <span class="c1"># Create a pickleable state dictionary for each chain</span>
                    <span class="n">chain_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pickleable_state</span><span class="p">()</span>
                    <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;chain_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                    <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;savedir&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/chain_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
                    
                    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">chain_state</span><span class="p">,</span>
                        <span class="n">niter</span><span class="p">,</span>
                        <span class="n">algorithm</span><span class="p">,</span>
                        <span class="n">gp_opt_freq</span><span class="p">,</span>
                        <span class="n">obj_opt_method</span><span class="p">,</span>
                        <span class="n">nopt</span><span class="p">,</span>
                        <span class="n">use_grad_opt</span><span class="p">,</span>
                        <span class="n">optimizer_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">chain_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                
                <span class="n">pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="n">max_processes</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running parallel chains (MP)&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="n">_run_chain_worker_mp</span><span class="p">,</span> <span class="n">chain_args</span><span class="p">):</span>
                            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Use map for simpler execution</span>
                    <span class="n">results</span> <span class="o">=</span> <span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_run_chain_worker_mp</span><span class="p">,</span> <span class="n">chain_args</span><span class="p">)</span>
                    
                <span class="bp">self</span><span class="o">.</span><span class="n">_close_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
                
                <span class="c1"># Process results</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">training_results</span> <span class="o">=</span> <span class="n">result</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_y</span><span class="p">)</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_results</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> failed or returned invalid result&quot;</span><span class="p">)</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]))</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
            
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Fallback to sequential execution</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running chains sequentially...&quot;</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">sequential_progress</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running chains sequentially&quot;</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running chain </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                    
                    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_chain_worker</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> 
                                                  <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span>
                                                  <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span> 
                                                  <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                  <span class="n">allow_opt_multiproc</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    
                    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">training_results</span> <span class="o">=</span> <span class="n">result</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_y</span><span class="p">)</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_results</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> failed&quot;</span><span class="p">)</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]))</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
                    
                    <span class="c1"># Update sequential progress bar</span>
                    <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                        <span class="n">sequential_progress</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="c1"># Close sequential progress bar</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">sequential_progress</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                    
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
            <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()</span>
            <span class="n">tb_line</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_tb</span><span class="p">(</span><span class="n">exc_traceback</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiprocessing execution failed (</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">) line </span><span class="si">{</span><span class="n">tb_line</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span>
        
        <span class="c1"># Combine all new training samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Combining training samples from all chains...&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_combine_chain_results</span><span class="p">(</span><span class="n">all_new_theta</span><span class="p">,</span> <span class="n">all_new_y</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">total_new_samples</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully combined </span><span class="si">{</span><span class="n">total_new_samples</span><span class="si">}</span><span class="s2"> new training samples from </span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> chains&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total training samples: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span><span class="si">}</span><span class="s2"> (was </span><span class="si">{</span><span class="n">original_ntrain</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        
        <span class="c1"># Final GP optimization with all combined data</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Performing final GP optimization with combined dataset...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_gp</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span></div>



<div class="viewcode-block" id="SurrogateModel.lnprob">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.lnprob">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">lnprob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log probability function used for ``emcee``, which sums the prior with the surrogate model likelihood</span>

<span class="sd">        .. math::</span>

<span class="sd">            \\ln P(\\theta | x) \\propto \\ln P(x | \\theta) + \\ln P(\\theta)</span>

<span class="sd">        where \\ln P(x | \\theta) is the surrogate likelihood function and \\ln P(\\theta) is the prior function.</span>

<span class="sd">        :param theta: (*array, required*) </span>
<span class="sd">            Array of model input parameters to evaluate model probability at.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;surrogate&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;GP has not been trained&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;prior_fn&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;prior_fn has not been specified&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;like_fn&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">lnp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lnp</span></div>



<div class="viewcode-block" id="SurrogateModel.find_map">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.find_map">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">find_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span><span class="p">,</span> <span class="n">nRestarts</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Not implemented.&quot;</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.run_emcee">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_emcee">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_emcee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nwalkers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nsteps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">),</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{},</span>
                  <span class="n">opt_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">multi_proc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prior_fn_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_ess</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the emcee affine-invariant MCMC algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the emcee package to perform Markov Chain Monte Carlo (MCMC) </span>
<span class="sd">        sampling on either the trained GP surrogate model or the true likelihood function.</span>
<span class="sd">        The affine-invariant ensemble sampler is robust and works well for a wide variety</span>
<span class="sd">        of posterior shapes without requiring manual tuning of step sizes.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)</span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_fn: (*callable or None, optional*)</span>
<span class="sd">            Log-prior function with signature prior_fn(theta). Should return log-probability</span>
<span class="sd">            density. If None, uses uniform prior with bounds from self.bounds.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param nwalkers: (*int or None, optional*)</span>
<span class="sd">            Number of MCMC walkers in the ensemble. Should be at least 2*ndim.</span>
<span class="sd">            If None, defaults to 10*ndim. More walkers improve convergence but increase</span>
<span class="sd">            computational cost. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param nsteps: (*int, optional*)</span>
<span class="sd">            Number of MCMC steps per walker. Total number of likelihood evaluations</span>
<span class="sd">            will be nwalkers * nsteps. Default is 50000.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to emcee.EnsembleSampler constructor.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;a&#39;: Stretch move scale parameter (default: 2.0)</span>
<span class="sd">            - &#39;moves&#39;: Custom proposal moves</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param run_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the run_mcmc() method.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;progress&#39;: Show progress bar (default: True)</span>
<span class="sd">            - &#39;store&#39;: Store chain in memory (default: True)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param opt_init: (*bool, optional*)</span>
<span class="sd">            Whether to initialize walkers near the maximum a posteriori (MAP) estimate.</span>
<span class="sd">            If True, uses find_map() to locate starting point. If False, initializes</span>
<span class="sd">            walkers randomly from the prior. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            Whether to use multiprocessing with self.ncore processes. Generally</span>
<span class="sd">            recommended for expensive likelihood evaluations. Default is True.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_fn_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior function for logging purposes. If None</span>
<span class="sd">            and prior_fn is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param burn: (*int or None, optional*)</span>
<span class="sd">            Number of burn-in samples to discard from each walker. If None, </span>
<span class="sd">            automatically estimates burn-in using autocorrelation analysis.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param thin: (*int or None, optional*)</span>
<span class="sd">            Thinning factor - keep every thin-th sample to reduce autocorrelation.</span>
<span class="sd">            If None, automatically estimates based on autocorrelation time.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param samples_file: (*str or None, optional*)</span>
<span class="sd">            If provided, saves the final samples to this file in NumPy .npz format.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param min_ess: (*int, optional*)</span>
<span class="sd">            Minimum effective sample size. If the number of final samples is less than </span>
<span class="sd">            min_ess, will run additional sampling rounds and combine samples until the </span>
<span class="sd">            total number of samples exceeds min_ess. Default is 0 (no minimum required).</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        sampler : emcee.EnsembleSampler</span>
<span class="sd">            The emcee sampler object containing full chain and metadata</span>
<span class="sd">        emcee_samples : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final MCMC samples after burn-in and thinning</span>
<span class="sd">        emcee_samples_full : ndarray of shape (nsteps, nwalkers, ndim)</span>
<span class="sd">            Full MCMC chain before processing</span>
<span class="sd">        emcee_samples_true : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        emcee_samples_gp : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final samples when using surrogate likelihood</span>
<span class="sd">        emcee_run : bool</span>
<span class="sd">            Flag indicating emcee has been successfully run</span>
<span class="sd">        emcee_runtime : float</span>
<span class="sd">            Wall-clock time taken for emcee sampling in seconds</span>
<span class="sd">        nwalkers : int</span>
<span class="sd">            Number of walkers used</span>
<span class="sd">        nsteps : int</span>
<span class="sd">            Number of steps per walker</span>
<span class="sd">        burn : int</span>
<span class="sd">            Burn-in length used for final samples</span>
<span class="sd">        thin : int</span>
<span class="sd">            Thinning factor used for final samples</span>
<span class="sd">        iburn : int</span>
<span class="sd">            Automatically estimated burn-in length</span>
<span class="sd">        ithin : int</span>
<span class="sd">            Automatically estimated thinning factor</span>
<span class="sd">        acc_frac : float</span>
<span class="sd">            Mean acceptance fraction across all walkers</span>
<span class="sd">        autcorr_time : float</span>
<span class="sd">            Mean autocorrelation time in steps</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;likelihood&quot;)</span>
<span class="sd">        prior_fn_comment : str</span>
<span class="sd">            Description of prior function used</span>
<span class="sd">            </span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">        </span>
<span class="sd">        Sample surrogate model with default settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee()</span>
<span class="sd">        </span>
<span class="sd">        Sample true likelihood with specific settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(like_fn=&quot;true&quot;, nwalkers=100, nsteps=10000)</span>
<span class="sd">        </span>
<span class="sd">        Use custom prior and optimize initialization:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; def log_prior(theta):</span>
<span class="sd">        ...     # Custom Gaussian prior</span>
<span class="sd">        ...     return -0.5 * np.sum((theta/2)**2)</span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(prior_fn=log_prior, opt_init=True)</span>
<span class="sd">        </span>
<span class="sd">        Run with manual burn-in and thinning:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(nsteps=100000, burn=10000, thin=10)</span>
<span class="sd">        </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        emcee documentation: https://emcee.readthedocs.io/</span>
<span class="sd">        Foreman-Mackey et al. (2013): &quot;emcee: The MCMC Hammer&quot;, PASP, 125, 306-312</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">emcee</span>

        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing emcee with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;likelihood&quot;</span>

        <span class="k">if</span> <span class="n">prior_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No prior_fn specified. Defaulting to uniform prior with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">lnprior_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

            <span class="c1"># Comment for output log file</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span>  <span class="sa">f</span><span class="s2">&quot;Default uniform prior. </span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: ut.prior_fn_uniform</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span> <span class="o">=</span> <span class="n">prior_fn</span>

            <span class="c1"># Comment for output log file</span>
            <span class="k">if</span> <span class="n">prior_fn_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;User defined prior.&quot;</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="s2">&quot;Prior function: unrecorded&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span> <span class="n">prior_fn_comment</span>

        <span class="c1"># number of walkers, and number of steps per walker</span>
        <span class="k">if</span> <span class="n">nwalkers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">nsteps</span><span class="p">)</span>

        <span class="c1"># Optimize walker initialization?</span>
        <span class="k">if</span> <span class="n">opt_init</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># start walkers near the estimated maximum</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_map</span><span class="p">(</span><span class="n">prior_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># start walkers at random points in the prior space</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># set up multiprocessing pool with MPI safety</span>
        <span class="n">emcee_pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span> <span class="k">if</span> <span class="n">multi_proc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">emcee_ncore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="k">if</span> <span class="p">(</span><span class="n">multi_proc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running emcee with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="si">}</span><span class="s2"> walkers for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span><span class="si">}</span><span class="s2"> steps on </span><span class="si">{</span><span class="n">emcee_ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>

        <span class="c1"># Multi-run setup for minimum effective sample size</span>
        <span class="n">all_chains</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_run_times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accumulated_samples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="k">while</span> <span class="n">accumulated_samples</span> <span class="o">&lt;</span> <span class="n">min_ess</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">: Need </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">min_ess</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">accumulated_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> more samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
            
            <span class="c1"># Run the sampler!</span>
            <span class="n">emcee_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="p">,</span> 
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> 
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">lnprob</span><span class="p">,</span> 
                                                 <span class="n">pool</span><span class="o">=</span><span class="n">emcee_pool</span><span class="p">,</span>
                                                 <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>
            
            <span class="c1"># close pool </span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_close_pool</span><span class="p">(</span><span class="n">emcee_pool</span><span class="p">)</span>

            <span class="c1"># record emcee runtime</span>
            <span class="n">current_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">emcee_t0</span>
            <span class="n">all_run_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_runtime</span><span class="p">)</span>

            <span class="c1"># burn, thin, and flatten samples for this run</span>
            <span class="n">current_iburn</span><span class="p">,</span> <span class="n">current_ithin</span> <span class="o">=</span> <span class="n">mcmc_utils</span><span class="o">.</span><span class="n">estimate_burnin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="n">current_samples_full</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">()</span>

            <span class="n">current_burn</span> <span class="o">=</span> <span class="n">burn</span> <span class="k">if</span> <span class="n">burn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">current_iburn</span>
            <span class="n">current_thin</span> <span class="o">=</span> <span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">current_ithin</span>

            <span class="n">current_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">(</span><span class="n">discard</span><span class="o">=</span><span class="n">current_burn</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="n">current_thin</span><span class="p">,</span> <span class="n">flat</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">all_chains</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_samples</span><span class="p">)</span>
            
            <span class="n">current_nsamples</span> <span class="o">=</span> <span class="n">current_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">accumulated_samples</span> <span class="o">+=</span> <span class="n">current_nsamples</span>
            
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2"> complete: </span><span class="si">{</span><span class="n">current_nsamples</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total accumulated samples: </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># If min_ess requirement met, break</span>
            <span class="k">if</span> <span class="n">accumulated_samples</span> <span class="o">&gt;=</span> <span class="n">min_ess</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">run_number</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Reset initial positions for next run (start from end of previous run)</span>
            <span class="k">if</span> <span class="n">run_number</span> <span class="o">&lt;=</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Limit to prevent infinite loops</span>
                <span class="c1"># Use final positions from this run as starting positions for next run</span>
                <span class="n">p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">get_last_sample</span><span class="p">()</span><span class="o">.</span><span class="n">coords</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: Reached maximum of 10 runs, stopping with </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="c1"># Combine all chains and compute overall statistics</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_chains</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_chains</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Combined </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_chains</span><span class="p">)</span><span class="si">}</span><span class="s2"> runs into </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> total samples&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span> <span class="o">=</span> <span class="n">all_chains</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Use final run for chain statistics and samples_full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_full</span> <span class="o">=</span> <span class="n">current_samples_full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iburn</span> <span class="o">=</span> <span class="n">current_iburn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ithin</span> <span class="o">=</span> <span class="n">current_ithin</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">burn</span> <span class="o">=</span> <span class="n">current_burn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">current_thin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_runtime</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">all_run_times</span><span class="p">)</span> 
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;surrogate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span>

        <span class="c1"># get acceptance fraction and autocorrelation time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acc_frac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">acceptance_fraction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autcorr_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_sampler</span><span class="o">.</span><span class="n">get_autocorr_time</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total samples: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean acceptance fraction: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">acc_frac</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean autocorrelation time: </span><span class="si">{0:.3f}</span><span class="s2"> steps&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autcorr_time</span><span class="p">))</span>

        <span class="c1"># record that emcee has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="n">samples_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">samples_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/emcee_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.npz&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>   
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/emcee_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving final emcee samples to </span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2"> ...&quot;</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="p">)</span></div>

            

<div class="viewcode-block" id="SurrogateModel.run_dynesty">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_dynesty">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_dynesty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;dynamic&quot;</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{},</span>
                    <span class="n">multi_proc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_ess</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the dynesty nested sampling algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the dynesty package to perform nested sampling on either the </span>
<span class="sd">        trained GP surrogate model or the true likelihood function. Dynesty is particularly </span>
<span class="sd">        effective for estimating the Bayesian evidence and exploring multi-modal posteriors.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)  </span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform: (*callable or None, optional*)</span>
<span class="sd">            Prior transformation function that maps from unit hypercube [0,1]^ndim</span>
<span class="sd">            to the parameter space. Should have signature prior_transform(u) where</span>
<span class="sd">            u is array of shape (ndim,) with values in [0,1]. If None, uses uniform</span>
<span class="sd">            prior with bounds from self.bounds. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param mode: (*{&quot;dynamic&quot;, &quot;static&quot;}, optional*)</span>
<span class="sd">            Dynesty sampling mode. &quot;dynamic&quot; uses DynamicNestedSampler which adaptively</span>
<span class="sd">            allocates live points, while &quot;static&quot; uses fixed number of live points.</span>
<span class="sd">            Dynamic mode is generally more efficient. Default is &quot;dynamic&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the dynesty sampler constructor.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;nlive&#39;: Number of live points (default: 50*ndim)</span>
<span class="sd">            - &#39;bound&#39;: Bounding method (&#39;multi&#39;, &#39;single&#39;, &#39;none&#39;)</span>
<span class="sd">            - &#39;sample&#39;: Sampling method (&#39;auto&#39;, &#39;unif&#39;, &#39;rwalk&#39;, &#39;slice&#39;, &#39;rslice&#39;, &#39;hslice&#39;)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param run_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the run_nested() method.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;dlogz&#39;: Target evidence uncertainty (default: 0.5)</span>
<span class="sd">            - &#39;maxiter&#39;: Maximum number of iterations (default: 50000)</span>
<span class="sd">            - &#39;wt_kwargs&#39;: Weight function arguments (default: {&#39;pfrac&#39;: 1.0})</span>
<span class="sd">            - &#39;stop_kwargs&#39;: Stopping criterion arguments (default: {&#39;pfrac&#39;: 1.0})</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            Whether to use multiprocessing. If True, uses self.ncore processes.</span>
<span class="sd">            Note that multiprocessing can sometimes be slower due to overhead.</span>
<span class="sd">            Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param save_iter: (*int or None, optional*)</span>
<span class="sd">            If provided, saves the sampler state every save_iter iterations to allow</span>
<span class="sd">            for checkpointing and resuming long runs. Saves to </span>
<span class="sd">            &#39;{savedir}/dynesty_sampler_{like_fn_name}.pkl&#39;. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior transform for logging purposes. If None</span>
<span class="sd">            and prior_transform is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param samples_file: (*str or None, optional*)</span>
<span class="sd">            If provided, saves the final samples to this file in NumPy .npz format.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param min_ess: (*int, optional*)</span>
<span class="sd">            Minimum effective sample size. If the number of final samples is less than </span>
<span class="sd">            min_ess, will run additional sampling rounds and combine samples until the </span>
<span class="sd">            total number of samples exceeds min_ess. Default is 0 (no minimum required).</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        res : dynesty.results.Results</span>
<span class="sd">            Complete dynesty results object containing samples, weights, evidence, etc.</span>
<span class="sd">        dynesty_samples : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Resampled posterior samples with equal weights</span>
<span class="sd">        dynesty_samples_true : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        dynesty_samples_surrogate : ndarray of shape (nsamples, ndim)  </span>
<span class="sd">            Posterior samples when using surrogate likelihood</span>
<span class="sd">        dynesty_run : bool</span>
<span class="sd">            Flag indicating dynesty has been successfully run</span>
<span class="sd">        dynesty_runtime : float</span>
<span class="sd">            Wall-clock time taken for dynesty sampling in seconds</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;custom&quot;)</span>
<span class="sd">        prior_transform_comment : str</span>
<span class="sd">            Description of prior transform used</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Dynesty is particularly well-suited for:</span>
<span class="sd">        - Computing Bayesian evidence for model comparison</span>
<span class="sd">        - Exploring multi-modal posteriors</span>
<span class="sd">        - Providing robust posterior sampling without tuning</span>
<span class="sd">        </span>
<span class="sd">        The default settings prioritize posterior sampling over evidence estimation</span>
<span class="sd">        by setting pfrac=1.0, which focuses computational effort on high-likelihood</span>
<span class="sd">        regions rather than exploring the full prior volume.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Sample surrogate model with default settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty()</span>
<span class="sd">        </span>
<span class="sd">        Sample true likelihood with more live points:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(like_fn=&quot;true&quot;, sampler_kwargs={&#39;nlive&#39;: 1000})</span>
<span class="sd">        </span>
<span class="sd">        Use custom prior with bounds [-5, 5] for each parameter:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; def my_prior(u):</span>
<span class="sd">        ...     return 10*u - 5  # maps [0,1] to [-5,5]</span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(prior_transform=my_prior)</span>
<span class="sd">        </span>
<span class="sd">        Run with checkpointing every 1000 iterations:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(save_iter=1000, run_kwargs={&#39;maxiter&#39;: 50000})</span>
<span class="sd">        </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Dynesty documentation: https://dynesty.readthedocs.io/</span>
<span class="sd">        Speagle (2020): &quot;dynesty: a dynamic nested sampling package for estimating</span>
<span class="sd">        Bayesian posteriors and evidences&quot;, MNRAS, 493, 3132-3158</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">dynesty</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedSampler</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">DynamicNestedSampler</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">dyfunc</span>
        
        <span class="c1"># Determine likelihood function and name</span>
        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">like_fn</span><span class="p">):</span>
            <span class="c1"># like_fn is a function/callable</span>
            <span class="k">if</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Custom function provided</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with user-provided likelihood function.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;custom&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">like_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># like_fn is a string</span>
            <span class="n">like_fn_lower</span> <span class="o">=</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">like_fn_lower</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;surrogate&quot;</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">,</span> <span class="s2">&quot;surrogate_log_likelihood&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn_lower</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="s2">&quot;true_log_likelihood&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown string identifier for like_fn: &#39;</span><span class="si">{</span><span class="n">like_fn</span><span class="si">}</span><span class="s2">&#39;. &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;Valid options: &#39;surrogate&#39;, &#39;true&#39;, &#39;gp&#39;, &#39;surrogate_log_likelihood&#39;, &#39;true_log_likelihood&#39;&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;like_fn must be None, a string, or a callable function. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Received type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">like_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="c1"># set up prior transform</span>
        <span class="k">if</span> <span class="n">prior_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_transform_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

            <span class="c1"># Comment for output log file</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span>  <span class="sa">f</span><span class="s2">&quot;Default uniform prior transform. </span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: ut.prior_transform_uniform</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span> <span class="o">=</span> <span class="n">prior_transform</span>

            <span class="c1"># Comment for output log file</span>
            <span class="k">if</span> <span class="n">prior_transform_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;User defined prior transform.&quot;</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="s2">&quot;Prior function: unrecorded&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="n">prior_transform_comment</span>

        <span class="c1"># start timing dynesty</span>
        <span class="n">dynesty_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="c1"># set up sampler kwargs</span>
        <span class="n">default_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;bound&quot;</span><span class="p">:</span> <span class="s2">&quot;multi&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;nlive&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                  <span class="s2">&quot;sample&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">}</span>
        
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">default_sampler_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sampler_kwargs</span><span class="p">:</span>
                <span class="n">sampler_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_sampler_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        
        <span class="c1"># set up multiprocessing pool </span>
        <span class="c1"># default to false. multiprocessing usually slower for some reason</span>
        <span class="n">dynesty_pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pool</span><span class="p">(</span><span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span> <span class="k">if</span> <span class="n">multi_proc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dynesty_pool</span>
        <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;queue_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="k">if</span> <span class="p">(</span><span class="n">multi_proc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">dynesty_ncore</span> <span class="o">=</span> <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;queue_size&quot;</span><span class="p">]</span>

        <span class="c1"># initialize our nested sampler</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;dynamic&quot;</span><span class="p">:</span>
            <span class="n">dsampler</span> <span class="o">=</span> <span class="n">DynamicNestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                            <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                            <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized dynesty DynamicNestedSampler.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span><span class="p">:</span>
            <span class="n">dsampler</span> <span class="o">=</span> <span class="n">NestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                     <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized dynesty NestedSampler.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mode </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> is not a valid option. Choose &#39;dynamic&#39; or &#39;static&#39;.&quot;</span><span class="p">)</span>
        
        <span class="c1"># set up run kwargs. default: 100% weight on posterior, 0% evidence</span>
        <span class="n">default_run_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;wt_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;pfrac&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span>
                              <span class="s2">&quot;stop_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;pfrac&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span>
                              <span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">),</span>
                              <span class="s2">&quot;dlogz_init&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">default_run_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">run_kwargs</span><span class="p">:</span>
                <span class="n">run_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_run_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running dynesty with </span><span class="si">{</span><span class="n">sampler_kwargs</span><span class="p">[</span><span class="s1">&#39;nlive&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> live points on </span><span class="si">{</span><span class="n">dynesty_ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>

        <span class="c1"># Multi-run setup for minimum effective sample size</span>
        <span class="n">all_samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_logz</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_run_times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accumulated_samples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="k">while</span> <span class="n">accumulated_samples</span> <span class="o">&lt;</span> <span class="n">min_ess</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">: Need </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">min_ess</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">accumulated_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> more samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
            
            <span class="c1"># Set timing for this run</span>
            <span class="n">run_start_time</span> <span class="o">=</span> <span class="n">dynesty_t0</span> <span class="k">if</span> <span class="n">run_number</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            
            <span class="c1"># Pickle sampler?</span>
            <span class="k">if</span> <span class="n">save_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">last_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">run_sampler</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="n">dsampler</span><span class="o">.</span><span class="n">run_nested</span><span class="p">(</span><span class="n">maxiter</span><span class="o">=</span><span class="n">save_iter</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>
                    <span class="n">current_results</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span>

                    <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;dynesty_sampler_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">.pkl&quot;</span><span class="p">)</span>

                    <span class="c1"># pickle dynesty sampler object</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Caching model to </span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>        
                        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dsampler</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

                    <span class="c1"># check if converged (i.e. hasn&#39;t run for more iterations)</span>
                    <span class="k">if</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">niter</span> <span class="o">&gt;</span> <span class="n">last_iter</span><span class="p">:</span>
                        <span class="n">last_iter</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">niter</span>
                        <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dsampler</span><span class="o">.</span><span class="n">run_nested</span><span class="p">(</span><span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>
                <span class="n">current_results</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span>

            <span class="c1"># Record current run statistics</span>
            <span class="n">current_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">run_start_time</span>
            <span class="n">all_run_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_runtime</span><span class="p">)</span>
            
            <span class="c1"># Get samples and weights for this run</span>
            <span class="n">current_samples</span> <span class="o">=</span> <span class="n">current_results</span><span class="o">.</span><span class="n">samples</span>
            <span class="n">current_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">current_results</span><span class="o">.</span><span class="n">logwt</span> <span class="o">-</span> <span class="n">current_results</span><span class="o">.</span><span class="n">logz</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">current_logz</span> <span class="o">=</span> <span class="n">current_results</span><span class="o">.</span><span class="n">logz</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Resample weighted samples for this run</span>
            <span class="n">current_resampled</span> <span class="o">=</span> <span class="n">dyfunc</span><span class="o">.</span><span class="n">resample_equal</span><span class="p">(</span><span class="n">current_samples</span><span class="p">,</span> <span class="n">current_weights</span><span class="p">)</span>
            <span class="n">all_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_resampled</span><span class="p">)</span>
            <span class="n">all_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>
            <span class="n">all_logz</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_logz</span><span class="p">)</span>
            
            <span class="n">current_nsamples</span> <span class="o">=</span> <span class="n">current_resampled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">accumulated_samples</span> <span class="o">+=</span> <span class="n">current_nsamples</span>
            
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2"> complete: </span><span class="si">{</span><span class="n">current_nsamples</span><span class="si">}</span><span class="s2"> samples, logZ = </span><span class="si">{</span><span class="n">current_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total accumulated samples: </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># If min_ess requirement met, break</span>
            <span class="k">if</span> <span class="n">accumulated_samples</span> <span class="o">&gt;=</span> <span class="n">min_ess</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">run_number</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Setup for next run</span>
            <span class="k">if</span> <span class="n">run_number</span> <span class="o">&lt;=</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Limit to prevent infinite loops</span>
                <span class="c1"># Create a new sampler for the next run</span>
                <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;dynamic&quot;</span><span class="p">:</span>
                    <span class="n">dsampler</span> <span class="o">=</span> <span class="n">dynesty</span><span class="o">.</span><span class="n">DynamicNestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                                            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                                            <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> 
                                                            <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dsampler</span> <span class="o">=</span> <span class="n">dynesty</span><span class="o">.</span><span class="n">NestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                                     <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                                     <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> 
                                                     <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: Reached maximum of 10 runs, stopping with </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="c1"># Combine results from all runs</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>
            <span class="c1"># Use the best (highest) log evidence</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_logz</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">all_logz</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Combined </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> runs into </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> total samples&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best log evidence: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dynesty_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span> <span class="o">=</span> <span class="n">all_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_logz</span> <span class="o">=</span> <span class="n">all_logz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Use final run for primary results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_sampler</span> <span class="o">=</span> <span class="n">dsampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_results</span> <span class="o">=</span> <span class="n">current_results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_logz_err</span> <span class="o">=</span> <span class="n">current_results</span><span class="o">.</span><span class="n">logzerr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_runtime</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">all_run_times</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;surrogate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples_surrogate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span>  

        <span class="c1"># record that dynesty has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="c1"># record dynesty runtime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">dynesty_t0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="n">samples_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">samples_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/dynesty_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.npz&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/dynesty_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved dynesty samples to </span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.run_pymultinest">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_pymultinest">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_pymultinest</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">multi_proc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                        <span class="n">prior_transform_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                        <span class="n">n_clustering_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outputfiles_basename</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_ess</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the PyMultiNest nested sampling algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the PyMultiNest package (Python wrapper for MultiNest) to perform </span>
<span class="sd">        nested sampling on either the trained GP surrogate model or the true likelihood function.</span>
<span class="sd">        MultiNest is particularly effective for multi-modal posteriors and computing Bayesian </span>
<span class="sd">        evidence with high accuracy.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)</span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform: (*callable or None, optional*)</span>
<span class="sd">            Prior transformation function that maps from unit hypercube [0,1]^ndim</span>
<span class="sd">            to the parameter space. Should have signature prior_transform(cube) where cube</span>
<span class="sd">            is array of shape (ndim,) with values in [0,1]. The function should modify</span>
<span class="sd">            cube in-place. If None, uses uniform prior with bounds from self.bounds.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to pymultinest.run().</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;n_live_points&#39;: Number of live points (default: 1000)</span>
<span class="sd">            - &#39;evidence_tolerance&#39;: Target evidence uncertainty (default: 0.5)</span>
<span class="sd">            - &#39;sampling_efficiency&#39;: Sampling efficiency parameter (default: 0.8)</span>
<span class="sd">            - &#39;n_iter_before_update&#39;: Iterations before evidence/posterior update (default: 100)</span>
<span class="sd">            - &#39;null_log_evidence&#39;: Null evidence for model comparison (default: -1e90)</span>
<span class="sd">            - &#39;max_modes&#39;: Maximum number of modes to find (default: 100)</span>
<span class="sd">            - &#39;mode_tolerance&#39;: Mode separation tolerance (default: -1e90)</span>
<span class="sd">            - &#39;seed&#39;: Random seed for reproducibility (default: -1, auto)</span>
<span class="sd">            - &#39;verbose&#39;: Verbosity level (default: True)</span>
<span class="sd">            - &#39;importance_nested_sampling&#39;: Use importance nested sampling (default: True)</span>
<span class="sd">            - &#39;multimodal&#39;: Enable multimodal mode detection (default: True)</span>
<span class="sd">            - &#39;const_efficiency_mode&#39;: Use constant efficiency mode (default: False)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            Whether to use multiprocessing. If True, uses self.ncore processes.</span>
<span class="sd">            MultiNest handles parallelization internally when MPI is available.</span>
<span class="sd">            </span>
<span class="sd">            .. note::</span>
<span class="sd">                This parameter is ignored for PyMultiNest as it uses MPI for </span>
<span class="sd">                parallelization, not Python&#39;s multiprocessing. When PyMultiNest</span>
<span class="sd">                runs with MPI, other alabi functions automatically disable their</span>
<span class="sd">                multiprocessing pools to avoid conflicts.</span>
<span class="sd">                </span>
<span class="sd">            Default is True.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior function for logging purposes. If None</span>
<span class="sd">            and prior_transform is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param samples_file: (*str or None, optional*)</span>
<span class="sd">            If provided, saves the final samples to this file in NumPy .npz format.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prefix: (*str or None, optional*)</span>
<span class="sd">            Prefix for MultiNest output files. If None, uses default based on</span>
<span class="sd">            likelihood function name and current directory. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param resume: (*bool, optional*)</span>
<span class="sd">            Whether to resume from previous run if output files exist.</span>
<span class="sd">            Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param n_clustering_params: (*int or None, optional*)</span>
<span class="sd">            Number of parameters to use for mode clustering. If None, uses all</span>
<span class="sd">            parameters (ndim). Set to lower value if some parameters are nuisance.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param outputfiles_basename: (*str or None, optional*)</span>
<span class="sd">            Base name for MultiNest output files. If None, constructs from savedir</span>
<span class="sd">            and likelihood function name. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param min_ess: (*int, optional*)</span>
<span class="sd">            Minimum effective sample size. If the number of final samples is less than </span>
<span class="sd">            min_ess, will run additional sampling rounds and combine samples until the </span>
<span class="sd">            total number of samples exceeds min_ess. Default is 0 (no minimum required).</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        pymultinest_samples : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples from MultiNest</span>
<span class="sd">        pymultinest_samples_true : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        pymultinest_samples_surrogate : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using surrogate likelihood</span>
<span class="sd">        pymultinest_weights : ndarray of shape (nsamples,)</span>
<span class="sd">            Sample weights from nested sampling</span>
<span class="sd">        pymultinest_logz : float</span>
<span class="sd">            Log Bayesian evidence estimate</span>
<span class="sd">        pymultinest_logz_err : float</span>
<span class="sd">            Uncertainty in log evidence estimate</span>
<span class="sd">        pymultinest_run : bool</span>
<span class="sd">            Flag indicating PyMultiNest has been successfully run</span>
<span class="sd">        pymultinest_runtime : float</span>
<span class="sd">            Wall-clock time taken for MultiNest sampling in seconds</span>
<span class="sd">        pymultinest_analyzer : pymultinest.Analyzer</span>
<span class="sd">            MultiNest analyzer object for accessing detailed results</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;custom&quot;)</span>
<span class="sd">        prior_transform_comment : str</span>
<span class="sd">            Description of prior function used</span>
<span class="sd">            </span>
<span class="sd">        .. note::</span>
<span class="sd">            PyMultiNest is particularly well-suited for:</span>
<span class="sd">            </span>
<span class="sd">            - Multi-modal posterior exploration with automatic mode detection</span>
<span class="sd">            - High-accuracy Bayesian evidence computation for model comparison</span>
<span class="sd">            - Robust sampling without manual tuning of MCMC parameters</span>
<span class="sd">            - Handling complex, irregular posterior shapes</span>
<span class="sd">            </span>
<span class="sd">            MultiNest generates several output files including detailed posterior</span>
<span class="sd">            samples, evidence estimates, and mode information. These files are</span>
<span class="sd">            saved to the model&#39;s savedir for later analysis.</span>
<span class="sd">            </span>
<span class="sd">            **MPI and Multiprocessing Compatibility:**</span>
<span class="sd">            </span>
<span class="sd">            PyMultiNest uses MPI for parallelization across multiple nodes/cores.</span>
<span class="sd">            When MPI is active, alabi automatically disables Python multiprocessing</span>
<span class="sd">            in other functions (run_emcee, run_dynesty) to prevent conflicts.</span>
<span class="sd">            This ensures that:</span>
<span class="sd">            </span>
<span class="sd">            - PyMultiNest can run efficiently with MPI</span>
<span class="sd">            - Other alabi functions fall back to serial execution when MPI is detected</span>
<span class="sd">            - No deadlocks or resource conflicts occur between MPI and multiprocessing</span>
<span class="sd">            </span>
<span class="sd">            To run PyMultiNest with MPI:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # Single node, multiple cores</span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest()  # Uses OpenMP if available</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # Multiple nodes with MPI (run from command line)</span>
<span class="sd">            &gt;&gt;&gt; # mpirun -n 4 python your_script.py</span>
<span class="sd">            </span>
<span class="sd">        :example:</span>
<span class="sd">            Sample surrogate model with default settings:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest()</span>
<span class="sd">            </span>
<span class="sd">            Sample true likelihood with more live points:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest(like_fn=&quot;true&quot;, </span>
<span class="sd">            ...                   sampler_kwargs={&#39;n_live_points&#39;: 2000})</span>
<span class="sd">            </span>
<span class="sd">            Use custom prior with bounds [-10, 10] for each parameter:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; def my_prior(cube):</span>
<span class="sd">            ...     for i in range(len(cube)):</span>
<span class="sd">            ...         cube[i] = 20*cube[i] - 10  # maps [0,1] to [-10,10]</span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest(prior_transform=my_prior)</span>
<span class="sd">            </span>
<span class="sd">            Enable multimodal mode detection with high accuracy:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest(sampler_kwargs={</span>
<span class="sd">            ...     &#39;multimodal&#39;: True,</span>
<span class="sd">            ...     &#39;evidence_tolerance&#39;: 0.1,</span>
<span class="sd">            ...     &#39;max_modes&#39;: 20</span>
<span class="sd">            ... })</span>
<span class="sd">            </span>
<span class="sd">            Run with custom output file prefix and resume capability:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_pymultinest(prefix=&quot;my_run_&quot;, resume=True)</span>
<span class="sd">            </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        PyMultiNest documentation: https://johannesbuchner.github.io/PyMultiNest/</span>
<span class="sd">        Feroz et al. (2009): &quot;MultiNest: an efficient and robust Bayesian inference</span>
<span class="sd">        tool for cosmology and particle physics&quot;, MNRAS, 398, 1601-1614</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">pymultinest</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;PyMultiNest is required but not installed. &quot;</span>
                            <span class="s2">&quot;Install with: pip install pymultinest&quot;</span><span class="p">)</span>
        
        <span class="c1"># Start timing</span>
        <span class="n">pymultinest_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="c1"># Determine likelihood function and name</span>
        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">like_fn</span><span class="p">):</span>
            <span class="c1"># like_fn is a function/callable</span>
            <span class="k">if</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Custom function provided</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with user-provided likelihood function.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;custom&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">like_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;surrogate&quot;</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing PyMultiNest with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid like_fn string: </span><span class="si">{</span><span class="n">like_fn</span><span class="si">}</span><span class="s2">. &quot;</span>
                               <span class="s2">&quot;Must be &#39;surrogate&#39;, &#39;gp&#39;, &#39;true&#39;, or callable.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;like_fn must be callable, string, or None. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">like_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Set up prior transformation function</span>
        <span class="k">if</span> <span class="n">prior_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default uniform prior using self.bounds</span>
            <span class="n">prior_transform_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_transform_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Default uniform prior transform.</span><span class="se">\n</span><span class="s2">Prior function: ut.prior_transform_uniform</span><span class="se">\n</span><span class="s2">Bounds: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prior_transform_fn</span> <span class="o">=</span> <span class="n">prior_transform</span>
            <span class="k">if</span> <span class="n">prior_transform_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prior_transform</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Custom prior function: </span><span class="si">{</span><span class="n">prior_transform</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="s2">&quot;Custom prior function&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="n">prior_transform_comment</span>
        
        <span class="c1"># Create PyMultiNest-compatible wrapper for prior transform</span>
        <span class="c1"># PyMultiNest expects Prior(cube, ndim, nparams) where cube is modified in-place</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">pymultinest_prior_wrapper</span><span class="p">(</span><span class="n">cube</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">nparams</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Wrapper to make prior transform compatible with PyMultiNest calling convention.&quot;&quot;&quot;</span>
            <span class="c1"># Extract values from ctypes array to regular Python list</span>
            <span class="n">cube_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">cube</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)]</span>
            <span class="n">cube_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cube_values</span><span class="p">)</span>
            
            <span class="c1"># Apply the prior transformation</span>
            <span class="n">transformed</span> <span class="o">=</span> <span class="n">prior_transform_fn</span><span class="p">(</span><span class="n">cube_array</span><span class="p">)</span>
            
            <span class="c1"># Modify cube in-place as expected by PyMultiNest</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
                <span class="n">cube</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        <span class="c1"># Use the wrapper as the actual prior function</span>
        <span class="n">prior_transform</span> <span class="o">=</span> <span class="n">pymultinest_prior_wrapper</span>
        
        <span class="c1"># Create PyMultiNest-compatible wrapper for likelihood function  </span>
        <span class="c1"># PyMultiNest expects LogLikelihood(cube, ndim, nparams) where cube contains parameters</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">pymultinest_likelihood_wrapper</span><span class="p">(</span><span class="n">cube</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">nparams</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Wrapper to make likelihood function compatible with PyMultiNest calling convention.&quot;&quot;&quot;</span>
            <span class="c1"># Extract values from ctypes array to regular Python list</span>
            <span class="n">params_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">cube</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)]</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">params_values</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c1"># Use the wrapper as the actual likelihood function</span>
        <span class="n">likelihood_fn</span> <span class="o">=</span> <span class="n">pymultinest_likelihood_wrapper</span>
        
        <span class="c1"># Set up output file basename</span>
        <span class="k">if</span> <span class="n">outputfiles_basename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;pymultinest_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">_&quot;</span>
            <span class="n">outputfiles_basename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Set default sampler kwargs</span>
        <span class="n">default_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;n_live_points&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
            <span class="s1">&#39;evidence_tolerance&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
            <span class="s1">&#39;sampling_efficiency&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
            <span class="s1">&#39;n_iter_before_update&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
            <span class="s1">&#39;null_log_evidence&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mf">1e90</span><span class="p">,</span>
            <span class="s1">&#39;max_modes&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
            <span class="s1">&#39;mode_tolerance&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mf">1e90</span><span class="p">,</span>
            <span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;importance_nested_sampling&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;multimodal&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;const_efficiency_mode&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">}</span>
        
        <span class="c1"># Update with user-provided kwargs</span>
        <span class="n">final_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">default_sampler_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">}</span>
        
        <span class="c1"># Set clustering parameters</span>
        <span class="k">if</span> <span class="n">n_clustering_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_clustering_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>
        
        <span class="c1"># Multi-run setup for minimum effective sample size</span>
        <span class="n">all_samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_logz</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_logz_err</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_run_times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accumulated_samples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="k">while</span> <span class="n">accumulated_samples</span> <span class="o">&lt;</span> <span class="nb">max</span><span class="p">(</span><span class="n">min_ess</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">: Need </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">min_ess</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">accumulated_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> more samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
            
            <span class="c1"># Create unique output basename for this run</span>
            <span class="n">current_outputfiles_basename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outputfiles_basename</span><span class="si">}</span><span class="s2">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">outputfiles_basename</span>
            
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running PyMultiNest with </span><span class="si">{</span><span class="n">final_sampler_kwargs</span><span class="p">[</span><span class="s1">&#39;n_live_points&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> live points...&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evidence tolerance: </span><span class="si">{</span><span class="n">final_sampler_kwargs</span><span class="p">[</span><span class="s1">&#39;evidence_tolerance&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output files: </span><span class="si">{</span><span class="n">current_outputfiles_basename</span><span class="si">}</span><span class="s2">*&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># Start timing this run</span>
            <span class="n">current_pymultinest_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            
            <span class="c1"># Run MultiNest</span>
            <span class="n">pymultinest</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="n">LogLikelihood</span><span class="o">=</span><span class="n">likelihood_fn</span><span class="p">,</span>
                <span class="n">Prior</span><span class="o">=</span><span class="n">prior_transform</span><span class="p">,</span>
                <span class="n">n_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                <span class="n">n_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                <span class="n">n_clustering_params</span><span class="o">=</span><span class="n">n_clustering_params</span><span class="p">,</span>
                <span class="n">outputfiles_basename</span><span class="o">=</span><span class="n">current_outputfiles_basename</span><span class="p">,</span>
                <span class="n">resume</span><span class="o">=</span><span class="n">resume</span><span class="p">,</span>
                <span class="o">**</span><span class="n">final_sampler_kwargs</span>
            <span class="p">)</span>
            
            <span class="c1"># Fix malformed scientific notation in PyMultiNest output files</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fix_pymultinest_output_format</span><span class="p">(</span><span class="n">current_outputfiles_basename</span><span class="p">)</span>
            
            <span class="c1"># Analyze results for this run</span>
            <span class="n">current_analyzer</span> <span class="o">=</span> <span class="n">pymultinest</span><span class="o">.</span><span class="n">Analyzer</span><span class="p">(</span>
                <span class="n">outputfiles_basename</span><span class="o">=</span><span class="n">current_outputfiles_basename</span><span class="p">,</span>
                <span class="n">n_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>
            <span class="p">)</span>
            
            <span class="c1"># Get samples and evidence for this run</span>
            <span class="n">current_samples</span> <span class="o">=</span> <span class="n">current_analyzer</span><span class="o">.</span><span class="n">get_equal_weighted_posterior</span><span class="p">()</span>
            <span class="n">current_samples_only</span> <span class="o">=</span> <span class="n">current_samples</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Remove log-likelihood column</span>
            <span class="n">current_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">current_samples_only</span><span class="p">))</span>  <span class="c1"># Equal weights</span>
            
            <span class="c1"># Get evidence estimates for this run</span>
            <span class="n">current_stats</span> <span class="o">=</span> <span class="n">current_analyzer</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
            <span class="n">current_logz</span> <span class="o">=</span> <span class="n">current_stats</span><span class="p">[</span><span class="s1">&#39;global evidence&#39;</span><span class="p">]</span>
            <span class="n">current_logz_err</span> <span class="o">=</span> <span class="n">current_stats</span><span class="p">[</span><span class="s1">&#39;global evidence error&#39;</span><span class="p">]</span>
            
            <span class="c1"># Record this run&#39;s results</span>
            <span class="n">all_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_samples_only</span><span class="p">)</span>
            <span class="n">all_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>
            <span class="n">all_logz</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_logz</span><span class="p">)</span>
            <span class="n">all_logz_err</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_logz_err</span><span class="p">)</span>
            
            <span class="n">current_nsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_samples_only</span><span class="p">)</span>
            <span class="n">accumulated_samples</span> <span class="o">+=</span> <span class="n">current_nsamples</span>
            
            <span class="c1"># Record runtime for this run</span>
            <span class="n">current_run_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">current_pymultinest_t0</span>
            <span class="n">all_run_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_run_time</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2"> complete: </span><span class="si">{</span><span class="n">current_nsamples</span><span class="si">}</span><span class="s2"> samples, logZ = </span><span class="si">{</span><span class="n">current_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">current_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total accumulated samples: </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># If min_ess requirement met, break</span>
            <span class="k">if</span> <span class="n">accumulated_samples</span> <span class="o">&gt;=</span> <span class="n">min_ess</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">run_number</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Limit to prevent infinite loops</span>
            <span class="k">if</span> <span class="n">run_number</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: Reached maximum of 10 runs, stopping with </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="c1"># Combine results from all runs</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">all_weights</span><span class="p">)</span>
            <span class="c1"># Use weighted average of log evidence (weights by number of samples)</span>
            <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_samples</span><span class="p">])</span>
            <span class="n">total_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_counts</span><span class="p">)</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">sample_counts</span> <span class="o">/</span> <span class="n">total_samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">all_logz</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_logz_err</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Combined </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> runs into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> total samples&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Combined log evidence: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span> <span class="o">=</span> <span class="n">all_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_weights</span> <span class="o">=</span> <span class="n">all_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz</span> <span class="o">=</span> <span class="n">all_logz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz_err</span> <span class="o">=</span> <span class="n">all_logz_err</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Set the final analyzer to the last run for compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_analyzer</span> <span class="o">=</span> <span class="n">current_analyzer</span>
        
        <span class="c1"># Calculate total runtime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_runtime</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">all_run_times</span><span class="p">)</span>
        
        <span class="c1"># Store samples based on likelihood function used</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyMultiNest complete. </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples_true</span><span class="p">)</span><span class="si">}</span><span class="s2"> posterior samples collected.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples_surrogate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyMultiNest complete. </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples_surrogate</span><span class="p">)</span><span class="si">}</span><span class="s2"> posterior samples collected.&quot;</span><span class="p">)</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log Evidence: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Record that PyMultiNest has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_run</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyMultiNest runtime: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_runtime</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        
        <span class="c1"># Save results if caching is enabled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
            
        <span class="c1"># Save samples to file</span>
        <span class="k">if</span> <span class="n">samples_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">samples_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/pymultinest_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.npz&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/pymultinest_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved PyMultiNest samples to </span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> 
                <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_samples</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_weights</span><span class="p">,</span>
                <span class="n">logz</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz</span><span class="p">,</span>
                <span class="n">logz_err</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pymultinest_logz_err</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.run_ultranest">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_ultranest">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_ultranest</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{},</span>
                      <span class="n">multi_proc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">prior_transform_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">log_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="s2">&quot;overwrite&quot;</span><span class="p">,</span> <span class="n">min_ess</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">),</span> <span class="n">slice_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the UltraNest nested sampling algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the UltraNest package to perform nested sampling on either</span>
<span class="sd">        the trained GP surrogate model or the true likelihood function. UltraNest</span>
<span class="sd">        is a highly robust nested sampling algorithm that automatically adapts to</span>
<span class="sd">        the problem complexity and provides reliable evidence computation.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)</span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform: (*callable or None, optional*)</span>
<span class="sd">            Prior transformation function that maps from unit hypercube [0,1]^ndim</span>
<span class="sd">            to the parameter space. Should have signature prior_transform(cube) where</span>
<span class="sd">            cube is array of shape (ndim,) with values in [0,1]. Must return transformed</span>
<span class="sd">            parameters as array. If None, creates uniform prior from self.bounds.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to UltraNest ReactiveNestedSampler().</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;derived_param_names&#39;: List of derived parameter names (default: [])</span>
<span class="sd">            - &#39;wrapped_params&#39;: List of bool indicating circular parameters (default: None)</span>
<span class="sd">            - &#39;resume&#39;: Resume behavior &#39;resume&#39;/&#39;overwrite&#39;/&#39;subfolder&#39; (default: &#39;subfolder&#39;)</span>
<span class="sd">            - &#39;run_num&#39;: Run number for subdirectory creation (default: None)</span>
<span class="sd">            - &#39;num_test_samples&#39;: Number of test samples for validation (default: 2)</span>
<span class="sd">            - &#39;draw_multiple&#39;: Enable dynamic point drawing (default: True)  </span>
<span class="sd">            - &#39;num_bootstraps&#39;: Number of bootstrap samples (default: 30)</span>
<span class="sd">            - &#39;vectorized&#39;: Whether functions accept arrays (default: False)</span>
<span class="sd">            - &#39;ndraw_min&#39;: Minimum points to draw per iteration (default: 128)</span>
<span class="sd">            - &#39;ndraw_max&#39;: Maximum points to draw per iteration (default: 65536)</span>
<span class="sd">            - &#39;storage_backend&#39;: Storage format &#39;hdf5&#39;/&#39;csv&#39;/&#39;tsv&#39; (default: &#39;hdf5&#39;)</span>
<span class="sd">            - &#39;warmstart_max_tau&#39;: Warmstart maximum tau (default: -1)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param run_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to sampler.run().</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;update_interval_volume_fraction&#39;: Volume fraction for region updates (default: 0.8)</span>
<span class="sd">            - &#39;update_interval_ncall&#39;: Number of calls between updates (optional, omit for auto)</span>
<span class="sd">            - &#39;log_interval&#39;: Iterations between status updates (optional, omit for auto)</span>
<span class="sd">            - &#39;show_status&#39;: Show integration progress (default: True)</span>
<span class="sd">            - &#39;viz_callback&#39;: Visualization callback function (default: False, disabled)</span>
<span class="sd">            - &#39;dlogz&#39;: Target log-evidence uncertainty (default: 0.5)</span>
<span class="sd">            - &#39;dKL&#39;: Target posterior uncertainty in nats (default: 0.5)</span>
<span class="sd">            - &#39;frac_remain&#39;: Fraction of evidence remaining to terminate (default: 0.01)</span>
<span class="sd">            - &#39;Lepsilon&#39;: Likelihood contour tolerance (default: 0.001)</span>
<span class="sd">            - &#39;min_ess&#39;: Target effective sample size (default: 400)</span>
<span class="sd">            - &#39;max_iters&#39;: Maximum number of iterations (optional, omit for unlimited)</span>
<span class="sd">            - &#39;max_ncalls&#39;: Maximum number of likelihood calls (optional, omit for unlimited)</span>
<span class="sd">            - &#39;max_num_improvement_loops&#39;: Maximum improvement loops (default: -1)</span>
<span class="sd">            - &#39;min_num_live_points&#39;: Minimum number of live points (default: 400)</span>
<span class="sd">            - &#39;cluster_num_live_points&#39;: Live points per cluster (default: 40)</span>
<span class="sd">            - &#39;insertion_test_zscore_threshold&#39;: Z-score threshold for insertion test (default: 4)</span>
<span class="sd">            - &#39;insertion_test_window&#39;: Window size for insertion test (default: 10)</span>
<span class="sd">            - &#39;region_class&#39;: Region sampling class (optional, can be passed via kwargs)</span>
<span class="sd">            - &#39;widen_before_initial_plateau_num_warn&#39;: Warning threshold for plateau (default: 10000)</span>
<span class="sd">            - &#39;widen_before_initial_plateau_num_max&#39;: Maximum plateau points (optional, omit for auto)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            **Deprecated and ignored.** This parameter is kept for backwards compatibility</span>
<span class="sd">            but no longer has any effect. UltraNest now runs in MPI-compatible mode without</span>
<span class="sd">            multiprocessing pools to avoid conflicts with MPI environments.</span>
<span class="sd">            Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior transform for logging purposes. If None</span>
<span class="sd">            and prior_transform is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param samples_file: (*str or None, optional*)</span>
<span class="sd">            If provided, saves the final samples to this file in NumPy .npz format.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param log_dir: (*str or None, optional*)</span>
<span class="sd">            Directory to store UltraNest output files and logs. If None, uses</span>
<span class="sd">            a subdirectory in self.savedir. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param resume: (*str, optional*)</span>
<span class="sd">            Resume behavior for interrupted runs. Options:</span>
<span class="sd">            - &#39;resume&#39;: Resume if possible, otherwise start fresh</span>
<span class="sd">            - &#39;resume-similar&#39;: Resume with similar but not identical setup</span>
<span class="sd">            - &#39;overwrite&#39;: Always start fresh, overwriting existing files (default)</span>
<span class="sd">            - &#39;subfolder&#39;: Create new timestamped subfolder</span>
<span class="sd">            Default is &#39;overwrite&#39;.</span>
<span class="sd">            </span>
<span class="sd">        :param min_ess: (*int, optional*)</span>
<span class="sd">            Minimum effective sample size. If the number of final samples is less than </span>
<span class="sd">            min_ess, will run additional sampling rounds and combine samples until the </span>
<span class="sd">            total number of samples exceeds min_ess. Default is 0 (no minimum required).</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        ultranest_results : ultranest.integrator.Result</span>
<span class="sd">            Complete UltraNest results object with samples, evidence, etc.</span>
<span class="sd">        ultranest_samples : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Equally weighted posterior samples from UltraNest</span>
<span class="sd">        ultranest_samples_true : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        ultranest_samples_surrogate : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using surrogate likelihood</span>
<span class="sd">        ultranest_weights : ndarray of shape (nsamples,)</span>
<span class="sd">            Sample weights (typically all equal after resampling)</span>
<span class="sd">        ultranest_logz : float</span>
<span class="sd">            Log Bayesian evidence estimate</span>
<span class="sd">        ultranest_logz_err : float</span>
<span class="sd">            Uncertainty in log evidence estimate</span>
<span class="sd">        ultranest_run : bool</span>
<span class="sd">            Flag indicating UltraNest has been successfully run</span>
<span class="sd">        ultranest_runtime : float</span>
<span class="sd">            Wall-clock time taken for UltraNest sampling in seconds</span>
<span class="sd">        ultranest_sampler : ultranest.ReactiveNestedSampler</span>
<span class="sd">            UltraNest sampler object for accessing detailed information</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;custom&quot;)</span>
<span class="sd">        prior_transform_comment : str</span>
<span class="sd">            Description of prior transform used</span>
<span class="sd">            </span>
<span class="sd">        .. note::</span>
<span class="sd">            UltraNest is particularly well-suited for:</span>
<span class="sd">            </span>
<span class="sd">            - Robust nested sampling without manual tuning</span>
<span class="sd">            - Automatic adaptation to problem complexity</span>
<span class="sd">            - High-dimensional and multi-modal problems</span>
<span class="sd">            - Reliable evidence computation for model comparison</span>
<span class="sd">            - Problems with complex, irregular likelihood shapes</span>
<span class="sd">            - MPI environments (runs in serial mode to avoid multiprocessing conflicts)</span>
<span class="sd">            </span>
<span class="sd">            UltraNest automatically determines the number of live points and</span>
<span class="sd">            adapts its sampling strategy based on the problem characteristics.</span>
<span class="sd">            It provides excellent performance across a wide range of problems</span>
<span class="sd">            without requiring parameter tuning.</span>
<span class="sd">            </span>
<span class="sd">            **MPI Compatibility:** This function runs UltraNest in serial mode,</span>
<span class="sd">            making it fully compatible with MPI environments. While UltraNest</span>
<span class="sd">            itself can use MPI for parallelization, this implementation avoids</span>
<span class="sd">            multiprocessing pools that can conflict with MPI.</span>
<span class="sd">            </span>
<span class="sd">        :example:</span>
<span class="sd">            Sample surrogate model with default settings:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_ultranest()</span>
<span class="sd">            </span>
<span class="sd">            Sample true likelihood with custom termination criteria:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_ultranest(like_fn=&quot;true&quot;, </span>
<span class="sd">            ...                  run_kwargs={&#39;dlogz&#39;: 0.1, &#39;min_ess&#39;: 1000})</span>
<span class="sd">            </span>
<span class="sd">            Use custom prior transform with bounds [-5, 5] for each parameter:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; def my_prior_transform(cube):</span>
<span class="sd">            ...     return 10 * cube - 5  # maps [0,1] to [-5,5]</span>
<span class="sd">            &gt;&gt;&gt; sm.run_ultranest(prior_transform=my_prior_transform)</span>
<span class="sd">            </span>
<span class="sd">            Run with increased live points for better accuracy:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_ultranest(like_fn=&quot;true&quot;,</span>
<span class="sd">            ...                  run_kwargs={&#39;min_num_live_points&#39;: 800})</span>
<span class="sd">            </span>
<span class="sd">            Customize output directory and resume behavior:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.run_ultranest(log_dir=&quot;ultranest_output&quot;, </span>
<span class="sd">            ...                  resume=&quot;overwrite&quot;)</span>
<span class="sd">            </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        UltraNest documentation: https://johannesbuchner.github.io/UltraNest/</span>
<span class="sd">        Buchner (2021): &quot;UltraNest - a robust, general purpose Bayesian inference</span>
<span class="sd">        library for cosmology and particle physics&quot;, Journal of Open Source Software</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">ultranest</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">ultranest</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReactiveNestedSampler</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;UltraNest is required but not installed. &quot;</span>
                            <span class="s2">&quot;Install with: pip install ultranest&quot;</span><span class="p">)</span>
        
        <span class="c1"># Start timing</span>
        <span class="n">ultranest_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="c1"># Determine likelihood function and name</span>
        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">like_fn</span><span class="p">):</span>
            <span class="c1"># like_fn is a function/callable</span>
            <span class="k">if</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Custom function provided</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with user-provided likelihood function.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;custom&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">like_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;surrogate&quot;</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing UltraNest with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid like_fn string: </span><span class="si">{</span><span class="n">like_fn</span><span class="si">}</span><span class="s2">. &quot;</span>
                               <span class="s2">&quot;Must be &#39;surrogate&#39;, &#39;gp&#39;, &#39;true&#39;, or callable.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;like_fn must be callable, string, or None. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">like_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Set up prior transformation function</span>
        <span class="k">if</span> <span class="n">prior_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default uniform prior using self.bounds</span>
            <span class="n">prior_transform_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_transform_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Uniform prior with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prior_transform_fn</span> <span class="o">=</span> <span class="n">prior_transform</span>
            <span class="k">if</span> <span class="n">prior_transform_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prior_transform</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Custom prior transform: </span><span class="si">{</span><span class="n">prior_transform</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="s2">&quot;Custom prior transform&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="n">prior_transform_comment</span>
        
        <span class="c1"># Set up log directory</span>
        <span class="k">if</span> <span class="n">log_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">log_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/ultranest_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Set default sampler kwargs (these go to ReactiveNestedSampler constructor)</span>
        <span class="n">default_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;derived_param_names&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;wrapped_params&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;num_test_samples&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s1">&#39;draw_multiple&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;num_bootstraps&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
            <span class="s1">&#39;vectorized&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s1">&#39;ndraw_min&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
            <span class="s1">&#39;ndraw_max&#39;</span><span class="p">:</span> <span class="mi">65536</span><span class="p">,</span>
            <span class="s1">&#39;storage_backend&#39;</span><span class="p">:</span> <span class="s1">&#39;hdf5&#39;</span><span class="p">,</span>
            <span class="s1">&#39;warmstart_max_tau&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">}</span>
        
        <span class="c1"># Update with user-provided kwargs</span>
        <span class="n">final_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">default_sampler_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">}</span>
        
        <span class="c1"># Set default run kwargs (these go to sampler.run() method)</span>
        <span class="n">default_run_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;update_interval_volume_fraction&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
            <span class="s1">&#39;show_status&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;viz_callback&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Disable visualization to avoid ipywidgets dependency</span>
            <span class="s1">&#39;dlogz&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
            <span class="s1">&#39;dKL&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
            <span class="s1">&#39;frac_remain&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
            <span class="s1">&#39;Lepsilon&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
            <span class="s1">&#39;min_ess&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
            <span class="s1">&#39;max_num_improvement_loops&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;min_num_live_points&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
            <span class="s1">&#39;cluster_num_live_points&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
            <span class="s1">&#39;insertion_test_zscore_threshold&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;insertion_test_window&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
            <span class="s1">&#39;widen_before_initial_plateau_num_warn&#39;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="p">}</span>
        
        <span class="c1"># Update with user-provided kwargs</span>
        <span class="n">final_run_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">default_run_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">}</span>
        
        <span class="c1"># Check if MPI is active for informational purposes</span>
        <span class="k">if</span> <span class="n">parallel_utils</span><span class="o">.</span><span class="n">is_mpi_active</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MPI environment detected. UltraNest will run in MPI-compatible mode.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">multi_proc</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: multi_proc=True ignored. UltraNest now runs in MPI-compatible serial mode.&quot;</span><span class="p">)</span>
        
        <span class="c1"># Define wrapped likelihood function for UltraNest</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">ultranest_likelihood</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Likelihood function wrapper for UltraNest.&quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running UltraNest with </span><span class="si">{</span><span class="n">final_run_kwargs</span><span class="p">[</span><span class="s1">&#39;min_num_live_points&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> minimum live points...&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log directory: </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Execution mode: MPI-compatible (no multiprocessing pools)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Termination: dlogz=</span><span class="si">{</span><span class="n">final_run_kwargs</span><span class="p">[</span><span class="s1">&#39;dlogz&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, min_ess=</span><span class="si">{</span><span class="n">final_run_kwargs</span><span class="p">[</span><span class="s1">&#39;min_ess&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Multi-run setup for minimum effective sample size</span>
        <span class="n">all_samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_logz</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_logz_err</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_run_times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accumulated_samples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">param_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;param_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        
        <span class="k">while</span> <span class="n">accumulated_samples</span> <span class="o">&lt;</span> <span class="n">min_ess</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">: Need </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">min_ess</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">accumulated_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> more samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
            
            <span class="c1"># Set timing for this run</span>
            <span class="n">run_start_time</span> <span class="o">=</span> <span class="n">ultranest_t0</span> <span class="k">if</span> <span class="n">run_number</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            
            <span class="c1"># Create a new sampler for each run (with updated log_dir if needed)</span>
            <span class="n">current_log_dir</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="k">if</span> <span class="n">run_number</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">log_dir</span> <span class="k">else</span> <span class="kc">None</span>
            
            <span class="c1"># Create UltraNest sampler instance</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_sampler</span> <span class="o">=</span> <span class="n">ReactiveNestedSampler</span><span class="p">(</span>
                <span class="n">param_names</span><span class="o">=</span><span class="n">param_names</span><span class="p">,</span>
                <span class="n">loglike</span><span class="o">=</span><span class="n">ultranest_likelihood</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">prior_transform_fn</span><span class="p">,</span>
                <span class="n">log_dir</span><span class="o">=</span><span class="n">current_log_dir</span><span class="p">,</span>
                <span class="n">resume</span><span class="o">=</span><span class="n">resume</span> <span class="k">if</span> <span class="n">run_number</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;overwrite&quot;</span><span class="p">,</span>  <span class="c1"># Only resume on first run</span>
                <span class="o">**</span><span class="n">final_sampler_kwargs</span>
            <span class="p">)</span>
            
            <span class="k">if</span> <span class="n">slice_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">ultranest</span><span class="w"> </span><span class="kn">import</span> <span class="n">stepsampler</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_sampler</span><span class="o">.</span><span class="n">stepsampler</span> <span class="o">=</span> <span class="n">stepsampler</span><span class="o">.</span><span class="n">SliceSampler</span><span class="p">(</span>
                    <span class="n">nsteps</span><span class="o">=</span><span class="n">slice_steps</span><span class="p">,</span>
                    <span class="n">generate_direction</span><span class="o">=</span><span class="n">stepsampler</span><span class="o">.</span><span class="n">generate_mixture_random_direction</span><span class="p">,</span>
                <span class="p">)</span>
            
            <span class="c1"># Run UltraNest sampling (always in serial/MPI-compatible mode)</span>
            <span class="n">current_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_sampler</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="o">**</span><span class="n">final_run_kwargs</span>
            <span class="p">)</span>
            
            <span class="c1"># Record current run statistics</span>
            <span class="n">current_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">run_start_time</span>
            <span class="n">all_run_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_runtime</span><span class="p">)</span>
            
            <span class="c1"># Extract results for this run</span>
            <span class="n">current_samples</span> <span class="o">=</span> <span class="n">current_results</span><span class="p">[</span><span class="s1">&#39;samples&#39;</span><span class="p">]</span>
            
            <span class="c1"># Extract weights from weighted_samples if available, otherwise use equal weights</span>
            <span class="k">if</span> <span class="s1">&#39;weighted_samples&#39;</span> <span class="ow">in</span> <span class="n">current_results</span><span class="p">:</span>
                <span class="n">current_weights</span> <span class="o">=</span> <span class="n">current_results</span><span class="p">[</span><span class="s1">&#39;weighted_samples&#39;</span><span class="p">][</span><span class="s1">&#39;weights&#39;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># For equally weighted samples, create uniform weights</span>
                <span class="n">current_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">current_samples</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_samples</span><span class="p">)</span>
            
            <span class="n">current_logz</span> <span class="o">=</span> <span class="n">current_results</span><span class="p">[</span><span class="s1">&#39;logz&#39;</span><span class="p">]</span>
            <span class="n">current_logz_err</span> <span class="o">=</span> <span class="n">current_results</span><span class="p">[</span><span class="s1">&#39;logzerr&#39;</span><span class="p">]</span>
            
            <span class="c1"># Store results from this run</span>
            <span class="n">all_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_samples</span><span class="p">)</span>
            <span class="n">all_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>
            <span class="n">all_logz</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_logz</span><span class="p">)</span>
            <span class="n">all_logz_err</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_logz_err</span><span class="p">)</span>
            
            <span class="n">current_nsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_samples</span><span class="p">)</span>
            <span class="n">accumulated_samples</span> <span class="o">+=</span> <span class="n">current_nsamples</span>
            
            <span class="k">if</span> <span class="n">min_ess</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run </span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s2"> complete: </span><span class="si">{</span><span class="n">current_nsamples</span><span class="si">}</span><span class="s2"> samples, logZ = </span><span class="si">{</span><span class="n">current_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">current_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total accumulated samples: </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># If min_ess requirement met, break</span>
            <span class="k">if</span> <span class="n">accumulated_samples</span> <span class="o">&gt;=</span> <span class="n">min_ess</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">run_number</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Limit to prevent infinite loops</span>
            <span class="k">if</span> <span class="n">run_number</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: Reached maximum of 10 runs, stopping with </span><span class="si">{</span><span class="n">accumulated_samples</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="c1"># Combine results from all runs</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">all_weights</span><span class="p">)</span>
            <span class="c1"># Use the best (highest) log evidence</span>
            <span class="n">best_run_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">all_logz</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz</span> <span class="o">=</span> <span class="n">all_logz</span><span class="p">[</span><span class="n">best_run_idx</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz_err</span> <span class="o">=</span> <span class="n">all_logz_err</span><span class="p">[</span><span class="n">best_run_idx</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Combined </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> runs into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span><span class="p">)</span><span class="si">}</span><span class="s2"> total samples&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best log evidence: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span> <span class="o">=</span> <span class="n">all_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_weights</span> <span class="o">=</span> <span class="n">all_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz</span> <span class="o">=</span> <span class="n">all_logz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz_err</span> <span class="o">=</span> <span class="n">all_logz_err</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Use final run for primary results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_results</span> <span class="o">=</span> <span class="n">current_results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_runtime</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">all_run_times</span><span class="p">)</span>
        
        <span class="c1"># Store samples based on likelihood function used</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;UltraNest complete. </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples_true</span><span class="p">)</span><span class="si">}</span><span class="s2"> posterior samples collected.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples_surrogate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;UltraNest complete. </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples_surrogate</span><span class="p">)</span><span class="si">}</span><span class="s2"> posterior samples collected.&quot;</span><span class="p">)</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log Evidence: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz_err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Effective Sample Size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ess&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;N/A&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of likelihood evaluations: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ncall&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;N/A&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Record that UltraNest has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ultranest_run</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;UltraNest runtime: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_runtime</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Save results if caching is enabled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
            
        <span class="c1"># Save samples to file</span>
        <span class="k">if</span> <span class="n">samples_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">samples_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/ultranest_samples_final_true.npz&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/ultranest_samples_final_surrogate_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved UltraNest samples to </span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> 
                <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_samples</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_weights</span><span class="p">,</span>
                <span class="n">logz</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz</span><span class="p">,</span>
                <span class="n">logz_err</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ultranest_logz_err</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.plot">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.plot">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plots</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cb_rng</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate diagnostic plots for training progress, GP performance, and MCMC results.</span>
<span class="sd">        </span>
<span class="sd">        This method creates various diagnostic plots to assess the quality of the surrogate</span>
<span class="sd">        model training, GP hyperparameter optimization, and MCMC sampling results. Plots</span>
<span class="sd">        are automatically saved to the model&#39;s save directory.</span>
<span class="sd">        </span>
<span class="sd">        :param plots: (*list of str, optional*)</span>
<span class="sd">            List of plot types to generate. Each plot requires specific data to be available</span>
<span class="sd">            (e.g., &#39;emcee_corner&#39; requires run_emcee() to have been called first). If None,</span>
<span class="sd">            no plots are generated. Available options:</span>
<span class="sd">            </span>
<span class="sd">            **Training diagnostics:**</span>
<span class="sd">            - &#39;test_mse&#39;: Mean squared error vs training iteration</span>
<span class="sd">            - &#39;test_scaled_mse&#39;: Scaled MSE vs training iteration  </span>
<span class="sd">            - &#39;test_log_mse&#39;: Log-scale MSE vs training iteration</span>
<span class="sd">            - &#39;gp_hyperparameters&#39;: GP hyperparameter evolution during training</span>
<span class="sd">            - &#39;gp_train_time&#39;: GP training time vs iteration</span>
<span class="sd">            - &#39;gp_train_corner&#39;: Corner plot of final training samples</span>
<span class="sd">            - &#39;gp_train_scatter&#39;: Scatter plot of training samples vs predictions</span>
<span class="sd">            </span>
<span class="sd">            **GP visualization (2D only):**</span>
<span class="sd">            - &#39;gp_fit_2D&#39;: 2D contour plot of GP surrogate surface</span>
<span class="sd">            </span>
<span class="sd">            **MCMC diagnostics:**</span>
<span class="sd">            - &#39;emcee_corner&#39;: Corner plot of emcee posterior samples</span>
<span class="sd">            - &#39;emcee_walkers&#39;: Walker trajectories for emcee chains</span>
<span class="sd">            - &#39;dynesty_corner&#39;: Corner plot of dynesty posterior samples  </span>
<span class="sd">            - &#39;dynesty_corner_kde&#39;: KDE version of dynesty corner plot</span>
<span class="sd">            - &#39;dynesty_traceplot&#39;: Trace plot of dynesty sampling</span>
<span class="sd">            - &#39;dynesty_runplot&#39;: Dynesty convergence diagnostics</span>
<span class="sd">            </span>
<span class="sd">            **Comparison plots:**</span>
<span class="sd">            - &#39;mcmc_comparison&#39;: Compare emcee and dynesty posteriors</span>
<span class="sd">            </span>
<span class="sd">            **Convenience options:**</span>
<span class="sd">            - &#39;gp_all&#39;: Generate all available GP training plots</span>
<span class="sd">            </span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param show: (*bool, optional*)</span>
<span class="sd">            Whether to display plots interactively in addition to saving them.</span>
<span class="sd">            If False, plots are only saved to disk. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param cb_rng: (*list of [float, float], optional*)</span>
<span class="sd">            Colorbar range for 2D contour plots as [vmin, vmax]. If [None, None],</span>
<span class="sd">            uses automatic range determination. Only applies to plots with colorbars</span>
<span class="sd">            like &#39;gp_fit_2D&#39;. Default is [None, None].</span>
<span class="sd">            </span>
<span class="sd">        :param log_scale: (*bool, optional*)</span>
<span class="sd">            Whether to use logarithmic color scale for 2D contour plots. If True,</span>
<span class="sd">            applies matplotlib.colors.LogNorm to the colorbar. Only applies to</span>
<span class="sd">            plots with colorbars. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :returns: *None or matplotlib.figure.Figure*</span>
<span class="sd">            Some individual plots may return figure objects for further customization.</span>
<span class="sd">            </span>
<span class="sd">        :raises NameError:</span>
<span class="sd">            If required data for a requested plot is not available (e.g., requesting</span>
<span class="sd">            &#39;emcee_corner&#39; before running run_emcee()).</span>
<span class="sd">        :raises AttributeError:</span>
<span class="sd">            If the model has not been properly initialized or trained.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Plots are automatically saved to the model&#39;s save directory (self.savedir)</span>
<span class="sd">        with descriptive filenames. The save directory is created if it doesn&#39;t exist.</span>
<span class="sd">        </span>
<span class="sd">        Training diagnostic plots help assess:</span>
<span class="sd">        - Convergence of active learning process</span>
<span class="sd">        - Quality of GP hyperparameter optimization  </span>
<span class="sd">        - Efficiency of training sample selection</span>
<span class="sd">        </span>
<span class="sd">        MCMC diagnostic plots help assess:</span>
<span class="sd">        - Posterior sampling convergence</span>
<span class="sd">        - Chain mixing and autocorrelation</span>
<span class="sd">        - Comparison between different sampling methods</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Generate all GP training plots:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;gp_all&#39;])</span>
<span class="sd">        </span>
<span class="sd">        Create MCMC comparison plots:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee()</span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty()</span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;emcee_corner&#39;, &#39;dynesty_corner&#39;, &#39;mcmc_comparison&#39;])</span>
<span class="sd">        </span>
<span class="sd">        Generate 2D GP visualization with custom colorbar:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;gp_fit_2D&#39;], cb_rng=[-10, 0], log_scale=True)</span>
<span class="sd">        </span>
<span class="sd">        Show plots interactively:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;test_mse&#39;, &#39;gp_hyperparameters&#39;], show=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># ================================</span>
        <span class="c1"># GP training plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;gp_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">gp_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;test_log_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_hyperparam&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_timing&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_train_scatter&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">gp_plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;gp_fit_2D&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">gp_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>
                
        <span class="c1"># GP mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_mse_vs_iteration.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># MSE </span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Mean Squared Error&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_mse.&quot;</span><span class="p">)</span>
            
        <span class="c1"># GP scaled mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_scaled_mse_vs_iteration.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the scaled gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># Scaled MSE</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Mean Squared Error / Variance&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_scaled_mse.&quot;</span><span class="p">)</span>
            
        <span class="c1"># GP mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_log_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_mse_vs_iteration_log.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the log gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># Log MSE</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Log(Mean Squared Error)&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_log_mse.&quot;</span><span class="p">)</span>

        <span class="c1"># GP hyperparameters vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp hyperparameters...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_hyperparam_vs_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting gp_hyperparameters.&quot;</span><span class="p">)</span>

        <span class="c1"># GP training time vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_time&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp timing...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_train_time_vs_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting gp_timing.&quot;</span><span class="p">)</span>

        <span class="c1"># N-D scatterplots and histograms colored by function value</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting training sample corner plot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_lnp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_train_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># N-D scatterplots and histograms</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_scatter&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting training sample corner plot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_train_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># GP fit (only for 2D functions)</span>
        <span class="k">if</span> <span class="s2">&quot;gp_fit_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp fit 2D...&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_gp_fit_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> 
                                              <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta must be 2D to use gp_fit_2D!&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_fit_2D.&quot;</span><span class="p">)</span>

        <span class="c1"># Objective function contour plot</span>
        <span class="k">if</span> <span class="s2">&quot;obj_fn_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting objective function contours 2D...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_utility_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and init_gp before plotting obj_fn_2D.&quot;</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="s2">&quot;true_fn_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting true function contours 2D...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_true_fit_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta must be 2D to use true_fn_2D!&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># emcee plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;emcee_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">emcee_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;emcee_corner&quot;</span><span class="p">,</span> <span class="s2">&quot;emcee_walkers&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">emcee_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>

        <span class="c1"># emcee posterior samples</span>
        <span class="k">if</span> <span class="s2">&quot;emcee_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee posterior...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;emcee_&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee before plotting emcee_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># emcee walkers</span>
        <span class="k">if</span> <span class="s2">&quot;emcee_walkers&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee walkers...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_emcee_walkers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee before plotting emcee_walkers.&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># dynesty plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">dynesty_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dynesty_corner&quot;</span><span class="p">,</span> <span class="s2">&quot;dynesty_corner_kde&quot;</span><span class="p">,</span> 
                             <span class="s2">&quot;dynesty_traceplot&quot;</span><span class="p">,</span> <span class="s2">&quot;dynesty_runplot&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">dynesty_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>

        <span class="c1"># dynesty posterior samples</span>
        <span class="k">if</span> <span class="s2">&quot;dynesty_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty posterior...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;dynesty_&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_corner.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_corner_kde&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;dynesty_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty posterior kde...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_kde</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_corner.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_traceplot&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty traceplot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_dynesty_traceplot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_traceplot.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_runplot&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty runplot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_dynesty_runplot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_runplot.&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># MCMC comparison plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;mcmc_comparison&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee vs dynesty posterior comparison...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_emcee_dynesty_comparison</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee and run_dynesty before plotting emcee_comparison.&quot;</span><span class="p">)</span></div>

    

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_chain_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_id</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;lbfgsb&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                          <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">allow_opt_multiproc</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Worker function to run a single active learning chain.</span>
<span class="sd">        This function is designed to be pickled for multiprocessing.</span>
<span class="sd">        </span>
<span class="sd">        :param chain_id: (*int*)</span>
<span class="sd">            Identifier for this chain</span>
<span class="sd">        Other parameters same as active_train()</span>
<span class="sd">        </span>
<span class="sd">        :returns: *tuple or None*</span>
<span class="sd">            (new_theta, new_y, training_results) if successful, None if failed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Create a copy of the current model for this chain</span>
            <span class="n">chain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chain_copy</span><span class="p">(</span><span class="n">chain_id</span><span class="o">=</span><span class="n">chain_id</span><span class="p">)</span>
            
            <span class="c1"># Store initial state</span>
            <span class="n">initial_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">initial_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            
            <span class="c1"># Run active learning on this chain (explicitly disable multiprocessing)</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span>
                              <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
                              <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span>
                              <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="n">show_progress</span><span class="p">,</span> 
                              <span class="n">allow_opt_multiproc</span><span class="o">=</span><span class="n">allow_opt_multiproc</span><span class="p">)</span>
            
            <span class="c1"># Extract only the new samples (excluding initial training data)</span>
            <span class="n">initial_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">)</span>
            <span class="n">new_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            <span class="n">new_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            
            <span class="k">return</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
            
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_chain_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_id</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a copy of the current SurrogateModel for a parallel chain.&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
        
        <span class="c1"># Create a deep copy of the current model</span>
        <span class="n">chain</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        
        <span class="c1"># Modify savedir to avoid conflicts</span>
        <span class="n">chain</span><span class="o">.</span><span class="n">savedir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/chain_</span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">savedir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">savedir</span><span class="p">)</span>
        
        <span class="c1"># Reset training results for this chain</span>
        <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                 <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[]}</span>
        
        <span class="c1"># Ensure chain copies use single-threaded operations</span>
        <span class="n">chain</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Force single-core for all operations in chain copies</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="s1">&#39;opt_gp_kwargs&#39;</span><span class="p">):</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">opt_gp_kwargs</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="p">[</span><span class="s2">&quot;multi_proc&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            
        <span class="c1"># Try to control threading at the library level</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Set NumPy to single-threaded if possible</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">np</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">):</span>
                <span class="c1"># Some NumPy operations respect thread limits better after reseeding</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">chain_id</span> <span class="o">+</span> <span class="mi">42</span><span class="p">)</span>  <span class="c1"># Different seed per chain</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
            
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Control scikit-learn backend</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">sklearn</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sklearn</span><span class="p">,</span> <span class="s1">&#39;get_config&#39;</span><span class="p">):</span>
                <span class="n">current_config</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
                <span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">assume_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">working_memory</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
        
        <span class="k">return</span> <span class="n">chain</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_combine_chain_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">all_new_theta</span><span class="p">,</span> <span class="n">all_new_y</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Combine results from all chains into the main model.&quot;&quot;&quot;</span>
        
        <span class="c1"># Concatenate all new training samples</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_new_theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span><span class="p">):</span>
            <span class="n">combined_new_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">theta</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
            <span class="n">combined_new_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">all_new_y</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
            
            <span class="c1"># CRITICAL: Remove near-duplicate points to prevent numerical instability</span>
            <span class="c1"># Check for points that are very close to each other (within 1e-6)</span>
            <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-6</span>
            <span class="n">keep_indices</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">point</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">combined_new_theta</span><span class="p">):</span>
                <span class="n">is_duplicate</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="c1"># Check against previously kept points</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">keep_indices</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">combined_new_theta</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">atol</span><span class="o">=</span><span class="n">tolerance</span><span class="p">):</span>
                        <span class="n">is_duplicate</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="k">break</span>
                <span class="c1"># Also check against existing training data</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_duplicate</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">existing_point</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">existing_point</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">tolerance</span><span class="p">):</span>
                            <span class="n">is_duplicate</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">break</span>
                
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_duplicate</span><span class="p">:</span>
                    <span class="n">keep_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">combined_new_theta</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Removed </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">combined_new_theta</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">keep_indices</span><span class="p">)</span><span class="si">}</span><span class="s2"> near-duplicate points from parallel chains&quot;</span><span class="p">)</span>
                <span class="n">combined_new_theta</span> <span class="o">=</span> <span class="n">combined_new_theta</span><span class="p">[</span><span class="n">keep_indices</span><span class="p">]</span>
                <span class="n">combined_new_y</span> <span class="o">=</span> <span class="n">combined_new_y</span><span class="p">[</span><span class="n">keep_indices</span><span class="p">]</span>
            
            <span class="c1"># Add to main model (only if we have points left)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">combined_new_theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">combined_new_theta</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">combined_new_y</span><span class="p">])</span>
            
                <span class="c1"># Update counters</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>
                
                <span class="c1"># Refit GP with all combined data while preserving hyperparameters  </span>
                <span class="c1"># Store current hyperparameters before refitting</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="c1"># Refit GP with combined data using current hyperparameters</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">())</span>
                        
                        <span class="c1"># Check for numerical stability by computing a test prediction</span>
                        <span class="k">try</span><span class="p">:</span>
                            <span class="n">test_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">test_pred</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">test_pred</span><span class="p">)):</span>
                                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;GP predictions contain NaN or Inf&quot;</span><span class="p">)</span>
                        <span class="k">except</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: GP numerically unstable after hyperparameter restoration, re-optimizing...&quot;</span><span class="p">)</span>
                            <span class="c1"># If unstable, re-optimize hyperparameters</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_gp</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">opt_gp_kwargs</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Hyperparameter preservation failed (</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">), falling back to standard fitting&quot;</span><span class="p">)</span>
                        <span class="c1"># If hyperparameter preservation fails, fall back to standard fitting  </span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># No existing GP, use standard fitting</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: No new points added after duplicate removal&quot;</span><span class="p">)</span>
        
        <span class="c1"># Combine training results from all chains</span>
        <span class="k">if</span> <span class="n">chain_results</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_merge_training_results</span><span class="p">(</span><span class="n">chain_results</span><span class="p">)</span>
        
        <span class="c1"># Ensure training results have at least one entry to avoid index errors</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;test_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_merge_training_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Merge training results from multiple chains.&quot;&quot;&quot;</span>
        
        <span class="c1"># Get the starting iteration number</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">start_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_iter</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="c1"># Merge results from all chains</span>
        <span class="k">for</span> <span class="n">chain_result</span> <span class="ow">in</span> <span class="n">chain_results</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">chain_result</span><span class="p">:</span>  <span class="c1"># Skip empty results</span>
                <span class="k">continue</span>
                
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">chain_result</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;iteration&quot;</span><span class="p">:</span>
                        <span class="c1"># Adjust iteration numbers to be sequential</span>
                        <span class="n">adjusted_iters</span> <span class="o">=</span> <span class="p">[</span><span class="n">iter_num</span> <span class="o">+</span> <span class="n">start_iter</span> <span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="n">chain_result</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">adjusted_iters</span><span class="p">)</span>
                        <span class="n">start_iter</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">adjusted_iters</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">adjusted_iters</span> <span class="k">else</span> <span class="n">start_iter</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chain_result</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>


<div class="viewcode-block" id="SurrogateModel.get_chain_diversity_metrics">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.get_chain_diversity_metrics">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_chain_diversity_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate diversity metrics for the combined training samples.</span>
<span class="sd">        Useful for assessing the effectiveness of parallel chains.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_theta&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>
        
        <span class="c1"># Get only the active learning samples (exclude initial training)</span>
        <span class="n">active_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:]</span>
        <span class="n">active_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:]</span>
        
        <span class="c1"># Calculate diversity metrics</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="c1"># Parameter space coverage</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
            <span class="n">param_range</span> <span class="o">=</span> <span class="n">active_theta</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">active_theta</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
            <span class="n">bound_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;param_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">_coverage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_range</span> <span class="o">/</span> <span class="n">bound_range</span>
        
        <span class="c1"># Function value diversity</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;function_value_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">active_y</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;function_value_range&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">active_y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">active_y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        
        <span class="c1"># Average pairwise distance in parameter space</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">scipy.spatial.distance</span><span class="w"> </span><span class="kn">import</span> <span class="n">pdist</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">active_theta</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;avg_pairwise_distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;min_pairwise_distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">metrics</span></div>



    <span class="k">def</span><span class="w"> </span><span class="nf">_fix_pymultinest_output_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputfiles_basename</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fix malformed scientific notation in PyMultiNest output files.</span>
<span class="sd">        </span>
<span class="sd">        PyMultiNest sometimes writes very small numbers in malformed scientific </span>
<span class="sd">        notation (e.g., &#39;1.23-100&#39; instead of &#39;1.23E-100&#39;), which causes numpy </span>
<span class="sd">        to fail when loading the files. This method fixes such formatting issues.</span>
<span class="sd">        </span>
<span class="sd">        :param outputfiles_basename: (*str*)</span>
<span class="sd">            Base name of PyMultiNest output files to fix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
        
        <span class="c1"># Common PyMultiNest output file extensions that may contain malformed numbers</span>
        <span class="n">suffixes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">&#39;post_equal_weights.dat&#39;</span><span class="p">,</span>    <span class="c1"># Equal weighted posterior samples</span>
            <span class="s1">&#39;phys_live.points&#39;</span><span class="p">,</span>          <span class="c1"># Live points</span>
            <span class="s1">&#39;post_separate.dat&#39;</span><span class="p">,</span>         <span class="c1"># Separate mode samples</span>
            <span class="s1">&#39;stats.dat&#39;</span><span class="p">,</span>                 <span class="c1"># Statistics file</span>
            <span class="s1">&#39;live.points&#39;</span><span class="p">,</span>               <span class="c1"># Live points file</span>
            <span class="s1">&#39;ev.dat&#39;</span><span class="p">,</span>                    <span class="c1"># Evidence file</span>
            <span class="s1">&#39;summary.txt&#39;</span><span class="p">,</span>               <span class="c1"># Summary file</span>
            <span class="s1">&#39;.txt&#39;</span><span class="p">,</span>                      <span class="c1"># Generic text output</span>
            <span class="s1">&#39;points.dat&#39;</span><span class="p">,</span>                <span class="c1"># Points file</span>
            <span class="s1">&#39;posterior_samples.dat&#39;</span><span class="p">,</span>     <span class="c1"># Alternative posterior samples name</span>
        <span class="p">]</span>
        
        <span class="k">for</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="n">suffixes</span><span class="p">:</span>
            <span class="n">filepath</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outputfiles_basename</span><span class="si">}{</span><span class="n">suffix</span><span class="si">}</span><span class="s2">&quot;</span>
            
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Read the file</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                        <span class="n">content</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                    
                    <span class="c1"># Fix malformed scientific notation using regex</span>
                    <span class="c1"># Pattern matches various forms of malformed scientific notation:</span>
                    <span class="c1"># 1. &quot;1.234567890123456789-123&quot; -&gt; &quot;1.234567890123456789E-123&quot;</span>
                    <span class="c1"># 2. &quot;-0.139060048608094152-308&quot; -&gt; &quot;-0.139060048608094152E-308&quot;</span>
                    <span class="c1"># 3. &quot;1.23+45&quot; -&gt; &quot;1.23E+45&quot; (if exponent is large)</span>
                    
                    <span class="c1"># More comprehensive pattern for malformed scientific notation</span>
                    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;(-?\d+\.?\d*)([+-]\d+)(?=\s|$|,|\t)&#39;</span>
                    
                    <span class="c1"># Function to fix scientific notation</span>
                    <span class="k">def</span><span class="w"> </span><span class="nf">fix_scientific</span><span class="p">(</span><span class="n">match</span><span class="p">):</span>
                        <span class="n">number</span> <span class="o">=</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">exponent</span> <span class="o">=</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                        <span class="c1"># Only fix if this looks like malformed scientific notation</span>
                        <span class="c1"># (exponent magnitude suggests it&#39;s not just arithmetic)</span>
                        <span class="n">exp_value</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">exponent</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">exp_value</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Likely scientific notation</span>
                            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">number</span><span class="si">}</span><span class="s2">E</span><span class="si">{</span><span class="n">exponent</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">return</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Leave unchanged</span>
                    
                    <span class="c1"># Apply the fix</span>
                    <span class="n">fixed_content</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">fix_scientific</span><span class="p">,</span> <span class="n">content</span><span class="p">)</span>
                    
                    <span class="c1"># Write back only if changes were made</span>
                    <span class="k">if</span> <span class="n">fixed_content</span> <span class="o">!=</span> <span class="n">content</span><span class="p">:</span>
                        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">fixed_content</span><span class="p">)</span>
                            
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Could not fix formatting in </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_pickleable_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get a pickleable representation of the model state for multiprocessing.</span>
<span class="sd">        </span>
<span class="sd">        :returns: *dict*</span>
<span class="sd">            Dictionary containing all necessary state information including GP hyperparameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
        
        <span class="c1"># Create a simplified state dictionary</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="s1">&#39;bounds&#39;</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">),</span>
            <span class="s1">&#39;ndim&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
            <span class="s1">&#39;ninit_train&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">,</span>
            <span class="s1">&#39;ntrain&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span><span class="p">,</span>
            <span class="s1">&#39;function&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">,</span>  <span class="c1"># Use the correct function attribute</span>
            <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="s1">&#39;ncore&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
        <span class="p">}</span>
        
        <span class="c1"># Include GP hyperparameters if GP exists and is trained</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
                <span class="c1"># Also include GP initialization settings for consistency</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_kernel&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_gp_kernel&#39;</span><span class="p">,</span> <span class="s1">&#39;ExpSquaredKernel&#39;</span><span class="p">)</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_fit_amp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_gp_fit_amp&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_fit_mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_gp_fit_mean&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_fit_white_noise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_gp_fit_white_noise&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_white_noise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_gp_white_noise&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">12</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="c1"># If we can&#39;t get hyperparameters, workers will initialize fresh GP</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">return</span> <span class="n">state</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_run_chain_worker_mp</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiprocessing worker function to run a single active learning chain.</span>
<span class="sd">    </span>
<span class="sd">    This function is designed to work with multiprocessing.Pool and must be</span>
<span class="sd">    defined at module level to be pickleable.</span>
<span class="sd">    </span>
<span class="sd">    :param args: (*tuple*)</span>
<span class="sd">        Tuple containing (chain_state, niter, algorithm, gp_opt_freq, </span>
<span class="sd">        obj_opt_method, nopt, use_grad_opt, optimizer_kwargs)</span>
<span class="sd">        </span>
<span class="sd">    :returns: *tuple or None*</span>
<span class="sd">        (new_theta, new_y, training_results) if successful, None if failed</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Unpack arguments</span>
        <span class="p">(</span><span class="n">chain_state</span><span class="p">,</span> <span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="p">,</span>
         <span class="n">nopt</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="p">)</span> <span class="o">=</span> <span class="n">args</span>
        
        <span class="n">chain_id</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;chain_id&#39;</span><span class="p">]</span>
        
        <span class="c1"># Reconstruct the model from the pickled state</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">SurrogateModel</span>
        
        <span class="c1"># Create a new model instance with the saved state</span>
        <span class="n">surrogate</span> <span class="o">=</span> <span class="n">SurrogateModel</span><span class="p">(</span>
            <span class="n">lnlike_fn</span><span class="o">=</span><span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;function&#39;</span><span class="p">],</span>
            <span class="n">bounds</span><span class="o">=</span><span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;bounds&#39;</span><span class="p">],</span>
            <span class="n">ncore</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Force single-core for this process</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># Disable verbose to avoid cluttering output</span>
        <span class="p">)</span>
        
        <span class="c1"># Restore the training data</span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> 
        <span class="n">surrogate</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;ndim&#39;</span><span class="p">]</span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;ninit_train&#39;</span><span class="p">]</span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;ntrain&#39;</span><span class="p">]</span>
        
        <span class="c1"># Set up the save directory for this chain</span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">savedir</span> <span class="o">=</span> <span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;savedir&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">surrogate</span><span class="o">.</span><span class="n">savedir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">surrogate</span><span class="o">.</span><span class="n">savedir</span><span class="p">)</span>
        
        <span class="c1"># CRITICAL: Set unique random seed for this chain to prevent identical point generation</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
        <span class="c1"># Use chain_id and current time to ensure unique randomization per chain</span>
        <span class="n">chain_random_seed</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">*</span> <span class="mi">1000000</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">))</span> <span class="o">+</span> <span class="n">chain_id</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">chain_random_seed</span><span class="p">)</span>
        
        <span class="c1"># Initialize GP for this chain using preserved hyperparameters</span>
        <span class="n">use_preserved_hyperparams</span> <span class="o">=</span> <span class="p">(</span><span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">use_preserved_hyperparams</span><span class="p">:</span>
            <span class="c1"># Initialize GP with same settings as parent process</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">init_gp</span><span class="p">(</span>
                <span class="n">kernel</span><span class="o">=</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gp_kernel&#39;</span><span class="p">,</span> <span class="s1">&#39;ExpSquaredKernel&#39;</span><span class="p">),</span>
                <span class="n">fit_amp</span><span class="o">=</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gp_fit_amp&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="n">fit_mean</span><span class="o">=</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gp_fit_mean&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="n">fit_white_noise</span><span class="o">=</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gp_fit_white_noise&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="n">white_noise</span><span class="o">=</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gp_white_noise&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">12</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Set the optimized hyperparameters from parent process</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">chain_state</span><span class="p">[</span><span class="s1">&#39;gp_hyperparameters&#39;</span><span class="p">])</span>
            <span class="c1"># CRITICAL: Compute the GP with the restored hyperparameters</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">surrogate</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
            
            <span class="c1"># Add small random perturbation to hyperparameters to prevent identical chains</span>
            <span class="c1"># This helps avoid numerical issues while preserving the optimization quality</span>
            <span class="n">current_hyperparams</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
            <span class="c1"># Add 1% random noise to hyperparameters</span>
            <span class="n">perturbation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">current_hyperparams</span><span class="p">))</span>
            <span class="n">perturbed_hyperparams</span> <span class="o">=</span> <span class="n">current_hyperparams</span> <span class="o">+</span> <span class="n">perturbation</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">perturbed_hyperparams</span><span class="p">)</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">surrogate</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
            
            <span class="c1"># IMPROVED: Allow limited GP re-optimization for accuracy</span>
            <span class="c1"># Increase gp_opt_freq to reduce re-optimization but don&#39;t disable it completely</span>
            <span class="c1"># This balances preserved hyperparameters with GP accuracy as data grows</span>
            <span class="n">effective_gp_opt_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gp_opt_freq</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># At least 3x original freq, min 50</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fallback to fresh initialization if hyperparameters not available</span>
            <span class="n">surrogate</span><span class="o">.</span><span class="n">init_gp</span><span class="p">()</span>
            <span class="n">effective_gp_opt_freq</span> <span class="o">=</span> <span class="n">gp_opt_freq</span>
        
        <span class="c1"># Store initial state for comparison</span>
        <span class="n">initial_theta</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">initial_y</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="c1"># Run active learning on this chain (force single-core execution)        </span>
        <span class="n">surrogate</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span>
            <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> 
            <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> 
            <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">effective_gp_opt_freq</span><span class="p">,</span>
            <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Don&#39;t save intermediate progress in parallel chains</span>
            <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
            <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> 
            <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span>
            <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> 
            <span class="n">show_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Don&#39;t show progress bars in parallel</span>
            <span class="n">allow_opt_multiproc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Critical: disable nested multiprocessing</span>
        <span class="p">)</span>
        
        <span class="c1"># Extract only the new samples (excluding initial training data)</span>
        <span class="n">initial_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">)</span>
        <span class="n">new_theta</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
        <span class="n">new_y</span> <span class="o">=</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
        
        <span class="k">return</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">surrogate</span><span class="o">.</span><span class="n">training_results</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiprocessing chain </span><span class="si">{</span><span class="n">chain_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;chain_id&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;?&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Jess Birky
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/notebook-outputs.js?v=9bb603c3"></script>
    </body>
</html>