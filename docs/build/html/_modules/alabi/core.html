<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />
        <link rel="canonical" href="https://alabi.jessicabirky.com/_modules/alabi/core.html" />

    <!-- Generated with Sphinx 7.3.7 and Furo 2025.07.19 -->
        <title>alabi.core - alabi</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0cf789f7" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --font-stack: Roboto Light, sans-serif;
  --font-stack--monospace: Courier, monospace;
  --color-background-secondary: #eff1f6;
  --color-inline-code-background: #eff1f6;
  --color-sidebar-item-background--hover: white;
  --color-brand-primary: #004080;
  --color-brand-content: #0059b3;
  --font-size--small: 0.875rem;
  --font-size--normal: 1rem;
  --font-size--large: 1.125rem;
  --font-size-h1: 2.2rem;
  --font-size-h2: 1.8rem;
  --font-size-h3: 1.5rem;
  --font-size-h4: 1.3rem;
  --font-size-h5: 1.1rem;
  --font-size-h6: 1rem;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">alabi</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  <span class="sidebar-brand-text">alabi</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html#quickstart-example">Quickstart Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../save_reload.html">Saving and Reloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gp_tutorial.html">GP Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mcmc_tutorial.html">MCMC sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_bayesian_optimization.html">Bayesian Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_1d.html">Visualize Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_2d.html">Test 2D Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_line_fit.html">Fit a line to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_kl_divergence.html">KL Divergence: Gaussian</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_gaussian_nd.html">Test computational scaling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../trappist_stellar_evolution.html">Stellar Evolution</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../modules.html">alabi</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of alabi</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../alabi.html">alabi package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/LICENSE">License</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/issues">Issues</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for alabi.core</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">:py:mod:`core.py` </span>
<span class="sd">-------------------------------------</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">utility</span> <span class="k">as</span> <span class="n">ut</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">visualization</span> <span class="k">as</span> <span class="n">vis</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">gp_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">mcmc_utils</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">alabi</span><span class="w"> </span><span class="kn">import</span> <span class="n">cache_utils</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">george</span><span class="w"> </span><span class="kn">import</span> <span class="n">kernels</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>


<span class="c1"># Define scaling functions </span>
<span class="k">def</span><span class="w"> </span><span class="nf">nlog</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nlog_inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="mi">10</span><span class="o">**</span><span class="n">x</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_scale</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_scale_inv</span><span class="p">(</span><span class="n">logx</span><span class="p">):</span> <span class="k">return</span> <span class="mi">10</span><span class="o">**</span><span class="n">logx</span>
<span class="k">def</span><span class="w"> </span><span class="nf">no_scale</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span>

<span class="n">nlog_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">nlog</span><span class="p">,</span> <span class="n">inverse_func</span><span class="o">=</span><span class="n">nlog_inv</span><span class="p">)</span>
<span class="n">log_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">log_scale</span><span class="p">,</span> <span class="n">inverse_func</span><span class="o">=</span><span class="n">log_scale_inv</span><span class="p">)</span>
<span class="n">minmax_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">no_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">no_scale</span><span class="p">,</span> <span class="n">inverse_func</span><span class="o">=</span><span class="n">no_scale</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SurrogateModel&quot;</span><span class="p">,</span>
           <span class="s2">&quot;nlog_scaler&quot;</span><span class="p">,</span> <span class="s2">&quot;log_scaler&quot;</span><span class="p">,</span> <span class="s2">&quot;minmax_scaler&quot;</span><span class="p">,</span> <span class="s2">&quot;no_scaler&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="SurrogateModel">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SurrogateModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Process surrogate model for Bayesian inference and optimization.</span>
<span class="sd">    </span>
<span class="sd">    A SurrogateModel uses a Gaussian Process to create a fast approximation of expensive</span>
<span class="sd">    likelihood functions, enabling efficient Bayesian inference, parameter estimation,</span>
<span class="sd">    and active learning. The model supports various active learning algorithms and </span>
<span class="sd">    scalers for handling different types of likelihood functions.</span>

<span class="sd">    :param lnlike_fn: (*callable, required*)</span>
<span class="sd">        Log-likelihood function that takes parameter array theta and returns scalar </span>
<span class="sd">        log-likelihood value. For Bayesian inference, this is your model&#39;s log-likelihood.</span>
<span class="sd">        Signature: lnlike_fn(theta) -&gt; float</span>
<span class="sd">        </span>
<span class="sd">    :param bounds: (*array-like, required*)</span>
<span class="sd">        Prior bounds for each parameter. List/array of (min, max) tuples for each dimension.</span>
<span class="sd">        Example: bounds = [(0, 1), (2, 3), (-1, 1)]</span>
<span class="sd">        </span>
<span class="sd">    :param param_names: (*array-like, optional*)</span>
<span class="sd">        Names/labels for each parameter. If None, defaults to θ₀, θ₁, etc.</span>
<span class="sd">        Length must match number of dimensions in bounds.</span>
<span class="sd">        </span>
<span class="sd">    :param theta_scaler: (*sklearn transformer, optional, default=MinMaxScaler()*)</span>
<span class="sd">        Scaler for input parameters. Applied to theta values before GP training.</span>
<span class="sd">        Common options: MinMaxScaler() (scale to [0,1]) or StandardScaler()</span>
<span class="sd">        </span>
<span class="sd">    :param y_scaler: (*sklearn transformer, optional, default=no_scaler*)</span>
<span class="sd">        Scaler for output values (log-likelihoods). Options include:</span>
<span class="sd">        - no_scaler: No scaling (default)</span>
<span class="sd">        - minmax_scaler: Scale to [0,1] </span>
<span class="sd">        - nlog_scaler: Apply -log10(-y) transformation for negative log-likelihoods</span>
<span class="sd">        - log_scaler: Apply log10(y) for positive values</span>
<span class="sd">        </span>
<span class="sd">    :param cache: (*bool, optional, default=True*)</span>
<span class="sd">        Whether to cache the trained model to disk for reuse</span>
<span class="sd">        </span>
<span class="sd">    :param savedir: (*str, optional, default=&quot;results/&quot;*)</span>
<span class="sd">        Directory for saving results, plots, and cached models</span>
<span class="sd">        </span>
<span class="sd">    :param model_name: (*str, optional, default=&quot;surrogate_model&quot;*)</span>
<span class="sd">        Name prefix for cached model files</span>
<span class="sd">        </span>
<span class="sd">    :param verbose: (*bool, optional, default=True*)</span>
<span class="sd">        Print progress information during training and inference</span>
<span class="sd">        </span>
<span class="sd">    :param ncore: (*int, optional, default=cpu_count()*)</span>
<span class="sd">        Number of CPU cores to use for parallel computation</span>
<span class="sd">        </span>
<span class="sd">    :param ignore_warnings: (*bool, optional, default=True*)</span>
<span class="sd">        Suppress sklearn and other package warnings</span>
<span class="sd">        </span>
<span class="sd">    :param random_state: (*int, optional, default=None*)</span>
<span class="sd">        Random seed for reproducible results</span>

<span class="sd">    .. attribute:: gp</span>
<span class="sd">        :type: george.GP</span>
<span class="sd">        </span>
<span class="sd">        Trained Gaussian Process model</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: bounds</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Original parameter bounds (unscaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _bounds</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Scaled parameter bounds used for GP training</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _theta</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Training parameter samples (scaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: _y</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Training likelihood values (scaled)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: ntrain</span>
<span class="sd">        :type: int</span>
<span class="sd">        </span>
<span class="sd">        Number of initial training samples</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: ndim</span>
<span class="sd">        :type: int</span>
<span class="sd">        </span>
<span class="sd">        Number of parameters/dimensions</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: emcee_samples</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        MCMC samples from emcee (if run_emcee called)</span>
<span class="sd">        </span>
<span class="sd">    .. attribute:: dynesty_samples</span>
<span class="sd">        :type: ndarray</span>
<span class="sd">        </span>
<span class="sd">        Nested sampling results from dynesty (if run_dynesty called)</span>

<span class="sd">    **Examples**</span>
<span class="sd">    </span>
<span class="sd">    Basic usage for Bayesian inference:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        def log_likelihood(theta):</span>
<span class="sd">            # Your model likelihood function</span>
<span class="sd">            return -0.5 * np.sum((theta - 2)**2)</span>
<span class="sd">         </span>
<span class="sd">        bounds = [(0, 4), (0, 4)]  # 2D parameter space</span>
<span class="sd">        sm = SurrogateModel(log_likelihood, bounds)</span>
<span class="sd">        sm.init_samples(ntrain=100)  # Initial training data</span>
<span class="sd">        sm.init_gp()  # Initialize Gaussian Process</span>
<span class="sd">        sm.active_train(niter=50)  # Active learning</span>
<span class="sd">        sm.run_dynesty()  # Bayesian inference</span>
<span class="sd">    </span>
<span class="sd">    For optimization problems:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        sm.active_train(algorithm=&quot;jones&quot;)  # Use Jones algorithm for optimization</span>
<span class="sd">    </span>
<span class="sd">    .. seealso::</span>
<span class="sd">    </span>
<span class="sd">        :meth:`init_samples` : Initialize training data</span>
<span class="sd">        :meth:`init_gp` : Initialize Gaussian Process</span>
<span class="sd">        :meth:`active_train` : Perform active learning</span>
<span class="sd">        :meth:`run_dynesty` : Run nested sampling</span>
<span class="sd">        :meth:`run_emcee` : Run MCMC sampling</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SurrogateModel.__init__">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lnlike_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">param_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">theta_scaler</span><span class="o">=</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">(),</span> <span class="n">y_scaler</span><span class="o">=</span><span class="n">no_scaler</span><span class="p">,</span>
                 <span class="n">cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">savedir</span><span class="o">=</span><span class="s2">&quot;results/&quot;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;surrogate_model&quot;</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span> <span class="n">ignore_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="c1"># Check all required inputs are specified</span>
        <span class="k">if</span> <span class="n">lnlike_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must supply lnlike_fn to train GP surrogate model.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must supply prior bounds.&quot;</span><span class="p">)</span>

        <span class="c1"># Set random seed for reproducibility</span>
        <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use a time-based seed to avoid clustering when called repeatedly</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">*</span> <span class="mi">1000000</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

        <span class="c1"># Set function for training the GP, and initial training samples</span>
        <span class="c1"># For bayesian inference problem this would be your log likelihood function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span> <span class="o">=</span> <span class="n">lnlike_fn</span>

        <span class="c1"># unscaled bounds for theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bounds</span><span class="p">)</span>
        
        <span class="c1"># Scale inputs between 0 and 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span> <span class="o">=</span> <span class="n">theta_scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Scale bounds to [0, 1] for training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Output scaling function. Fit when initial training samples are created </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span> <span class="o">=</span> <span class="n">y_scaler</span>

        <span class="c1"># define prior sampler with scaled and unscaled bounds </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_sampler</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Determine dimensionality </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span>
        
        <span class="c1"># Set parameter names</span>
        <span class="k">if</span> <span class="n">param_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">bounds</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Length of param_names must match length of bounds.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names</span> <span class="o">=</span> <span class="n">param_names</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">param_names</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$\theta_</span><span class="si">%s</span><span class="s2">$&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;theta_</span><span class="si">{i}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>

        <span class="c1"># Cache surrogate model as pickle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="n">cache</span> 

        <span class="c1"># Directory to save results and plots; defaults to local dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">savedir</span> <span class="o">=</span> <span class="n">savedir</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">)</span>

        <span class="c1"># Name of model cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>

        <span class="c1"># Print progress statements</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        
        <span class="c1"># Ignore warnings</span>
        <span class="k">if</span> <span class="n">ignore_warnings</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>

        <span class="c1"># Number of cores alabi is allowed to use</span>
        <span class="k">if</span> <span class="n">ncore</span> <span class="o">&gt;</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">ncore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">=</span> <span class="n">ncore</span>

        <span class="c1"># false if emcee and dynesty have not been run for this object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                 <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[]}</span></div>


    
<div class="viewcode-block" id="SurrogateModel.save">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.save">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pickle ``SurrogateModel`` object and write summary to a text file</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>

        <span class="c1"># pickle surrogate model object</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Caching model to </span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="o">+</span><span class="s2">&quot;.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>        
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
            <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_emcee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cache_utils</span><span class="o">.</span><span class="n">write_report_dynesty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span></div>

            
    <span class="k">def</span><span class="w"> </span><span class="nf">_lnlike_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_theta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal function to evaluate the model function ``lnlike_fn`` at scaled theta.</span>
<span class="sd">        This is used to avoid scaling the theta in the main function call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Unscale theta</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        
        <span class="c1"># Evaluate function</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Scale y - ensure y is a numpy array</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">_y</span>
            
<div class="viewcode-block" id="SurrogateModel.theta">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.theta">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">theta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return unscaled training theta values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SurrogateModel.y">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.y">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">y</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return unscaled training y values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span></div>


<div class="viewcode-block" id="SurrogateModel.init_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;initial_training_sample.npz&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param nsample: (*int, optional*) </span>
<span class="sd">            Number of samples. Defaults to ``nsample = 50 * self.ndim``</span>

<span class="sd">        :param sampler: (*str, optional*) </span>
<span class="sd">            Sampling method. Defaults to ``&#39;sobol&#39;``. </span>
<span class="sd">            See ``utility.prior_sampler`` for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">nsample</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nsample</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>

        <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">nsample</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
        
        <span class="n">y</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">eval_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span></div>



<div class="viewcode-block" id="SurrogateModel.load_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.load_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache_file</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reload training samples from cache file and apply scalers.</span>
<span class="sd">        </span>
<span class="sd">        :param cache_file: (*str, required*) </span>
<span class="sd">            Name of cache file relative to savedir. Must be a .npz file containing &#39;theta&#39; and &#39;y&#39; arrays.</span>

<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            Scaled training samples (_theta, _y) after loading from cache.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">sims</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">sims</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
        
        <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        
        <span class="c1"># Fit and transform y scale </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dimension of bounds (n=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">) does not </span><span class="se">\</span>
<span class="s2">                              match dimension of training theta (n=</span><span class="si">{</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span></div>



<div class="viewcode-block" id="SurrogateModel.init_samples">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_samples">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntrain</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ntest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reload</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
                     <span class="n">train_file</span><span class="o">=</span><span class="s2">&quot;initial_training_sample.npz&quot;</span><span class="p">,</span> <span class="n">test_file</span><span class="o">=</span><span class="s2">&quot;initial_test_sample.npz&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize training and test samples for the surrogate model.</span>
<span class="sd">        </span>
<span class="sd">        Creates initial dataset by either loading cached samples or computing new ones</span>
<span class="sd">        by evaluating the likelihood function at randomly sampled parameter values.</span>
<span class="sd">        </span>
<span class="sd">        :param ntrain: (*int, optional, default=100*)</span>
<span class="sd">            Number of training samples to generate. Used only if not loading cached samples.</span>
<span class="sd">            </span>
<span class="sd">        :param ntest: (*int, optional, default=0*)</span>
<span class="sd">            Number of test samples to generate. Currently unused.</span>
<span class="sd">            </span>
<span class="sd">        :param reload: (*bool, optional, default=False*)</span>
<span class="sd">            Whether to attempt loading cached samples from previous runs.</span>
<span class="sd">            If True, tries to load from default cache files first.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler: (*str, optional, default=&quot;uniform&quot;*)</span>
<span class="sd">            Sampling method for generating parameter values. Options:</span>
<span class="sd">            </span>
<span class="sd">            - &quot;uniform&quot;: Uniform sampling within bounds (default)</span>
<span class="sd">            - &quot;sobol&quot;: Low-discrepancy Sobol sequence sampling</span>
<span class="sd">            - &quot;lhs&quot;: Latin hypercube sampling</span>
<span class="sd">            </span>
<span class="sd">        :param train_file: (*str, optional, default=&quot;initial_training_sample.npz&quot;*)</span>
<span class="sd">            Filename for cached training samples relative to savedir.</span>
<span class="sd">            Format: .npz file containing &#39;theta&#39; and &#39;y&#39; arrays.</span>
<span class="sd">            </span>
<span class="sd">        :param test_file: (*str, optional, default=&quot;initial_test_sample.npz&quot;*)</span>
<span class="sd">            Filename for cached test samples relative to savedir. Currently unused.</span>

<span class="sd">        .. note::</span>
<span class="sd">        </span>
<span class="sd">            This method must be called before init_gp() to provide training data for the</span>
<span class="sd">            Gaussian Process. The samples are automatically scaled using the configured</span>
<span class="sd">            theta_scaler and y_scaler.</span>
<span class="sd">            </span>
<span class="sd">            The method sets several important attributes:</span>
<span class="sd">                - _theta, _y: Scaled training samples used by GP</span>
<span class="sd">                - theta0, y0: Unscaled original training samples  </span>
<span class="sd">                - ntrain: Number of training samples</span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            Basic usage with default uniform sampling:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.init_samples(ntrain=200)</span>
<span class="sd">            </span>
<span class="sd">            Use Sobol sampling for better space coverage:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.init_samples(ntrain=150, sampler=&quot;sobol&quot;)</span>
<span class="sd">            </span>
<span class="sd">            Reload from cached file:</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; sm.init_samples(reload=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Load or create training sample</span>
        <span class="k">if</span> <span class="n">reload</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">cache_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">train_file</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading training sample from </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_train</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
                
            <span class="k">except</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unable to reload </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">. Computing new samples...&quot;</span><span class="p">)</span>
                <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntrain</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">train_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntrain</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
            
        <span class="c1"># Training dataset scaled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta0</span> <span class="o">=</span> <span class="n">_theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">_theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">_y</span>
        
        <span class="c1"># Training dataset unscaled </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        
        <span class="c1"># record number of training samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># --------------------------------------------------------</span>
        <span class="c1"># Load or create test sample</span>
        <span class="k">if</span> <span class="n">ntest</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">reload</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">cache_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">test_file</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading test sample from </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                    <span class="n">_theta_test</span><span class="p">,</span> <span class="n">_y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_test</span><span class="p">(</span><span class="n">cache_file</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unable to reload </span><span class="si">{</span><span class="n">cache_file</span><span class="si">}</span><span class="s2">. Computing new samples...&quot;</span><span class="p">)</span>
                    <span class="n">_theta_test</span><span class="p">,</span> <span class="n">_y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntest</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">test_file</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_theta_test</span><span class="p">,</span> <span class="n">_y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_train</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="n">ntest</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="n">test_file</span><span class="p">)</span>

            <span class="c1"># Test dataset scaled</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span> <span class="o">=</span> <span class="n">_theta_test</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span> <span class="o">=</span> <span class="n">_y_test</span>
            
            <span class="c1"># Test dataset unscaled</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="c1"># record number of test samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntest</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_test</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntest</span> <span class="o">=</span> <span class="mi">0</span></div>



<div class="viewcode-block" id="SurrogateModel.set_hyperparam_prior_bounds">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.set_hyperparam_prior_bounds">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_hyperparam_prior_bounds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure prior bounds for GP hyperparameters based on current training data.</span>
<span class="sd">        </span>
<span class="sd">        By default ranges for parameters:</span>
<span class="sd">            - mean: [mean(y) - std(y), mean(y) + std(y)]</span>
<span class="sd">            - amplitude: [0.1, 10]</span>
<span class="sd">            - white noise: [white_noise - 3, white_noise + 3]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Configure GP hyperparameter prior</span>
        <span class="n">hp_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_bounds</span><span class="p">()</span>
        <span class="n">pnames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">(</span><span class="n">include_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">:</span>
            <span class="n">mean_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)]</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;mean:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">mean_bounds</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">:</span>
            <span class="n">amp_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;kernel:k1:log_constant&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">amp_bounds</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">:</span>
            <span class="n">wn_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]</span>
            <span class="n">hp_bounds</span><span class="p">[</span><span class="n">pnames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;white_noise:value&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">wn_bounds</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hp_bounds</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_hyper_prior</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">lnprior_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">)</span></div>


    
<div class="viewcode-block" id="SurrogateModel.fit_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.fit_gp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_theta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit Gaussian Process to training data with current hyperparameters.</span>
<span class="sd">        </span>
<span class="sd">        :param _theta: (*array, optional*) </span>
<span class="sd">            Scaled training parameter samples. If None, uses ``self._theta``.</span>
<span class="sd">            </span>
<span class="sd">        :param _y: (*array, optional*) </span>
<span class="sd">            Scaled training output values. If None, uses ``self._y``.</span>
<span class="sd">            </span>
<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            - gp: Fitted george.GP object</span>
<span class="sd">            - timing: Time taken to fit the GP in seconds</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">_theta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>

        <span class="k">if</span> <span class="n">_y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparam_prior_bounds</span><span class="p">()</span>
        <span class="n">gp</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">configure_gp</span><span class="p">(</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                    <span class="n">fit_amp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">,</span> 
                                    <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span>
                                    <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">,</span>
                                    <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: fit_gp failed with point </span><span class="si">{</span><span class="n">_theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">. Reoptimizing hyperparameters...&quot;</span><span class="p">)</span>
            <span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp</span><span class="p">()</span>
            
        <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
        
        <span class="n">timing</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>

        <span class="k">return</span> <span class="n">gp</span><span class="p">,</span> <span class="n">timing</span></div>



<div class="viewcode-block" id="SurrogateModel.opt_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.opt_gp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">opt_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize GP hyperparameters by maximizing the log marginal likelihood.</span>
<span class="sd">        </span>
<span class="sd">        :returns: (*tuple*) </span>
<span class="sd">            - op_gp: Optimized george.GP object with updated hyperparameters</span>
<span class="sd">            - timing: Time taken for optimization in seconds</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparam_prior_bounds</span><span class="p">()</span>
        
        <span class="n">failed</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">failed</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># create array of random initial hyperparameters:</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">,</span> <span class="n">nsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_nopt</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
                <span class="c1"># if gp exists, use current hyperparameters as a starting point </span>
                <span class="n">current_hp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">(</span><span class="n">include_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_hyper_prior</span><span class="p">(</span><span class="n">current_hp</span><span class="p">)):</span>
                    <span class="n">p0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_hp</span>

            <span class="n">op_gp</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">optimize_gp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">gp_hyper_prior</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span>
                                        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_method</span><span class="p">,</span>
                                        <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hp_bounds</span><span class="p">,</span>
                                        <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">op_gp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: opt_gp hyperparameter optimization failed. Reoptimizing hyperparameters...&quot;</span><span class="p">)</span>
                <span class="n">failed</span> <span class="o">=</span> <span class="kc">True</span> 
            <span class="k">else</span><span class="p">:</span>
                <span class="n">failed</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">tf</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">timing</span> <span class="o">=</span> <span class="n">tf</span> <span class="o">-</span> <span class="n">t0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparam_opt_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timing</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimized </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">(</span><span class="n">include_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span><span class="si">}</span><span class="s2"> hyperparameters: (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">timing</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s)&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">op_gp</span><span class="p">,</span> <span class="n">timing</span></div>


        
<div class="viewcode-block" id="SurrogateModel.init_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.init_gp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_gp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ExpSquaredKernel&quot;</span><span class="p">,</span>
                <span class="n">fit_amp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">fit_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">fit_white_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">white_noise</span><span class="o">=-</span><span class="mi">12</span><span class="p">,</span> 
                <span class="n">gp_scale_rng</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">gp_opt_method</span><span class="o">=</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">,</span> 
                <span class="n">gp_nopt</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">}):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Gaussian Process surrogate model with specified kernel and hyperparameters.</span>

<span class="sd">        This function sets up a Gaussian Process (GP) using the george library with the specified </span>
<span class="sd">        kernel type and configuration. The GP is initialized with random scale lengths and then</span>
<span class="sd">        fitted to the current training data.</span>

<span class="sd">        :param kernel: (*str or george kernel object, optional*) </span>
<span class="sd">            Kernel type for the Gaussian Process. Can be either a string specifying one of the </span>
<span class="sd">            built-in kernels or a george kernel object. Default is &quot;ExpSquaredKernel&quot;.</span>
<span class="sd">            </span>
<span class="sd">            Built-in options:</span>
<span class="sd">                - ``&#39;ExpSquaredKernel&#39;``: Squared exponential (RBF) kernel, smooth functions</span>
<span class="sd">                - ``&#39;Matern32Kernel&#39;``: Matérn kernel with ν=3/2, moderately smooth functions  </span>
<span class="sd">                - ``&#39;Matern52Kernel&#39;``: Matérn kernel with ν=5/2, smooth functions</span>
<span class="sd">                - ``&#39;RationalQuadraticKernel&#39;``: Rational quadratic kernel, scale mixture of RBF kernels</span>
<span class="sd">                </span>
<span class="sd">            See https://george.readthedocs.io/en/latest/user/kernels/ for more details.</span>

<span class="sd">        :param fit_amp: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the amplitude (overall scale) hyperparameter of the kernel.</span>
<span class="sd">            If True, the GP will learn the optimal amplitude from data. Default is True.</span>

<span class="sd">        :param fit_mean: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the mean function hyperparameter. If True, the GP will learn</span>
<span class="sd">            a constant mean offset. If False, assumes zero mean. Default is True.</span>

<span class="sd">        :param fit_white_noise: (*bool, optional*) </span>
<span class="sd">            Whether to optimize the white noise (nugget) hyperparameter. If True, the GP will</span>
<span class="sd">            learn the optimal noise level. If False, uses the fixed value from white_noise.</span>
<span class="sd">            Default is True.</span>

<span class="sd">        :param white_noise: (*float, optional*) </span>
<span class="sd">            Log-scale white noise parameter. If fit_white_noise=False, this fixed value is used.</span>
<span class="sd">            If fit_white_noise=True, this serves as the initial guess. Typical values are </span>
<span class="sd">            between -15 (very low noise) and -5 (high noise). Default is -12.</span>

<span class="sd">        :param gp_scale_rng: (*list of two floats, optional*) </span>
<span class="sd">            Log-scale bounds for the characteristic length scale parameters of the kernel.</span>
<span class="sd">            Format: [log_min_scale, log_max_scale]. These bounds apply to all input dimensions.</span>
<span class="sd">            Default is [-2, 2], corresponding to scales between ~0.14 and ~7.4 in original units.</span>

<span class="sd">        :param overwrite: (*bool, optional*) </span>
<span class="sd">            If True, allows reinitializing the GP even if one already exists. If False and a GP</span>
<span class="sd">            already exists, raises an AssertionError. Default is False.</span>

<span class="sd">        :param gp_opt_method: (*str, optional*) </span>
<span class="sd">            Optimization method for GP hyperparameter optimization. Passed to scipy.optimize.minimize.</span>
<span class="sd">            Common options: &#39;newton-cg&#39;, &#39;l-bfgs-b&#39;, &#39;bfgs&#39;, &#39;cg&#39;. Default is &#39;newton-cg&#39;.</span>

<span class="sd">        :param gp_nopt: (*int, optional*) </span>
<span class="sd">            Number of optimization restarts for GP hyperparameter optimization. Multiple restarts</span>
<span class="sd">            help avoid local minima. Default is 3.</span>

<span class="sd">        :param optimizer_kwargs: (*dict, optional*) </span>
<span class="sd">            Additional keyword arguments passed to the scipy optimizer. Common options include</span>
<span class="sd">            &#39;max_iter&#39; (maximum iterations) and convergence tolerances. </span>
<span class="sd">            Default is {&quot;max_iter&quot;: 50}.</span>

<span class="sd">        :raises AssertionError: </span>
<span class="sd">            If a GP already exists and overwrite=False.</span>
<span class="sd">        :raises ValueError: </span>
<span class="sd">            If an invalid kernel name is provided.</span>
<span class="sd">        :raises Exception: </span>
<span class="sd">            If GP initialization fails after multiple attempts with different scale lengths.</span>

<span class="sd">        .. note:: </span>
<span class="sd">        </span>
<span class="sd">            This function must be called after init_samples() since it requires training data</span>
<span class="sd">            to initialize the GP. The function will automatically retry initialization with</span>
<span class="sd">            different random scale lengths if the initial attempt fails.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            &gt;&gt;&gt; # Basic initialization with default settings</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp()</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # Custom kernel with specific hyperparameter settings</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp(kernel=&quot;Matern52Kernel&quot;, </span>
<span class="sd">            ...            fit_white_noise=False, </span>
<span class="sd">            ...            white_noise=-10,</span>
<span class="sd">            ...            gp_scale_rng=[-1, 1])</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; # High-precision optimization</span>
<span class="sd">            &gt;&gt;&gt; sm.init_gp(gp_opt_method=&quot;l-bfgs-b&quot;,</span>
<span class="sd">            ...            gp_nopt=5,</span>
<span class="sd">            ...            optimizer_kwargs={&quot;max_iter&quot;: 100, &quot;ftol&quot;: 1e-9})</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">overwrite</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;GP kernel already assigned. Use overwrite=True to re-assign the kernel.&quot;</span><span class="p">)</span>
            
        <span class="c1"># optional hyperparameter choices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span> <span class="o">=</span> <span class="n">fit_amp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span> <span class="o">=</span> <span class="n">fit_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span> <span class="o">=</span> <span class="n">fit_white_noise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span> <span class="o">=</span> <span class="n">white_noise</span>
        
        <span class="c1"># count total optional hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_nparam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp_nparam</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp_nparam</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp_nparam</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># GP hyperparameter optimization method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_method</span> <span class="o">=</span> <span class="n">gp_opt_method</span>

        <span class="c1"># GP hyperparameter number of opt restarts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_nopt</span> <span class="o">=</span> <span class="n">gp_nopt</span>

        <span class="c1"># GP hyperparameter optimization kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span>

        <span class="c1"># set the bounds for scale length parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_scale_rng</span> <span class="o">=</span> <span class="n">gp_scale_rng</span>
        <span class="c1"># metric_bounds expects log-scale bounds</span>
        <span class="n">log_metric_bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">metric_bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>

        <span class="n">valid_scales</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">max_attempts</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Prevent infinite loops</span>
        <span class="n">attempt</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">while</span> <span class="n">valid_scales</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_attempts</span><span class="p">:</span>
            <span class="n">attempt</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Generate initial scale length in linear scale (metric parameter expects linear scale)</span>
            <span class="c1"># gp_scale_rng is in log scale, so convert to linear scale for initial guess</span>
            <span class="n">log_initial_lscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">gp_scale_rng</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
            <span class="n">initial_lscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_initial_lscale</span><span class="p">)</span>

            <span class="c1"># if self.verbose:</span>
            <span class="c1">#     print(f&quot;Attempt {attempt}: Trying initial scale lengths: {initial_lscale} (log: {log_initial_lscale})&quot;)</span>
            
            <span class="c1"># Note: metric is linear scale, but metric_bounds are log scale!</span>
            <span class="c1"># https://github.com/dfm/george/issues/150</span>
            
            <span class="c1"># Initialize GP kernel</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Stationary kernels</span>
                <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;ExpSquaredKernel&quot;</span><span class="p">:</span>
                    <span class="c1"># Guess initial metric, or scale length of the covariances (must be &gt; 0)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSquaredKernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with squared exponential kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;RationalQuadraticKernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">RationalQuadraticKernel</span><span class="p">(</span><span class="n">log_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with rational quadratic kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;Matern32Kernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern32Kernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with Matérn-3/2 kernel.&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;Matern52Kernel&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern52Kernel</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">initial_lscale</span><span class="p">,</span> <span class="n">metric_bounds</span><span class="o">=</span><span class="n">log_metric_bounds</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span> <span class="o">=</span> <span class="n">kernel</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized GP with Matérn-5/2 kernel.&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; is not a valid option. Valid options: ExpSquaredKernel, Matern32Kernel, Matern52Kernel, RationalQuadraticKernel&quot;</span><span class="p">)</span>
                
                <span class="c1"># create GP first time </span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="o">=</span> <span class="n">gp_utils</span><span class="o">.</span><span class="n">configure_gp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                                <span class="n">fit_amp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_amp</span><span class="p">,</span> 
                                                <span class="n">fit_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_mean</span><span class="p">,</span>
                                                <span class="n">fit_white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_white_noise</span><span class="p">,</span>
                                                <span class="n">white_noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">white_noise</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: configure_gp returned None.&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data shape: theta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, y=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scale lengths: </span><span class="si">{</span><span class="n">initial_lscale</span><span class="si">}</span><span class="s2"> (log: </span><span class="si">{</span><span class="n">log_initial_lscale</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scale bounds: </span><span class="si">{</span><span class="n">metric_bounds</span><span class="si">}</span><span class="s2"> (log: </span><span class="si">{</span><span class="n">log_metric_bounds</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Retrying with new initial scale length...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">valid_scales</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully initialized GP on attempt </span><span class="si">{</span><span class="n">attempt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception during GP initialization on attempt </span><span class="si">{</span><span class="n">attempt</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Kernel: </span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Initial scales: </span><span class="si">{</span><span class="n">initial_lscale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Scale bounds: </span><span class="si">{</span><span class="n">log_metric_bounds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Data shape: theta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, y=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exception: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Retrying with new initial scale length...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">continue</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_scales</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize GP after </span><span class="si">{</span><span class="n">max_attempts</span><span class="si">}</span><span class="s2"> attempts. &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;Check your data, kernel choice, and scale bounds. &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;Current settings: kernel=</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">, gp_scale_rng=</span><span class="si">{</span><span class="n">gp_scale_rng</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Fit GP with training sample and kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_gp</span><span class="p">()</span>

        <span class="c1"># Optimize GP hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp</span><span class="p">()</span></div>

        
    
<div class="viewcode-block" id="SurrogateModel.eval_gp_at_iteration">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.eval_gp_at_iteration">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eval_gp_at_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        
        <span class="n">gp_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span> 
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
            <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Handle iter=-1 case to use all data instead of excluding last element</span>
            <span class="k">if</span> <span class="nb">iter</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span>
                <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span>
                <span class="c1"># Use the latest parameters (last in the list)</span>
                <span class="n">gp_iter</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_theta_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[:</span><span class="nb">iter</span><span class="p">]</span>
                <span class="n">_y_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[:</span><span class="nb">iter</span><span class="p">]</span>
                <span class="n">gp_iter</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">][</span><span class="nb">iter</span><span class="p">])</span>
            
        <span class="n">gp_iter</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta_cond</span><span class="p">)</span>

        <span class="k">return</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">gp_iter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">_y_cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.surrogate_log_likelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.surrogate_log_likelihood">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">surrogate_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta_xs</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate predictive mean of the GP at point(s) ``theta_xs``</span>
<span class="sd">        </span>
<span class="sd">        This method is vectorized to handle both single parameter vectors and</span>
<span class="sd">        arrays of parameter vectors efficiently.</span>

<span class="sd">        :param theta_xs: Point(s) to evaluate GP mean at. Can be:</span>
<span class="sd">            - 1D array of shape (ndim,) for single point</span>
<span class="sd">            - 2D array of shape (npoints, ndim) for multiple points</span>
<span class="sd">        :type theta_xs: *array-like*</span>
<span class="sd">        :param iter: Iteration number of GP to use. If -1, uses most recent GP.</span>
<span class="sd">        :type iter: *int, optional*</span>
<span class="sd">        :param return_var: Whether to also return variance predictions.</span>
<span class="sd">        :type return_var: *bool, optional*</span>

<span class="sd">        :returns: </span>
<span class="sd">            - **ypred** (*array*) -- GP mean(s) evaluated at ``theta_xs``. Shape matches input.</span>
<span class="sd">            - **varpred** (*array, optional*) -- GP variance(s) if return_var=True.</span>
<span class="sd">        :rtype: *array or tuple of arrays*</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Convert input to numpy array and handle dimensionality</span>
        <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Handle 1D input (single point)</span>
        <span class="k">if</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">theta_xs</span> <span class="o">=</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">original_shape_1d</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;theta_xs must be 1D or 2D array, got </span><span class="si">{</span><span class="n">theta_xs</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling transformation</span>
        <span class="n">_theta_xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        
        <span class="c1"># Get GP at specified iteration</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">gp_ii</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_gp_at_iteration</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gp_ii</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="n">return_var</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Apply the GP and handle return values</span>
        <span class="k">if</span> <span class="n">return_var</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">_ypred</span> <span class="o">=</span> <span class="n">gp_ii</span><span class="p">(</span><span class="n">_theta_xs</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="c1"># Return single value if input was 1D, otherwise return array</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_ypred</span><span class="p">,</span> <span class="n">_varpred</span> <span class="o">=</span> <span class="n">gp_ii</span><span class="p">(</span><span class="n">_theta_xs</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">varpred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_varpred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="c1"># Return single values if input was 1D, otherwise return arrays</span>
            <span class="k">if</span> <span class="n">original_shape_1d</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">varpred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">varpred</span></div>

    
    
<div class="viewcode-block" id="SurrogateModel.surrogate_likelihood">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.surrogate_likelihood">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">surrogate_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta_xs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate predictive probability (not log-probability) of the GP at point(s) theta_xs</span>
<span class="sd">        </span>
<span class="sd">        This method is vectorized to handle both single parameter vectors and</span>
<span class="sd">        arrays of parameter vectors efficiently.</span>
<span class="sd">        </span>
<span class="sd">        :param theta_xs: Point(s) to evaluate GP probability at. Can be:</span>
<span class="sd">            - 1D array of shape (ndim,) for single point</span>
<span class="sd">            - 2D array of shape (npoints, ndim) for multiple points</span>
<span class="sd">        :type theta_xs: *array-like*</span>
<span class="sd">        </span>
<span class="sd">        :returns: GP probability/probabilities evaluated at ``theta_xs``. Shape matches input.</span>
<span class="sd">        :rtype: *float or array*</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get log-probability from GP (already vectorized)</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">(</span><span class="n">theta_xs</span><span class="p">)</span>
        
        <span class="c1"># Convert to probability (works element-wise for arrays)</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">prob</span></div>



<div class="viewcode-block" id="SurrogateModel.find_next_point">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.find_next_point">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">find_next_point</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{}):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find next set of ``(theta, y)`` training points by maximizing the</span>
<span class="sd">        active learning utility function.</span>

<span class="sd">        :param nopt: (*int, optional*) </span>
<span class="sd">            Number of times to restart the objective function optimization. </span>
<span class="sd">            Defaults to 1. Increase to avoid converging to local maxima.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">opt_timing_0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">utility</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_obj_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grad_obj_fn</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="n">_thetaN</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">minimize_objective</span><span class="p">(</span><span class="n">obj_fn</span><span class="p">,</span> 
                                           <span class="n">grad_obj_fn</span><span class="p">,</span>
                                           <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">,</span>
                                           <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span>
                                           <span class="n">ps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior_sampler</span><span class="p">,</span>
                                           <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_opt_method</span><span class="p">,</span>
                                           <span class="n">ncore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">,</span>
                                           <span class="n">options</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span>
                                           <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">)</span>

        <span class="n">opt_timing</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">opt_timing_0</span>

        <span class="c1"># evaluate function at the optimized theta</span>
        <span class="n">_thetaN</span> <span class="o">=</span> <span class="n">_thetaN</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">_yN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lnlike_fn</span><span class="p">(</span><span class="n">_thetaN</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_thetaN</span><span class="p">,</span> <span class="n">_yN</span><span class="p">,</span> <span class="n">opt_timing</span></div>



<div class="viewcode-block" id="SurrogateModel.active_train">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.active_train">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">active_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> 
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform active learning to iteratively improve the surrogate model.</span>
<span class="sd">        </span>
<span class="sd">        Uses acquisition functions to intelligently select new training points that</span>
<span class="sd">        will most improve the Gaussian Process model. Different algorithms balance</span>
<span class="sd">        exploration (uncertainty reduction) vs exploitation (finding optima).</span>
<span class="sd">        </span>
<span class="sd">        :param niter: (*int, optional, default=100*)</span>
<span class="sd">            Number of active learning iterations. Each iteration adds one new training point.</span>
<span class="sd">            </span>
<span class="sd">        :param algorithm: (*str, optional, default=&quot;bape&quot;*)</span>
<span class="sd">            Active learning algorithm. Options:</span>
<span class="sd">            - &quot;bape&quot;: Bayesian Active Parameter Estimation (exploration-focused)</span>
<span class="sd">            - &quot;jones&quot;: Jones algorithm (exploitation-focused, good for optimization)</span>
<span class="sd">            - &quot;agp&quot;: Augmented Gaussian Process (balanced)</span>
<span class="sd">            - &quot;alternate&quot;: Alternates between exploration and exploitation</span>
<span class="sd">            </span>
<span class="sd">        :param gp_opt_freq: (*int, optional, default=20*)</span>
<span class="sd">            Frequency of GP hyperparameter re-optimization. GP hyperparameters are</span>
<span class="sd">            re-optimized every gp_opt_freq iterations. Lower values = more optimization.</span>
<span class="sd">            </span>
<span class="sd">        :param save_progress: (*bool, optional, default=False*)</span>
<span class="sd">            Whether to save training progress data for later analysis.</span>
<span class="sd">            </span>
<span class="sd">        :param obj_opt_method: (*str, optional, default=&quot;nelder-mead&quot;*)</span>
<span class="sd">            Optimization method for acquisition function. Options:</span>
<span class="sd">            - &quot;l-bfgs-b&quot;: L-BFGS-B (good with gradients)</span>
<span class="sd">            - &quot;nelder-mead&quot;: Nelder-Mead simplex (gradient-free)</span>
<span class="sd">            </span>
<span class="sd">        :param nopt: (*int, optional, default=1*)</span>
<span class="sd">            Number of optimization restarts for acquisition function. Higher values</span>
<span class="sd">            help avoid local minima but increase computation time.</span>
<span class="sd">            </span>
<span class="sd">        :param n_attempts: (*int, optional, default=5*)</span>
<span class="sd">            Number of attempts for each optimization restart.</span>
<span class="sd">            </span>
<span class="sd">        :param use_grad_opt: (*bool, optional, default=True*)</span>
<span class="sd">            Whether to use gradient information if available. Set False for</span>
<span class="sd">            gradient-free optimization.</span>
<span class="sd">            </span>
<span class="sd">        :param optimizer_kwargs: (*dict, optional, default={}*)</span>
<span class="sd">            Additional keyword arguments passed to the optimizer.</span>
<span class="sd">            </span>
<span class="sd">        :param show_progress: (*bool, optional, default=True*)</span>
<span class="sd">            Whether to display progress bar during training.</span>

<span class="sd">        .. note::</span>
<span class="sd">        </span>
<span class="sd">            Active learning algorithms have different purposes:</span>
<span class="sd">            </span>
<span class="sd">            - **BAPE**: Best for uncertainty quantification and space-filling</span>
<span class="sd">            - **Jones**: Best for finding likelihood maxima/minima (optimization)  </span>
<span class="sd">            - **Alternate**: Good balance for both exploration and exploitation</span>
<span class="sd">            - **AGP**: Another balanced approach</span>
<span class="sd">            </span>
<span class="sd">            The method automatically handles GP re-training and hyperparameter optimization</span>
<span class="sd">            based on the specified frequency. Training data is accumulated in _theta and _y</span>
<span class="sd">            attributes.</span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">        Basic active learning with BAPE:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=50, algorithm=&quot;bape&quot;)</span>
<span class="sd">        </span>
<span class="sd">        Optimization-focused active learning:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=30, algorithm=&quot;jones&quot;, gp_opt_freq=10)</span>
<span class="sd">        </span>
<span class="sd">        Balanced approach with frequent GP optimization:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.active_train(niter=40, algorithm=&quot;alternate&quot;, gp_opt_freq=5)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Set algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">utility</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">assign_utility</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obj_opt_method</span> <span class="o">=</span> <span class="s2">&quot;nelder-mead&quot;</span>
        <span class="k">if</span> <span class="n">use_grad_opt</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_utility</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># GP hyperparameter optimization frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">=</span> <span class="n">gp_opt_freq</span>

        <span class="c1"># Objective function optimization method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obj_opt_method</span> <span class="o">=</span> <span class="n">obj_opt_method</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">first_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">first_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">niter</span><span class="si">}</span><span class="s2"> active learning iterations using </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

        <span class="c1"># start timing active learning </span>
        <span class="n">train_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Create iterator with or without progress bar based on show_progress parameter</span>
        <span class="n">iterator</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">niter</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="n">show_progress</span> <span class="k">else</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">niter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>

            <span class="c1"># Find next training point!</span>
            <span class="n">thetaN</span><span class="p">,</span> <span class="n">yN</span><span class="p">,</span> <span class="n">opt_timing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_next_point</span><span class="p">(</span><span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>

            <span class="c1"># add theta and y to training samples</span>
            <span class="n">theta_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">thetaN</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">yN</span><span class="p">)</span>

            <span class="c1"># Fit GP with new training point</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">fit_gp_timing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="n">theta_prop</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="n">y_prop</span><span class="p">)</span>

            <span class="c1"># If proposed (theta, y) did not cause fitting issues, save to surrogate model obj</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">theta_prop</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">y_prop</span>

            <span class="c1"># print(&quot;Total training samples:&quot;, len(self._theta))</span>

            <span class="c1"># record active learning train runtime</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">train_t0</span> 

            <span class="c1"># evaluate gp training error (scaled)</span>
            <span class="n">_ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ypred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ypred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">training_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">()</span> <span class="o">-</span> <span class="n">ypred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">training_scaled_mse</span> <span class="o">=</span> <span class="n">training_mse</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">())</span>

            <span class="c1"># evaluate gp test error (scaled)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_theta_test&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_y_test&#39;</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="n">_ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta_test</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">ytest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_ytest</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                    <span class="n">ytest_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                    <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ytest_true</span> <span class="o">-</span> <span class="n">ytest</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">test_mse</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                    <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                <span class="n">test_scaled_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># evaluate convergence criteria</span>
            <span class="n">gp_kl_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># save results to a dictionary</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_scaled_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_scaled_mse</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_kl_divergence&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gp_kl_divergence</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_train_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_gp_timing</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;obj_fn_opt_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">opt_timing</span><span class="p">)</span>

            <span class="c1"># record total number of training samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
            <span class="c1"># number of active training samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>

            <span class="c1"># Optimize GP?</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

                <span class="c1"># re-optimize hyperparamters</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp</span><span class="p">()</span>
                
                <span class="c1"># record which iteration hyperparameters were optimized</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span> <span class="o">+</span> <span class="n">first_iter</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="p">(</span><span class="n">save_progress</span> <span class="o">==</span> <span class="kc">True</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ii</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_error&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_hyperparam&quot;</span><span class="p">])</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_fit_2D&quot;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plots</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp_train_scatter&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span></div>



<div class="viewcode-block" id="SurrogateModel.lnprob">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.lnprob">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">lnprob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log probability function used for ``emcee``, which sums the prior with the surrogate model likelihood</span>

<span class="sd">        .. math::</span>

<span class="sd">            \\ln P(\\theta | x) \\propto \\ln P(x | \\theta) + \\ln P(\\theta)</span>

<span class="sd">        where \\ln P(x | \\theta) is the surrogate likelihood function and \\ln P(\\theta) is the prior function.</span>

<span class="sd">        :param theta: (*array, required*) </span>
<span class="sd">            Array of model input parameters to evaluate model probability at.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;gp&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;GP has not been trained&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;prior_fn&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;prior_fn has not been specified&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;like_fn&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">lnp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lnp</span></div>



<div class="viewcode-block" id="SurrogateModel.find_map">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.find_map">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">find_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span><span class="p">,</span> <span class="n">nRestarts</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Not implemented.&quot;</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.run_emcee">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_emcee">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_emcee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nwalkers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nsteps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">),</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{},</span>
                  <span class="n">opt_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">multi_proc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prior_fn_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the emcee affine-invariant MCMC algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the emcee package to perform Markov Chain Monte Carlo (MCMC) </span>
<span class="sd">        sampling on either the trained GP surrogate model or the true likelihood function.</span>
<span class="sd">        The affine-invariant ensemble sampler is robust and works well for a wide variety</span>
<span class="sd">        of posterior shapes without requiring manual tuning of step sizes.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)</span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_fn: (*callable or None, optional*)</span>
<span class="sd">            Log-prior function with signature prior_fn(theta). Should return log-probability</span>
<span class="sd">            density. If None, uses uniform prior with bounds from self.bounds.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param nwalkers: (*int or None, optional*)</span>
<span class="sd">            Number of MCMC walkers in the ensemble. Should be at least 2*ndim.</span>
<span class="sd">            If None, defaults to 10*ndim. More walkers improve convergence but increase</span>
<span class="sd">            computational cost. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param nsteps: (*int, optional*)</span>
<span class="sd">            Number of MCMC steps per walker. Total number of likelihood evaluations</span>
<span class="sd">            will be nwalkers * nsteps. Default is 50000.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to emcee.EnsembleSampler constructor.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;a&#39;: Stretch move scale parameter (default: 2.0)</span>
<span class="sd">            - &#39;moves&#39;: Custom proposal moves</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param run_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the run_mcmc() method.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;progress&#39;: Show progress bar (default: True)</span>
<span class="sd">            - &#39;store&#39;: Store chain in memory (default: True)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param opt_init: (*bool, optional*)</span>
<span class="sd">            Whether to initialize walkers near the maximum a posteriori (MAP) estimate.</span>
<span class="sd">            If True, uses find_map() to locate starting point. If False, initializes</span>
<span class="sd">            walkers randomly from the prior. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            Whether to use multiprocessing with self.ncore processes. Generally</span>
<span class="sd">            recommended for expensive likelihood evaluations. Default is True.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_fn_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior function for logging purposes. If None</span>
<span class="sd">            and prior_fn is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param burn: (*int or None, optional*)</span>
<span class="sd">            Number of burn-in samples to discard from each walker. If None, </span>
<span class="sd">            automatically estimates burn-in using autocorrelation analysis.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param thin: (*int or None, optional*)</span>
<span class="sd">            Thinning factor - keep every thin-th sample to reduce autocorrelation.</span>
<span class="sd">            If None, automatically estimates based on autocorrelation time.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        sampler : emcee.EnsembleSampler</span>
<span class="sd">            The emcee sampler object containing full chain and metadata</span>
<span class="sd">        emcee_samples : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final MCMC samples after burn-in and thinning</span>
<span class="sd">        emcee_samples_full : ndarray of shape (nsteps, nwalkers, ndim)</span>
<span class="sd">            Full MCMC chain before processing</span>
<span class="sd">        emcee_samples_true : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        emcee_samples_gp : ndarray of shape (nsamples_final, ndim)</span>
<span class="sd">            Final samples when using surrogate likelihood</span>
<span class="sd">        emcee_run : bool</span>
<span class="sd">            Flag indicating emcee has been successfully run</span>
<span class="sd">        emcee_runtime : float</span>
<span class="sd">            Wall-clock time taken for emcee sampling in seconds</span>
<span class="sd">        nwalkers : int</span>
<span class="sd">            Number of walkers used</span>
<span class="sd">        nsteps : int</span>
<span class="sd">            Number of steps per walker</span>
<span class="sd">        burn : int</span>
<span class="sd">            Burn-in length used for final samples</span>
<span class="sd">        thin : int</span>
<span class="sd">            Thinning factor used for final samples</span>
<span class="sd">        iburn : int</span>
<span class="sd">            Automatically estimated burn-in length</span>
<span class="sd">        ithin : int</span>
<span class="sd">            Automatically estimated thinning factor</span>
<span class="sd">        acc_frac : float</span>
<span class="sd">            Mean acceptance fraction across all walkers</span>
<span class="sd">        autcorr_time : float</span>
<span class="sd">            Mean autocorrelation time in steps</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;likelihood&quot;)</span>
<span class="sd">        prior_fn_comment : str</span>
<span class="sd">            Description of prior function used</span>
<span class="sd">            </span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">        </span>
<span class="sd">        Sample surrogate model with default settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee()</span>
<span class="sd">        </span>
<span class="sd">        Sample true likelihood with specific settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(like_fn=&quot;true&quot;, nwalkers=100, nsteps=10000)</span>
<span class="sd">        </span>
<span class="sd">        Use custom prior and optimize initialization:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; def log_prior(theta):</span>
<span class="sd">        ...     # Custom Gaussian prior</span>
<span class="sd">        ...     return -0.5 * np.sum((theta/2)**2)</span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(prior_fn=log_prior, opt_init=True)</span>
<span class="sd">        </span>
<span class="sd">        Run with manual burn-in and thinning:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee(nsteps=100000, burn=10000, thin=10)</span>
<span class="sd">        </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        emcee documentation: https://emcee.readthedocs.io/</span>
<span class="sd">        Foreman-Mackey et al. (2013): &quot;emcee: The MCMC Hammer&quot;, PASP, 125, 306-312</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">emcee</span>

        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing emcee with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;likelihood&quot;</span>

        <span class="k">if</span> <span class="n">prior_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No prior_fn specified. Defaulting to uniform prior with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">lnprior_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

            <span class="c1"># Comment for output log file</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span>  <span class="sa">f</span><span class="s2">&quot;Default uniform prior. </span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: ut.prior_fn_uniform</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span> <span class="o">=</span> <span class="n">prior_fn</span>

            <span class="c1"># Comment for output log file</span>
            <span class="k">if</span> <span class="n">prior_fn_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;User defined prior.&quot;</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">+=</span> <span class="s2">&quot;Prior function: unrecorded&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_fn_comment</span> <span class="o">=</span> <span class="n">prior_fn_comment</span>

        <span class="c1"># number of walkers, and number of steps per walker</span>
        <span class="k">if</span> <span class="n">nwalkers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">nsteps</span><span class="p">)</span>

        <span class="c1"># Optimize walker initialization?</span>
        <span class="k">if</span> <span class="n">opt_init</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># start walkers near the estimated maximum</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_map</span><span class="p">(</span><span class="n">prior_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_fn</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># start walkers at random points in the prior space</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">prior_sampler</span><span class="p">(</span><span class="n">nsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># set up multiprocessing pool</span>
        <span class="k">if</span> <span class="n">multi_proc</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span>
            <span class="n">emcee_ncore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">emcee_ncore</span> <span class="o">=</span> <span class="mi">1</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running emcee with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="si">}</span><span class="s2"> walkers for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span><span class="si">}</span><span class="s2"> steps on </span><span class="si">{</span><span class="n">emcee_ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>

        <span class="c1"># Run the sampler!</span>
        <span class="n">emcee_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nwalkers</span><span class="p">,</span> 
                                             <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> 
                                             <span class="bp">self</span><span class="o">.</span><span class="n">lnprob</span><span class="p">,</span> 
                                             <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
                                             <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nsteps</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>

        <span class="c1"># record emcee runtime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">emcee_t0</span>

        <span class="c1"># burn, thin, and flatten samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iburn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ithin</span> <span class="o">=</span> <span class="n">mcmc_utils</span><span class="o">.</span><span class="n">estimate_burnin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_full</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">burn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">burn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iburn</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">burn</span> <span class="o">=</span> <span class="n">burn</span>

        <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ithin</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">(</span><span class="n">discard</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">burn</span><span class="p">,</span> <span class="n">flat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">thin</span><span class="p">)</span> 
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;surrogate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples_gp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span>

        <span class="c1"># get acceptance fraction and autocorrelation time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acc_frac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">acceptance_fraction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autcorr_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_autocorr_time</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total samples: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean acceptance fraction: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">acc_frac</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean autocorrelation time: </span><span class="si">{0:.3f}</span><span class="s2"> steps&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autcorr_time</span><span class="p">))</span>

        <span class="c1"># record that emcee has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emcee_run</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># close pool</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pool</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/emcee_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.npz&quot;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>   
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/emcee_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="p">)</span></div>

                

<div class="viewcode-block" id="SurrogateModel.run_dynesty">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.run_dynesty">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">run_dynesty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">like_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;dynamic&quot;</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">run_kwargs</span><span class="o">=</span><span class="p">{},</span>
                    <span class="n">multi_proc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior_transform_comment</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the posterior using the dynesty nested sampling algorithm.</span>
<span class="sd">        </span>
<span class="sd">        This method uses the dynesty package to perform nested sampling on either the </span>
<span class="sd">        trained GP surrogate model or the true likelihood function. Dynesty is particularly </span>
<span class="sd">        effective for estimating the Bayesian evidence and exploring multi-modal posteriors.</span>
<span class="sd">        </span>
<span class="sd">        :param like_fn: (*callable, str, or None, optional*)</span>
<span class="sd">            Likelihood function to sample. Options:</span>
<span class="sd">            - None (default): Uses the trained GP surrogate model (self.surrogate_log_likelihood)</span>
<span class="sd">            - &quot;surrogate&quot;, &quot;gp&quot;: Uses the GP surrogate model explicitly</span>
<span class="sd">            - &quot;true&quot;: Uses the true likelihood function (self.true_log_likelihood)  </span>
<span class="sd">            - callable: Custom likelihood function with signature like_fn(theta)</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform: (*callable or None, optional*)</span>
<span class="sd">            Prior transformation function that maps from unit hypercube [0,1]^ndim</span>
<span class="sd">            to the parameter space. Should have signature prior_transform(u) where</span>
<span class="sd">            u is array of shape (ndim,) with values in [0,1]. If None, uses uniform</span>
<span class="sd">            prior with bounds from self.bounds. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param mode: (*{&quot;dynamic&quot;, &quot;static&quot;}, optional*)</span>
<span class="sd">            Dynesty sampling mode. &quot;dynamic&quot; uses DynamicNestedSampler which adaptively</span>
<span class="sd">            allocates live points, while &quot;static&quot; uses fixed number of live points.</span>
<span class="sd">            Dynamic mode is generally more efficient. Default is &quot;dynamic&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param sampler_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the dynesty sampler constructor.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;nlive&#39;: Number of live points (default: 50*ndim)</span>
<span class="sd">            - &#39;bound&#39;: Bounding method (&#39;multi&#39;, &#39;single&#39;, &#39;none&#39;)</span>
<span class="sd">            - &#39;sample&#39;: Sampling method (&#39;auto&#39;, &#39;unif&#39;, &#39;rwalk&#39;, &#39;slice&#39;, &#39;rslice&#39;, &#39;hslice&#39;)</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param run_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional keyword arguments passed to the run_nested() method.</span>
<span class="sd">            Common options include:</span>
<span class="sd">            - &#39;dlogz&#39;: Target evidence uncertainty (default: 0.5)</span>
<span class="sd">            - &#39;maxiter&#39;: Maximum number of iterations (default: 50000)</span>
<span class="sd">            - &#39;wt_kwargs&#39;: Weight function arguments (default: {&#39;pfrac&#39;: 1.0})</span>
<span class="sd">            - &#39;stop_kwargs&#39;: Stopping criterion arguments (default: {&#39;pfrac&#39;: 1.0})</span>
<span class="sd">            Default is {}.</span>
<span class="sd">            </span>
<span class="sd">        :param multi_proc: (*bool, optional*)</span>
<span class="sd">            Whether to use multiprocessing. If True, uses self.ncore processes.</span>
<span class="sd">            Note that multiprocessing can sometimes be slower due to overhead.</span>
<span class="sd">            Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param save_iter: (*int or None, optional*)</span>
<span class="sd">            If provided, saves the sampler state every save_iter iterations to allow</span>
<span class="sd">            for checkpointing and resuming long runs. Saves to </span>
<span class="sd">            &#39;{savedir}/dynesty_sampler_{like_fn_name}.pkl&#39;. Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param prior_transform_comment: (*str or None, optional*)</span>
<span class="sd">            Comment describing the prior transform for logging purposes. If None</span>
<span class="sd">            and prior_transform is provided, attempts to extract function name.</span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        Attributes Set</span>
<span class="sd">        --------------</span>
<span class="sd">        res : dynesty.results.Results</span>
<span class="sd">            Complete dynesty results object containing samples, weights, evidence, etc.</span>
<span class="sd">        dynesty_samples : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Resampled posterior samples with equal weights</span>
<span class="sd">        dynesty_samples_true : ndarray of shape (nsamples, ndim)</span>
<span class="sd">            Posterior samples when using true likelihood (like_fn=&quot;true&quot;)</span>
<span class="sd">        dynesty_samples_surrogate : ndarray of shape (nsamples, ndim)  </span>
<span class="sd">            Posterior samples when using surrogate likelihood</span>
<span class="sd">        dynesty_run : bool</span>
<span class="sd">            Flag indicating dynesty has been successfully run</span>
<span class="sd">        dynesty_runtime : float</span>
<span class="sd">            Wall-clock time taken for dynesty sampling in seconds</span>
<span class="sd">        like_fn_name : str</span>
<span class="sd">            Name of likelihood function used (&quot;true&quot;, &quot;surrogate&quot;, or &quot;custom&quot;)</span>
<span class="sd">        prior_transform_comment : str</span>
<span class="sd">            Description of prior transform used</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Dynesty is particularly well-suited for:</span>
<span class="sd">        - Computing Bayesian evidence for model comparison</span>
<span class="sd">        - Exploring multi-modal posteriors</span>
<span class="sd">        - Providing robust posterior sampling without tuning</span>
<span class="sd">        </span>
<span class="sd">        The default settings prioritize posterior sampling over evidence estimation</span>
<span class="sd">        by setting pfrac=1.0, which focuses computational effort on high-likelihood</span>
<span class="sd">        regions rather than exploring the full prior volume.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Sample surrogate model with default settings:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty()</span>
<span class="sd">        </span>
<span class="sd">        Sample true likelihood with more live points:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(like_fn=&quot;true&quot;, sampler_kwargs={&#39;nlive&#39;: 1000})</span>
<span class="sd">        </span>
<span class="sd">        Use custom prior with bounds [-5, 5] for each parameter:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; def my_prior(u):</span>
<span class="sd">        ...     return 10*u - 5  # maps [0,1] to [-5,5]</span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(prior_transform=my_prior)</span>
<span class="sd">        </span>
<span class="sd">        Run with checkpointing every 1000 iterations:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty(save_iter=1000, run_kwargs={&#39;maxiter&#39;: 50000})</span>
<span class="sd">        </span>
<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Dynesty documentation: https://dynesty.readthedocs.io/</span>
<span class="sd">        Speagle (2020): &quot;dynesty: a dynamic nested sampling package for estimating</span>
<span class="sd">        Bayesian posteriors and evidences&quot;, MNRAS, 493, 3132-3158</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedSampler</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">DynamicNestedSampler</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">dynesty</span><span class="w"> </span><span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">dyfunc</span>
        
        <span class="c1"># Determine likelihood function and name</span>
        <span class="k">if</span> <span class="n">like_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">like_fn</span><span class="p">):</span>
            <span class="c1"># like_fn is a function/callable</span>
            <span class="k">if</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Custom function provided</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with user-provided likelihood function.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;custom&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="n">like_fn</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">like_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># like_fn is a string</span>
            <span class="n">like_fn_lower</span> <span class="o">=</span> <span class="n">like_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">like_fn_lower</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;surrogate&quot;</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">,</span> <span class="s2">&quot;surrogate_log_likelihood&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.surrogate_log_likelihood surrogate model as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;surrogate&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_log_likelihood</span>
            <span class="k">elif</span> <span class="n">like_fn_lower</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="s2">&quot;true_log_likelihood&quot;</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing dynesty with self.true_log_likelihood as likelihood.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown string identifier for like_fn: &#39;</span><span class="si">{</span><span class="n">like_fn</span><span class="si">}</span><span class="s2">&#39;. &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;Valid options: &#39;surrogate&#39;, &#39;true&#39;, &#39;gp&#39;, &#39;surrogate_log_likelihood&#39;, &#39;true_log_likelihood&#39;&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;like_fn must be None, a string, or a callable function. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Received type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">like_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="c1"># set up prior transform</span>
        <span class="k">if</span> <span class="n">prior_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">prior_transform_uniform</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>

            <span class="c1"># Comment for output log file</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span>  <span class="sa">f</span><span class="s2">&quot;Default uniform prior transform. </span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: ut.prior_transform_uniform</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">with bounds </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span> <span class="o">=</span> <span class="n">prior_transform</span>

            <span class="c1"># Comment for output log file</span>
            <span class="k">if</span> <span class="n">prior_transform_comment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;User defined prior transform.&quot;</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Prior function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">+=</span> <span class="s2">&quot;Prior function: unrecorded&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform_comment</span> <span class="o">=</span> <span class="n">prior_transform_comment</span>

        <span class="c1"># start timing dynesty</span>
        <span class="n">dynesty_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="c1"># set up sampler kwargs</span>
        <span class="n">default_sampler_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;bound&quot;</span><span class="p">:</span> <span class="s2">&quot;multi&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;nlive&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                  <span class="s2">&quot;sample&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">}</span>
        
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">default_sampler_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sampler_kwargs</span><span class="p">:</span>
                <span class="n">sampler_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_sampler_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        
        <span class="c1"># set up multiprocessing pool</span>
        <span class="c1"># default to false. multiprocessing usually slower for some reason</span>
        <span class="k">if</span> <span class="n">multi_proc</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span>
            <span class="n">pool</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
            <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pool</span>
            <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;queue_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
            <span class="n">dynesty_ncore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">dynesty_ncore</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># initialize our nested sampler</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;dynamic&quot;</span><span class="p">:</span>
            <span class="n">dsampler</span> <span class="o">=</span> <span class="n">DynamicNestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                            <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                            <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                            <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized dynesty DynamicNestedSampler.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span><span class="p">:</span>
            <span class="n">dsampler</span> <span class="o">=</span> <span class="n">NestedSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn</span><span class="p">,</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">prior_transform</span><span class="p">,</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                     <span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialized dynesty NestedSampler.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mode </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> is not a valid option. Choose &#39;dynamic&#39; or &#39;static&#39;.&quot;</span><span class="p">)</span>
        
        <span class="c1"># set up run kwargs. default: 100% weight on posterior, 0% evidence</span>
        <span class="n">default_run_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;wt_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;pfrac&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span>
                              <span class="s2">&quot;stop_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;pfrac&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span>
                              <span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">),</span>
                              <span class="s2">&quot;dlogz_init&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">default_run_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">run_kwargs</span><span class="p">:</span>
                <span class="n">run_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_run_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running dynesty with </span><span class="si">{</span><span class="n">sampler_kwargs</span><span class="p">[</span><span class="s1">&#39;nlive&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> live points on </span><span class="si">{</span><span class="n">dynesty_ncore</span><span class="si">}</span><span class="s2"> cores...&quot;</span><span class="p">)</span>

        <span class="c1"># Pickle sampler?</span>
        <span class="k">if</span> <span class="n">save_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">last_iter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">run_sampler</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">dsampler</span><span class="o">.</span><span class="n">run_nested</span><span class="p">(</span><span class="n">maxiter</span><span class="o">=</span><span class="n">save_iter</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">res</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span>

                <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;dynesty_sampler_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span><span class="p">)</span>

                <span class="c1"># pickle dynesty sampler object</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Caching model to </span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>        
                    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dsampler</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

                <span class="c1"># check if converged (i.e. hasn&#39;t run for more iterations)</span>
                <span class="k">if</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">niter</span> <span class="o">&gt;</span> <span class="n">last_iter</span><span class="p">:</span>
                    <span class="n">last_iter</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">niter</span>
                    <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">run_sampler</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dsampler</span><span class="o">.</span><span class="n">run_nested</span><span class="p">(</span><span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">res</span> <span class="o">=</span> <span class="n">dsampler</span><span class="o">.</span><span class="n">results</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res</span><span class="o">.</span><span class="n">samples</span>  <span class="c1"># samples</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">res</span><span class="o">.</span><span class="n">logwt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">res</span><span class="o">.</span><span class="n">logz</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Resample weighted samples.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span> <span class="o">=</span> <span class="n">dyfunc</span><span class="o">.</span><span class="n">resample_equal</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;surrogate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples_surrogate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span>  

        <span class="c1"># record that dynesty has been run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_run</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="c1"># record dynesty runtime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">dynesty_t0</span>

        <span class="c1"># close pool</span>
        <span class="k">if</span> <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampler_kwargs</span><span class="p">[</span><span class="s2">&quot;pool&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/dynesty_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">.npz&quot;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/dynesty_samples_final_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">like_fn_name</span><span class="si">}</span><span class="s2">_iter_</span><span class="si">{</span><span class="n">current_iter</span><span class="si">}</span><span class="s2">.npz&quot;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="p">)</span></div>



<div class="viewcode-block" id="SurrogateModel.plot">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.plot">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plots</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cb_rng</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate diagnostic plots for training progress, GP performance, and MCMC results.</span>
<span class="sd">        </span>
<span class="sd">        This method creates various diagnostic plots to assess the quality of the surrogate</span>
<span class="sd">        model training, GP hyperparameter optimization, and MCMC sampling results. Plots</span>
<span class="sd">        are automatically saved to the model&#39;s save directory.</span>
<span class="sd">        </span>
<span class="sd">        :param plots: (*list of str, optional*)</span>
<span class="sd">            List of plot types to generate. Each plot requires specific data to be available</span>
<span class="sd">            (e.g., &#39;emcee_corner&#39; requires run_emcee() to have been called first). If None,</span>
<span class="sd">            no plots are generated. Available options:</span>
<span class="sd">            </span>
<span class="sd">            **Training diagnostics:**</span>
<span class="sd">            - &#39;test_mse&#39;: Mean squared error vs training iteration</span>
<span class="sd">            - &#39;test_scaled_mse&#39;: Scaled MSE vs training iteration  </span>
<span class="sd">            - &#39;test_log_mse&#39;: Log-scale MSE vs training iteration</span>
<span class="sd">            - &#39;gp_hyperparameters&#39;: GP hyperparameter evolution during training</span>
<span class="sd">            - &#39;gp_train_time&#39;: GP training time vs iteration</span>
<span class="sd">            - &#39;gp_train_corner&#39;: Corner plot of final training samples</span>
<span class="sd">            - &#39;gp_train_scatter&#39;: Scatter plot of training samples vs predictions</span>
<span class="sd">            </span>
<span class="sd">            **GP visualization (2D only):**</span>
<span class="sd">            - &#39;gp_fit_2D&#39;: 2D contour plot of GP surrogate surface</span>
<span class="sd">            </span>
<span class="sd">            **MCMC diagnostics:**</span>
<span class="sd">            - &#39;emcee_corner&#39;: Corner plot of emcee posterior samples</span>
<span class="sd">            - &#39;emcee_walkers&#39;: Walker trajectories for emcee chains</span>
<span class="sd">            - &#39;dynesty_corner&#39;: Corner plot of dynesty posterior samples  </span>
<span class="sd">            - &#39;dynesty_corner_kde&#39;: KDE version of dynesty corner plot</span>
<span class="sd">            - &#39;dynesty_traceplot&#39;: Trace plot of dynesty sampling</span>
<span class="sd">            - &#39;dynesty_runplot&#39;: Dynesty convergence diagnostics</span>
<span class="sd">            </span>
<span class="sd">            **Comparison plots:**</span>
<span class="sd">            - &#39;mcmc_comparison&#39;: Compare emcee and dynesty posteriors</span>
<span class="sd">            </span>
<span class="sd">            **Convenience options:**</span>
<span class="sd">            - &#39;gp_all&#39;: Generate all available GP training plots</span>
<span class="sd">            </span>
<span class="sd">            Default is None.</span>
<span class="sd">            </span>
<span class="sd">        :param show: (*bool, optional*)</span>
<span class="sd">            Whether to display plots interactively in addition to saving them.</span>
<span class="sd">            If False, plots are only saved to disk. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :param cb_rng: (*list of [float, float], optional*)</span>
<span class="sd">            Colorbar range for 2D contour plots as [vmin, vmax]. If [None, None],</span>
<span class="sd">            uses automatic range determination. Only applies to plots with colorbars</span>
<span class="sd">            like &#39;gp_fit_2D&#39;. Default is [None, None].</span>
<span class="sd">            </span>
<span class="sd">        :param log_scale: (*bool, optional*)</span>
<span class="sd">            Whether to use logarithmic color scale for 2D contour plots. If True,</span>
<span class="sd">            applies matplotlib.colors.LogNorm to the colorbar. Only applies to</span>
<span class="sd">            plots with colorbars. Default is False.</span>
<span class="sd">            </span>
<span class="sd">        :returns: *None or matplotlib.figure.Figure*</span>
<span class="sd">            Some individual plots may return figure objects for further customization.</span>
<span class="sd">            </span>
<span class="sd">        :raises NameError:</span>
<span class="sd">            If required data for a requested plot is not available (e.g., requesting</span>
<span class="sd">            &#39;emcee_corner&#39; before running run_emcee()).</span>
<span class="sd">        :raises AttributeError:</span>
<span class="sd">            If the model has not been properly initialized or trained.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Plots are automatically saved to the model&#39;s save directory (self.savedir)</span>
<span class="sd">        with descriptive filenames. The save directory is created if it doesn&#39;t exist.</span>
<span class="sd">        </span>
<span class="sd">        Training diagnostic plots help assess:</span>
<span class="sd">        - Convergence of active learning process</span>
<span class="sd">        - Quality of GP hyperparameter optimization  </span>
<span class="sd">        - Efficiency of training sample selection</span>
<span class="sd">        </span>
<span class="sd">        MCMC diagnostic plots help assess:</span>
<span class="sd">        - Posterior sampling convergence</span>
<span class="sd">        - Chain mixing and autocorrelation</span>
<span class="sd">        - Comparison between different sampling methods</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Generate all GP training plots:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;gp_all&#39;])</span>
<span class="sd">        </span>
<span class="sd">        Create MCMC comparison plots:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.run_emcee()</span>
<span class="sd">        &gt;&gt;&gt; sm.run_dynesty()</span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;emcee_corner&#39;, &#39;dynesty_corner&#39;, &#39;mcmc_comparison&#39;])</span>
<span class="sd">        </span>
<span class="sd">        Generate 2D GP visualization with custom colorbar:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;gp_fit_2D&#39;], cb_rng=[-10, 0], log_scale=True)</span>
<span class="sd">        </span>
<span class="sd">        Show plots interactively:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; sm.plot(plots=[&#39;test_mse&#39;, &#39;gp_hyperparameters&#39;], show=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># ================================</span>
        <span class="c1"># GP training plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;gp_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">gp_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;test_log_mse&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_hyperparam&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_timing&quot;</span><span class="p">,</span> <span class="s2">&quot;gp_train_scatter&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">gp_plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;gp_fit_2D&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">gp_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>
                
        <span class="c1"># GP mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_mse_vs_iteration.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># MSE </span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Mean Squared Error&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_mse.&quot;</span><span class="p">)</span>
            
        <span class="c1"># GP scaled mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_scaled_mse_vs_iteration.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the scaled gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># Scaled MSE</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_scaled_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Mean Squared Error / Variance&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_scaled_mse.&quot;</span><span class="p">)</span>
            
        <span class="c1"># GP mean squared error vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;test_log_mse&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            
            <span class="n">savename</span> <span class="o">=</span> <span class="s2">&quot;gp_mse_vs_iteration_log.png&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plotting the log gp mean squared error with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntest</span><span class="si">}</span><span class="s2"> test samples...&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">savename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">iarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span>
                
                <span class="c1"># Log MSE</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_error_vs_iteration</span><span class="p">(</span><span class="n">iarray</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">],</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;Log(Mean Squared Error)&quot;</span><span class="p">,</span>
                                            <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span>
                                            <span class="n">savedir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="p">,</span>
                                            <span class="n">savename</span><span class="o">=</span><span class="n">savename</span><span class="p">,</span>
                                            <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting test_log_mse.&quot;</span><span class="p">)</span>

        <span class="c1"># GP hyperparameters vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp hyperparameters...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_hyperparam_vs_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting gp_hyperparameters.&quot;</span><span class="p">)</span>

        <span class="c1"># GP training time vs iteration</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_time&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;training_results&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp timing...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_train_time_vs_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run active_train before plotting gp_timing.&quot;</span><span class="p">)</span>

        <span class="c1"># N-D scatterplots and histograms colored by function value</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting training sample corner plot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_lnp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_train_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># N-D scatterplots and histograms</span>
        <span class="k">if</span> <span class="s2">&quot;gp_train_scatter&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting training sample corner plot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_train_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># GP fit (only for 2D functions)</span>
        <span class="k">if</span> <span class="s2">&quot;gp_fit_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting gp fit 2D...&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_gp_fit_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2"> surrogate&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> 
                                              <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta must be 2D to use gp_fit_2D!&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and/or active_train before plotting gp_fit_2D.&quot;</span><span class="p">)</span>

        <span class="c1"># Objective function contour plot</span>
        <span class="k">if</span> <span class="s2">&quot;obj_fn_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_theta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_y&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gp&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting objective function contours 2D...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_utility_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run init_train and init_gp before plotting obj_fn_2D.&quot;</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="s2">&quot;true_fn_2D&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting true function contours 2D...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_true_fit_2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cb_rng</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_scale</span><span class="o">=</span><span class="n">log_scale</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta must be 2D to use true_fn_2D!&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># emcee plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;emcee_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">emcee_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;emcee_corner&quot;</span><span class="p">,</span> <span class="s2">&quot;emcee_walkers&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">emcee_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>

        <span class="c1"># emcee posterior samples</span>
        <span class="k">if</span> <span class="s2">&quot;emcee_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee posterior...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emcee_samples</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;emcee_&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee before plotting emcee_corner.&quot;</span><span class="p">)</span>

        <span class="c1"># emcee walkers</span>
        <span class="k">if</span> <span class="s2">&quot;emcee_walkers&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee walkers...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_emcee_walkers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee before plotting emcee_walkers.&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># dynesty plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_all&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="n">dynesty_plots</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dynesty_corner&quot;</span><span class="p">,</span> <span class="s2">&quot;dynesty_corner_kde&quot;</span><span class="p">,</span> 
                             <span class="s2">&quot;dynesty_traceplot&quot;</span><span class="p">,</span> <span class="s2">&quot;dynesty_runplot&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pl</span> <span class="ow">in</span> <span class="n">dynesty_plots</span><span class="p">:</span>
                <span class="n">plots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pl</span><span class="p">)</span>

        <span class="c1"># dynesty posterior samples</span>
        <span class="k">if</span> <span class="s2">&quot;dynesty_corner&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty posterior...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynesty_samples</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="s2">&quot;dynesty_&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">);</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_corner.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_corner_kde&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>  
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;dynesty_samples&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty posterior kde...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_corner_kde</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_corner.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_traceplot&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty traceplot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_dynesty_traceplot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_traceplot.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;dynesty_runplot&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting dynesty runplot...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_dynesty_runplot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_dynesty before plotting dynesty_runplot.&quot;</span><span class="p">)</span>

        <span class="c1"># ================================</span>
        <span class="c1"># MCMC comparison plots</span>
        <span class="c1"># ================================</span>

        <span class="k">if</span> <span class="s2">&quot;mcmc_comparison&quot;</span> <span class="ow">in</span> <span class="n">plots</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;emcee_samples&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;res&quot;</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plotting emcee vs dynesty posterior comparison...&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">vis</span><span class="o">.</span><span class="n">plot_emcee_dynesty_comparison</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="n">show</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;Must run run_emcee and run_dynesty before plotting emcee_comparison.&quot;</span><span class="p">)</span></div>

    

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_chain_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_id</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;lbfgsb&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                          <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Worker function to run a single active learning chain.</span>
<span class="sd">        This function is designed to be pickled for multiprocessing.</span>
<span class="sd">        </span>
<span class="sd">        :param chain_id: (*int*)</span>
<span class="sd">            Identifier for this chain</span>
<span class="sd">        Other parameters same as active_train()</span>
<span class="sd">        </span>
<span class="sd">        :returns: *tuple or None*</span>
<span class="sd">            (new_theta, new_y, training_results) if successful, None if failed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Create a copy of the current model for this chain</span>
            <span class="n">chain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chain_copy</span><span class="p">(</span><span class="n">chain_id</span><span class="o">=</span><span class="n">chain_id</span><span class="p">)</span>
            
            <span class="c1"># Store initial state</span>
            <span class="n">initial_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">initial_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            
            <span class="c1"># Run active learning on this chain</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span>
                              <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
                              <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span>
                              <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="n">show_progress</span><span class="p">)</span>
            
            <span class="c1"># Extract only the new samples (excluding initial training data)</span>
            <span class="n">initial_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">)</span>
            <span class="n">new_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            <span class="n">new_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            
            <span class="k">return</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>


<div class="viewcode-block" id="SurrogateModel.active_train_parallel">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.active_train_parallel">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">active_train_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nchains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                             <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span><span class="p">,</span> <span class="n">nopt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                             <span class="n">n_attempts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{},</span> 
                             <span class="n">combine_frequency</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run multiple active learning chains in parallel and combine their training samples.</span>
<span class="sd">        </span>
<span class="sd">        :param niter: (*int, optional*) </span>
<span class="sd">            Number of iterations per chain. Default 100.</span>
<span class="sd">            </span>
<span class="sd">        :param nchains: (*int, optional*) </span>
<span class="sd">            Number of parallel chains to run. Default 4.</span>
<span class="sd">            </span>
<span class="sd">        :param algorithm: (*str, optional*) </span>
<span class="sd">            Active learning algorithm. Default &quot;bape&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param gp_opt_freq: (*int, optional*)</span>
<span class="sd">            Frequency of GP hyperparameter optimization. Default 20.</span>
<span class="sd">            </span>
<span class="sd">        :param save_progress: (*bool, optional*)</span>
<span class="sd">            Whether to save progress during training. Default False.</span>
<span class="sd">            </span>
<span class="sd">        :param obj_opt_method: (*str, optional*)</span>
<span class="sd">            Optimization method for acquisition function. Default &quot;nelder-mead&quot;.</span>
<span class="sd">            </span>
<span class="sd">        :param nopt: (*int, optional*)</span>
<span class="sd">            Number of restarts for acquisition optimization. Default 1.</span>
<span class="sd">            </span>
<span class="sd">        :param n_attempts: (*int, optional*)</span>
<span class="sd">            Number of attempts for optimization. Default 5.</span>
<span class="sd">            </span>
<span class="sd">        :param use_grad_opt: (*bool, optional*)</span>
<span class="sd">            Whether to use gradient-based optimization. Default True.</span>
<span class="sd">            </span>
<span class="sd">        :param optimizer_kwargs: (*dict, optional*)</span>
<span class="sd">            Additional optimizer kwargs. Default {}.</span>
<span class="sd">            </span>
<span class="sd">        :param combine_frequency: (*int, optional*)</span>
<span class="sd">            How often to combine chains (in iterations). If None, only combines at the end.</span>
<span class="sd">            </span>
<span class="sd">        :param show_progress: (*bool, optional*) </span>
<span class="sd">            Whether to display progress bar during parallel chain execution. When True, shows</span>
<span class="sd">            a progress bar tracking chain completion. Individual active_train progress bars</span>
<span class="sd">            are disabled to avoid clutter. Default is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Set algorithm attribute to avoid save issues</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        
        <span class="c1"># Initialize training results if not present</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;training_results&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                   <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                   <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                   <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[]}</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> parallel active learning chains for </span><span class="si">{</span><span class="n">niter</span><span class="si">}</span><span class="s2"> iterations each...&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Algorithm: </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2">, Method: </span><span class="si">{</span><span class="n">obj_opt_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Store original training data</span>
        <span class="n">original_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">original_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">original_ntrain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span>
        
        <span class="c1"># Track results from all chains</span>
        <span class="n">all_new_theta</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_new_y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">chain_results</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Run chains in parallel using multiprocessing</span>
        <span class="k">if</span> <span class="n">nchains</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> chains in parallel using </span><span class="si">{</span><span class="nb">min</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">ncore</span><span class="p">)</span><span class="si">}</span><span class="s2"> processes...&quot;</span><span class="p">)</span>
            
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Create process-safe copies of the chains first </span>
                <span class="n">chain_copies</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                    <span class="n">chain_copy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chain_copy</span><span class="p">(</span><span class="n">chain_id</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
                    <span class="n">chain_copies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chain_copy</span><span class="p">)</span>
                
                <span class="c1"># Use threading for now instead of multiprocessing to avoid pickling issues</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">queue</span>
                
                <span class="n">results_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
                <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">completed_chains</span> <span class="o">=</span> <span class="mi">0</span>
                
                <span class="c1"># Progress tracking for parallel execution</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running parallel chains&quot;</span><span class="p">)</span>
                
                <span class="k">def</span><span class="w"> </span><span class="nf">run_chain_thread</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">chain_id</span><span class="p">,</span> <span class="n">results_q</span><span class="p">):</span>
                    <span class="k">nonlocal</span> <span class="n">completed_chains</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="c1"># Store initial state</span>
                        <span class="n">initial_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                        <span class="n">initial_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                        
                        <span class="c1"># Run active learning on this chain (disable individual progress bars for parallel runs)</span>
                        <span class="n">chain</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span>
                                          <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
                                          <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span>
                                          <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                        
                        <span class="c1"># Extract only the new samples (excluding initial training data)</span>
                        <span class="n">initial_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">)</span>
                        <span class="n">new_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
                        <span class="n">new_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
                        
                        <span class="n">results_q</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">chain_id</span><span class="p">,</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span><span class="p">))</span>
                        
                        <span class="c1"># Update progress bar</span>
                        <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                            <span class="n">completed_chains</span> <span class="o">+=</span> <span class="mi">1</span>
                            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        
                    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="n">results_q</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">chain_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                        
                        <span class="c1"># Update progress bar even for failed chains</span>
                        <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                            <span class="n">completed_chains</span> <span class="o">+=</span> <span class="mi">1</span>
                            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="c1"># Start all threads</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chain</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chain_copies</span><span class="p">):</span>
                    <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">run_chain_thread</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">results_queue</span><span class="p">))</span>
                    <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
                
                <span class="c1"># Wait for all threads to complete</span>
                <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
                    <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
                
                <span class="c1"># Close progress bar</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">progress_bar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                
                <span class="c1"># Collect results from all threads</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">results_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                        <span class="n">chain_id</span><span class="p">,</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">training_results</span> <span class="o">=</span> <span class="n">result</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_y</span><span class="p">)</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_results</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">chain_id</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2"> failed&quot;</span><span class="p">)</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]))</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
                        
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parallel execution failed (</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">), falling back to sequential...&quot;</span><span class="p">)</span>
                <span class="c1"># Clear any partial results</span>
                <span class="n">all_new_theta</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">all_new_y</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">chain_results</span> <span class="o">=</span> <span class="p">[]</span>
                
                <span class="c1"># Fallback to sequential execution</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">fallback_progress</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running chains sequentially (fallback)&quot;</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running chain </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                    
                    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_chain_worker</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> 
                                                  <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span>
                                                  <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> 
                                                  <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span> 
                                                  <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    
                    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">training_results</span> <span class="o">=</span> <span class="n">result</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_y</span><span class="p">)</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_results</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> failed&quot;</span><span class="p">)</span>
                        <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
                        <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]))</span>
                        <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
                    
                    <span class="c1"># Update fallback progress bar</span>
                    <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                        <span class="n">fallback_progress</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="c1"># Close fallback progress bar</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">fallback_progress</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                    
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fallback to sequential execution</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running chains sequentially...&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                <span class="n">sequential_progress</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running chains sequentially&quot;</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running chain </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_chain_worker</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> 
                                              <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span>
                                              <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> 
                                              <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span> 
                                              <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">training_results</span> <span class="o">=</span> <span class="n">result</span>
                    <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_theta</span><span class="p">)</span>
                    <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_y</span><span class="p">)</span>
                    <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_results</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> failed&quot;</span><span class="p">)</span>
                    <span class="n">all_new_theta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
                    <span class="n">all_new_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]))</span>
                    <span class="n">chain_results</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
                
                <span class="c1"># Update sequential progress bar</span>
                <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                    <span class="n">sequential_progress</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Close sequential progress bar</span>
            <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                <span class="n">sequential_progress</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        
        <span class="c1"># Combine all new training samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Combining training samples from all chains...&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_combine_chain_results</span><span class="p">(</span><span class="n">all_new_theta</span><span class="p">,</span> <span class="n">all_new_y</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">total_new_samples</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully combined </span><span class="si">{</span><span class="n">total_new_samples</span><span class="si">}</span><span class="s2"> new training samples from </span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> chains&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total training samples: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span><span class="si">}</span><span class="s2"> (was </span><span class="si">{</span><span class="n">original_ntrain</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        
        <span class="c1"># Final GP optimization with all combined data</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Performing final GP optimization with combined dataset...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_gp</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">()</span></div>

    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_chain_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_id</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a copy of the current SurrogateModel for a parallel chain.&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
        
        <span class="c1"># Create a deep copy of the current model</span>
        <span class="n">chain</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        
        <span class="c1"># Modify savedir to avoid conflicts</span>
        <span class="n">chain</span><span class="o">.</span><span class="n">savedir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">savedir</span><span class="si">}</span><span class="s2">/chain_</span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">savedir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">savedir</span><span class="p">)</span>
        
        <span class="c1"># Reset training results for this chain</span>
        <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span> 
                                 <span class="s2">&quot;gp_hyperparameters&quot;</span> <span class="p">:</span> <span class="p">[],</span>  
                                 <span class="s2">&quot;training_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;training_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;test_scaled_mse&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_kl_divergence&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_train_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;obj_fn_opt_time&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparameter_opt_iteration&quot;</span> <span class="p">:</span> <span class="p">[],</span>
                                 <span class="s2">&quot;gp_hyperparam_opt_time&quot;</span> <span class="p">:</span> <span class="p">[]}</span>
        
        <span class="k">return</span> <span class="n">chain</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_run_single_chain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run a single active learning chain.&quot;&quot;&quot;</span>
        <span class="p">(</span><span class="n">chain_id</span><span class="p">,</span> <span class="n">chain</span><span class="p">,</span> <span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="p">,</span> 
         <span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="n">combine_frequency</span><span class="p">)</span> <span class="o">=</span> <span class="n">args</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Store initial state</span>
            <span class="n">initial_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">initial_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            
            <span class="c1"># Run active learning on this chain</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">gp_opt_freq</span><span class="o">=</span><span class="n">gp_opt_freq</span><span class="p">,</span>
                              <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
                              <span class="n">nopt</span><span class="o">=</span><span class="n">nopt</span><span class="p">,</span> <span class="n">n_attempts</span><span class="o">=</span><span class="n">n_attempts</span><span class="p">,</span> <span class="n">use_grad_opt</span><span class="o">=</span><span class="n">use_grad_opt</span><span class="p">,</span>
                              <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
            
            <span class="c1"># Extract only the new samples (excluding initial training data)</span>
            <span class="n">initial_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_theta</span><span class="p">)</span>
            <span class="n">new_theta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            <span class="n">new_y</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">initial_len</span><span class="p">:]</span>
            
            <span class="k">return</span> <span class="n">chain_id</span><span class="p">,</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">chain</span><span class="o">.</span><span class="n">training_results</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chain </span><span class="si">{</span><span class="n">chain_id</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">chain_id</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]),</span> <span class="p">{}</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_combine_chain_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">all_new_theta</span><span class="p">,</span> <span class="n">all_new_y</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Combine results from all chains into the main model.&quot;&quot;&quot;</span>
        
        <span class="c1"># Concatenate all new training samples</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_new_theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span><span class="p">):</span>
            <span class="n">combined_new_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">theta</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">all_new_theta</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
            <span class="n">combined_new_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">all_new_y</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
            
            <span class="c1"># Add to main model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">combined_new_theta</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">combined_new_y</span><span class="p">])</span>
            
            <span class="c1"># Update counters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nactive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span>
            
            <span class="c1"># Refit GP with all combined data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_gp</span><span class="p">(</span><span class="n">_theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>
        
        <span class="c1"># Combine training results from all chains</span>
        <span class="k">if</span> <span class="n">chain_results</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_merge_training_results</span><span class="p">(</span><span class="n">chain_results</span><span class="p">)</span>
        
        <span class="c1"># Ensure training results have at least one entry to avoid index errors</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;test_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;test_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;training_scaled_mse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
    
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_merge_training_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chain_results</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Merge training results from multiple chains.&quot;&quot;&quot;</span>
        
        <span class="c1"># Get the starting iteration number</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">start_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_iter</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s2">&quot;iteration&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="c1"># Merge results from all chains</span>
        <span class="k">for</span> <span class="n">chain_result</span> <span class="ow">in</span> <span class="n">chain_results</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">chain_result</span><span class="p">:</span>  <span class="c1"># Skip empty results</span>
                <span class="k">continue</span>
                
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">chain_result</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;iteration&quot;</span><span class="p">:</span>
                        <span class="c1"># Adjust iteration numbers to be sequential</span>
                        <span class="n">adjusted_iters</span> <span class="o">=</span> <span class="p">[</span><span class="n">iter_num</span> <span class="o">+</span> <span class="n">start_iter</span> <span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="n">chain_result</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">adjusted_iters</span><span class="p">)</span>
                        <span class="n">start_iter</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">adjusted_iters</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">adjusted_iters</span> <span class="k">else</span> <span class="n">start_iter</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chain_result</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>


<div class="viewcode-block" id="SurrogateModel.get_chain_diversity_metrics">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.get_chain_diversity_metrics">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_chain_diversity_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate diversity metrics for the combined training samples.</span>
<span class="sd">        Useful for assessing the effectiveness of parallel chains.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_theta&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>
        
        <span class="c1"># Get only the active learning samples (exclude initial training)</span>
        <span class="n">active_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:]</span>
        <span class="n">active_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">:]</span>
        
        <span class="c1"># Calculate diversity metrics</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="c1"># Parameter space coverage</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
            <span class="n">param_range</span> <span class="o">=</span> <span class="n">active_theta</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">active_theta</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
            <span class="n">bound_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;param_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">_coverage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_range</span> <span class="o">/</span> <span class="n">bound_range</span>
        
        <span class="c1"># Function value diversity</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;function_value_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">active_y</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;function_value_range&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">active_y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">active_y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        
        <span class="c1"># Average pairwise distance in parameter space</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">scipy.spatial.distance</span><span class="w"> </span><span class="kn">import</span> <span class="n">pdist</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">active_theta</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;avg_pairwise_distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;min_pairwise_distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">metrics</span></div>



<div class="viewcode-block" id="SurrogateModel.compare_parallel_vs_sequential">
<a class="viewcode-back" href="../../alabi.html#alabi.core.SurrogateModel.compare_parallel_vs_sequential">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">compare_parallel_vs_sequential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">nchains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;bape&quot;</span><span class="p">,</span> 
                                      <span class="n">obj_opt_method</span><span class="o">=</span><span class="s2">&quot;nelder-mead&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compare parallel vs sequential active learning performance.</span>
<span class="sd">        </span>
<span class="sd">        :param niter: Number of iterations for comparison</span>
<span class="sd">        :param nchains: Number of chains for parallel version</span>
<span class="sd">        :param algorithm: Active learning algorithm to use</span>
<span class="sd">        :param obj_opt_method: Optimization method</span>
<span class="sd">        :param kwargs: Additional arguments passed to active_train methods</span>
<span class="sd">        </span>
<span class="sd">        :returns: Dictionary with comparison results</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparing parallel vs sequential active learning...&quot;</span><span class="p">)</span>
        
        <span class="c1"># Create copies for fair comparison</span>
        <span class="n">model_sequential</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">model_parallel</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        
        <span class="c1"># Run sequential active learning</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running sequential active learning (</span><span class="si">{</span><span class="n">niter</span><span class="si">}</span><span class="s2"> iterations)...&quot;</span><span class="p">)</span>
        
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">model_sequential</span><span class="o">.</span><span class="n">active_train</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> 
                                    <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">sequential_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        
        <span class="c1"># Run parallel active learning</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running parallel active learning (</span><span class="si">{</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> chains × </span><span class="si">{</span><span class="n">niter</span><span class="o">//</span><span class="n">nchains</span><span class="si">}</span><span class="s2"> iterations)...&quot;</span><span class="p">)</span>
        
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">model_parallel</span><span class="o">.</span><span class="n">active_train_parallel</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="o">//</span><span class="n">nchains</span><span class="p">,</span> <span class="n">nchains</span><span class="o">=</span><span class="n">nchains</span><span class="p">,</span> 
                                            <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">obj_opt_method</span><span class="o">=</span><span class="n">obj_opt_method</span><span class="p">,</span> 
                                            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">parallel_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        
        <span class="c1"># Calculate comparison metrics</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;sequential&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;total_samples&#39;</span><span class="p">:</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ntrain</span><span class="p">,</span>
                <span class="s1">&#39;new_samples&#39;</span><span class="p">:</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">,</span>
                <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="n">sequential_time</span><span class="p">,</span>
                <span class="s1">&#39;final_gp_error&#39;</span><span class="p">:</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s1">&#39;training_scaled_mse&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s1">&#39;training_scaled_mse&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
            <span class="p">},</span>
            <span class="s1">&#39;parallel&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;total_samples&#39;</span><span class="p">:</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">ntrain</span><span class="p">,</span>
                <span class="s1">&#39;new_samples&#39;</span><span class="p">:</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">,</span>
                <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="n">parallel_time</span><span class="p">,</span>
                <span class="s1">&#39;final_gp_error&#39;</span><span class="p">:</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s1">&#39;training_scaled_mse&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">training_results</span><span class="p">[</span><span class="s1">&#39;training_scaled_mse&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                <span class="s1">&#39;diversity_metrics&#39;</span><span class="p">:</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">get_chain_diversity_metrics</span><span class="p">()</span>
            <span class="p">},</span>
            <span class="s1">&#39;speedup&#39;</span><span class="p">:</span> <span class="n">sequential_time</span> <span class="o">/</span> <span class="n">parallel_time</span> <span class="k">if</span> <span class="n">parallel_time</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
            <span class="s1">&#39;efficiency&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">model_parallel</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="n">model_parallel</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">model_sequential</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">-</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">)</span> <span class="k">if</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ntrain</span> <span class="o">&gt;</span> <span class="n">model_sequential</span><span class="o">.</span><span class="n">ninit_train</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="p">}</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Comparison Results:&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sequential: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;sequential&#39;</span><span class="p">][</span><span class="s1">&#39;new_samples&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples in </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;sequential&#39;</span><span class="p">][</span><span class="s1">&#39;runtime&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parallel: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;parallel&#39;</span><span class="p">][</span><span class="s1">&#39;new_samples&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples in </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;parallel&#39;</span><span class="p">][</span><span class="s1">&#39;runtime&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;speedup&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample efficiency: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;efficiency&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span></div>



    <span class="k">def</span><span class="w"> </span><span class="nf">_get_pickleable_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get a pickleable representation of the model state for multiprocessing.</span>
<span class="sd">        </span>
<span class="sd">        :returns: *dict*</span>
<span class="sd">            Dictionary containing all necessary state information</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
        
        <span class="c1"># Create a simplified state dictionary</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_theta</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="s1">&#39;bounds&#39;</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bounds</span><span class="p">),</span>
            <span class="s1">&#39;ndim&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
            <span class="s1">&#39;ninit_train&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ninit_train</span><span class="p">,</span>
            <span class="s1">&#39;ntrain&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntrain</span><span class="p">,</span>
            <span class="s1">&#39;function&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_log_likelihood</span><span class="p">,</span>  <span class="c1"># Use the correct function attribute</span>
            <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="s1">&#39;ncore&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ncore</span>
        <span class="p">}</span>
        
        <span class="k">return</span> <span class="n">state</span></div>

</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Jess Birky
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/notebook-outputs.js?v=9bb603c3"></script>
    </body>
</html>