<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html"><link rel="search" title="Search" href="../../search.html">
        <link rel="canonical" href="https://alabi.jessicabirky.com/_modules/alabi/gp_utils.html">

    <!-- Generated with Sphinx 9.0.4 and Furo 2025.12.19 -->
        <title>alabi.gp_utils - alabi</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0cf789f7" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --font-stack: Roboto Light, sans-serif;
  --font-stack--monospace: Courier, monospace;
  --color-background-secondary: #eff1f6;
  --color-inline-code-background: #eff1f6;
  --color-sidebar-item-background--hover: white;
  --color-brand-primary: #004080;
  --color-brand-content: #0059b3;
  --font-size--small: 0.875rem;
  --font-size--normal: 1rem;
  --font-size--large: 1.125rem;
  --font-size-h1: 2.2rem;
  --font-size-h2: 1.8rem;
  --font-size-h3: 1.5rem;
  --font-size-h4: 1.3rem;
  --font-size-h5: 1.1rem;
  --font-size-h6: 1rem;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">alabi</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  <span class="sidebar-brand-text">alabi</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html#quickstart-example">Quickstart Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../save_reload.html">Saving and Reloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gp_tutorial.html">GP Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mcmc_tutorial.html">MCMC sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_bayesian_optimization.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_hp_settings.html">Automated Hyperparameter Selection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_1d.html">Visualize Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_demo_2d.html">Test 2D Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_line_fit.html">Fit a line to data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_kl_divergence.html">KL Divergence: Gaussian</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot_gaussian_nd.html">Test computational scaling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../trappist_stellar_evolution.html">Stellar Evolution</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../modules.html">alabi</a><input aria-label="Toggle navigation of alabi" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../alabi.html">alabi package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/LICENSE">License</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jbirky/alabi/issues">Issues</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for alabi.gp_utils</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">:py:mod:`gp_utils.py` </span>
<span class="sd">-------------------------------------------------</span>

<span class="sd">Gaussian process utility functions for initializing GPs and optimizing their</span>
<span class="sd">hyperparameters.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">alabi</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">george</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">lognorm</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;configure_gp&quot;</span><span class="p">,</span> 
           <span class="s2">&quot;optimize_gp&quot;</span><span class="p">,</span>
           <span class="s2">&quot;optimize_gp_kfold_cv&quot;</span><span class="p">,</span>
           <span class="s2">&quot;regularization_term&quot;</span><span class="p">,</span>
           <span class="s2">&quot;regularization_gradient&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="regularization_term">
<a class="viewcode-back" href="../../alabi.html#alabi.gp_utils.regularization_term">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">regularization_term</span><span class="p">(</span><span class="n">hparams</span><span class="p">,</span> <span class="n">lengthscale_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the regularization term (negative log prior) from Hvafner 2024 Equation 4.</span>
<span class="sd">    </span>
<span class="sd">    This implements the dimensionality-scaled LogNormal prior:</span>
<span class="sd">    p(ℓ_i | d) = LogNormal(μ_0 + log(√d), σ_0)</span>
<span class="sd">    </span>
<span class="sd">    The regularization term is -log p(ℓ | d) = -Σ log p(ℓ_i | d)</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    mu_0 : float, default=0.0</span>
<span class="sd">        The base location parameter for the 1D LogNormal prior</span>
<span class="sd">    sigma_0 : float, default=1.0</span>
<span class="sd">        The scale parameter for the LogNormal prior</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    float</span>
<span class="sd">        The regularization term (negative log prior)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
    <span class="n">log_lengthscales</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">[</span><span class="n">lengthscale_indices</span><span class="p">]</span>
    
    <span class="c1"># Scaled location parameter: μ = μ_0 + log(√d)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
    
    <span class="c1"># Compute negative log likelihood for each lengthscale</span>
    <span class="c1"># For LogNormal(μ, σ), if X ~ LogNormal(μ, σ), then log(X) ~ Normal(μ, σ)</span>
    <span class="c1"># log p(ℓ_i) = -log(ℓ_i) - log(σ√(2π)) - (log(ℓ_i) - μ)²/(2σ²)</span>
    
    <span class="c1"># Negative log prior for each dimension</span>
    <span class="n">neg_log_prior</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">log_lengthscales</span> <span class="o">+</span>  <span class="c1"># log(ℓ_i) term from the Jacobian</span>
        <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span>  <span class="c1"># normalization constant</span>
        <span class="p">(</span><span class="n">log_lengthscales</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># quadratic term</span>
    <span class="p">)</span>
    
    <span class="c1"># Sum over all dimensions</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">amp_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">neg_log_prior</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reg</span></div>



<div class="viewcode-block" id="regularization_gradient">
<a class="viewcode-back" href="../../alabi.html#alabi.gp_utils.regularization_gradient">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">regularization_gradient</span><span class="p">(</span><span class="n">hparams</span><span class="p">,</span> <span class="n">lengthscale_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the regularization term with respect to lengthscales.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    lengthscales : array-like, shape (d,)</span>
<span class="sd">        The lengthscale parameters for each dimension</span>
<span class="sd">    d : int</span>
<span class="sd">        The dimensionality of the problem</span>
<span class="sd">    mu_0 : float, default=0.0</span>
<span class="sd">        The base location parameter for the 1D LogNormal prior</span>
<span class="sd">    sigma_0 : float, default=1.0</span>
<span class="sd">        The scale parameter for the LogNormal prior</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    ndarray, shape (d,)</span>
<span class="sd">        The gradient of the regularization term with respect to each lengthscale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
    <span class="n">gradient_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
    
    <span class="n">log_lengthscales</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">[</span><span class="n">lengthscale_indices</span><span class="p">]</span>
    <span class="n">lengthscales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_lengthscales</span><span class="p">)</span>
    
    <span class="c1"># Scaled location parameter</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
    
    <span class="c1"># d/dℓ_i [-log p(ℓ_i)] = d/dℓ_i [log(ℓ_i) + C + (log(ℓ_i) - μ)²/(2σ²)]</span>
    <span class="c1">#                       = 1/ℓ_i + (log(ℓ_i) - μ)/(σ² ℓ_i)</span>
    <span class="c1">#                       = [1 + (log(ℓ_i) - μ)/σ²] / ℓ_i</span>

    <span class="n">gradient_vector</span><span class="p">[</span><span class="n">lengthscale_indices</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="p">(</span><span class="n">log_lengthscales</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">lengthscales</span>
    
    <span class="k">return</span> <span class="n">amp_0</span> <span class="o">*</span> <span class="n">gradient_vector</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gp_hyper_prior</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given parameters and data, compute the negative log likelihood of the data</span>
<span class="sd">    under the george Gaussian process.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    p : array</span>
<span class="sd">        GP hyperparameters</span>
<span class="sd">    gp : george.GP</span>
<span class="sd">    y : array</span>
<span class="sd">        data to condition GP on</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    nll : float</span>
<span class="sd">        negative log-likelihood of y under gp</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">gp_hyper_prior</span><span class="p">(</span><span class="n">p</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    
    <span class="c1"># Catch singular matrices</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="n">ll</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">ll</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_grad_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given parameters and data, compute the gradient of the negative log</span>
<span class="sd">    likelihood of the data under the george Gaussian process.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    p : array</span>
<span class="sd">        GP hyperparameters</span>
<span class="sd">    gp : george.GP</span>
<span class="sd">    y : array</span>
<span class="sd">        data to condition GP on</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    gnll : array</span>
<span class="sd">        gradient of the negative log-likelihood of y under gp</span>
<span class="sd">    &quot;&quot;&quot;</span>
        
    <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># Negative gradient of log likelihood</span>
    <span class="n">grad_nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">gp</span><span class="o">.</span><span class="n">grad_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">grad_nll</span>


<div class="viewcode-block" id="configure_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.gp_utils.configure_gp">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">configure_gp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> 
                 <span class="n">fit_amp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fit_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fit_white_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">white_noise</span><span class="o">=-</span><span class="mi">12</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configure and initialize a Gaussian Process with robust error handling.</span>
<span class="sd">    </span>
<span class="sd">    Creates a george.GP object with the specified kernel and configuration options.</span>
<span class="sd">    Includes automatic fixes for common numerical issues such as singular matrices,</span>
<span class="sd">    duplicate points, and poor conditioning.</span>
<span class="sd">    </span>
<span class="sd">    :param theta: (*array-like, shape (n_samples, n_features)*)</span>
<span class="sd">        Training input locations (parameters). Must contain finite values only.</span>
<span class="sd">        </span>
<span class="sd">    :param y: (*array-like, shape (n_samples,)*)</span>
<span class="sd">        Training target values (function evaluations). Must contain finite values only.</span>
<span class="sd">        </span>
<span class="sd">    :param kernel: (*george kernel object*)</span>
<span class="sd">        George kernel object defining the covariance function. Common options include</span>
<span class="sd">        ExpSquaredKernel, Matern32Kernel, Matern52Kernel.</span>
<span class="sd">        </span>
<span class="sd">    :param fit_amp: (*bool, optional, default=True*)</span>
<span class="sd">        Whether to fit the kernel amplitude. If True, scales the kernel by the </span>
<span class="sd">        variance of y to improve conditioning.</span>
<span class="sd">        </span>
<span class="sd">    :param fit_mean: (*bool, optional, default=True*)</span>
<span class="sd">        Whether to fit a constant mean function. If True, initializes mean to </span>
<span class="sd">        median(y) and allows optimization.</span>
<span class="sd">        </span>
<span class="sd">    :param fit_white_noise: (*bool, optional, default=False*)</span>
<span class="sd">        Whether to fit the white noise (nugget) parameter. If True, the noise</span>
<span class="sd">        level will be optimized during hyperparameter tuning.</span>
<span class="sd">        </span>
<span class="sd">    :param white_noise: (*float, optional, default=-12*)</span>
<span class="sd">        Log-scale white noise parameter. Acts as regularization to prevent</span>
<span class="sd">        singular matrices. More negative values = less noise.</span>
<span class="sd">        </span>
<span class="sd">    :param hyperparameters: (*array-like, optional, default=None*)</span>
<span class="sd">        Pre-specified hyperparameters to set. If provided, these values are used</span>
<span class="sd">        instead of the kernel&#39;s default initialization.</span>
<span class="sd">        </span>
<span class="sd">    :returns: *george.GP or None*</span>
<span class="sd">        Configured and computed GP object ready for predictions, or None if</span>
<span class="sd">        configuration failed despite all attempted fixes.</span>
<span class="sd">        </span>
<span class="sd">    :raises ValueError:</span>
<span class="sd">        If theta or y contain non-finite values (NaN or inf).</span>
<span class="sd">        </span>
<span class="sd">    :raises LinAlgError:</span>
<span class="sd">        If GP computation fails due to singular covariance matrix, automatically</span>
<span class="sd">        attempts several fixes before giving up.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">theta</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All theta values must be finite!&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All y values must be finite!&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fit_amp</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">kernel</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">fit_mean</span><span class="o">=</span><span class="n">fit_mean</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                   <span class="n">white_noise</span><span class="o">=</span><span class="n">white_noise</span><span class="p">,</span> <span class="n">fit_white_noise</span><span class="o">=</span><span class="n">fit_white_noise</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hyperparameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">hyperparameters</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hyperparameters:&quot;</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All hyperparameter values must be finite!&quot;</span><span class="p">)</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">hyperparameters</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;configure_gp error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">gp</span></div>



<div class="viewcode-block" id="optimize_gp">
<a class="viewcode-back" href="../../alabi.html#alabi.gp_utils.optimize_gp">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_gp</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">gp_hyper_prior</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span> <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="n">regularize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">lengthscale_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimize Gaussian Process hyperparameters by maximizing marginal likelihood.</span>
<span class="sd">    </span>
<span class="sd">    Performs hyperparameter optimization for a Gaussian Process using scipy&#39;s</span>
<span class="sd">    minimize function. Supports multiple optimization restarts and automatically</span>
<span class="sd">    selects the result with highest marginal likelihood.</span>
<span class="sd">    </span>
<span class="sd">    :param gp: (*george.GP*)</span>
<span class="sd">        Configured Gaussian Process object. Should be computed with training data.</span>
<span class="sd">        </span>
<span class="sd">    :param _theta: (*array-like, shape (n_samples, n_features)*)</span>
<span class="sd">        Training input locations (parameters). Will be squeezed if 1D.</span>
<span class="sd">        </span>
<span class="sd">    :param _y: (*array-like, shape (n_samples,)*)</span>
<span class="sd">        Training target values (function evaluations). Will be squeezed if 1D.</span>
<span class="sd">        </span>
<span class="sd">    :param gp_hyper_prior: (*callable*)</span>
<span class="sd">        Prior function for hyperparameters. Should return log-probability density</span>
<span class="sd">        for given hyperparameter vector. Used to constrain optimization.</span>
<span class="sd">        </span>
<span class="sd">    :param p0: (*array-like, shape (n_restarts, n_hyperparams) or (n_hyperparams,)*)</span>
<span class="sd">        Initial guesses for hyperparameter optimization. If 2D, performs multiple</span>
<span class="sd">        restarts with different initializations.</span>
<span class="sd">        </span>
<span class="sd">    :param bounds: (*list of tuples, optional, default=None*)</span>
<span class="sd">        Bounds for hyperparameter optimization as [(min, max), ...]. Only used</span>
<span class="sd">        for methods that support bounds (e.g., &#39;l-bfgs-b&#39;).</span>
<span class="sd">        </span>
<span class="sd">    :param method: (*str, optional, default=&quot;l-bfgs-b&quot;*)</span>
<span class="sd">        Scipy optimization method. Supported methods:</span>
<span class="sd">        </span>
<span class="sd">        - &#39;l-bfgs-b&#39;: L-BFGS-B with bounds support (default)</span>
<span class="sd">        - &#39;newton-cg&#39;: Newton-CG with gradients</span>
<span class="sd">        - &#39;bfgs&#39;: BFGS (no bounds support)</span>
<span class="sd">        - &#39;powell&#39;: Powell method (derivative-free)</span>
<span class="sd">        </span>
<span class="sd">    :param optimizer_kwargs: (*dict, optional, default=None*)</span>
<span class="sd">        Additional keyword arguments passed to scipy.optimize.minimize.</span>
<span class="sd">        If None, uses method-specific defaults optimized for GP optimization.</span>
<span class="sd">        </span>
<span class="sd">    :param max_iter: (*int, optional, default=50*)</span>
<span class="sd">        Maximum number of iterations for optimization. Used as default in</span>
<span class="sd">        optimizer_kwargs if not specified.</span>
<span class="sd">        </span>
<span class="sd">    :param regularize: (*bool, optional, default=False*)</span>
<span class="sd">        Whether to apply Hvarfner dimensionality-scaled regularization (Equation 4</span>
<span class="sd">        from &quot;Vanilla Bayesian Optimization Performs Great in High Dimensions&quot;).</span>
<span class="sd">        When True, lengthscale priors are scaled as LogNormal(μ_0 + log(√d), σ_0).</span>
<span class="sd">        </span>
<span class="sd">    :param mu_0: (*float, optional, default=0.0*)</span>
<span class="sd">        Base location parameter for Hvarfner regularization. Only used if regularize=True.</span>
<span class="sd">        </span>
<span class="sd">    :param sigma_0: (*float, optional, default=1.0*)</span>
<span class="sd">        Scale parameter for Hvarfner regularization. Only used if regularize=True.</span>
<span class="sd">        </span>
<span class="sd">    :param lengthscale_indices: (*list of int, optional, default=None*)</span>
<span class="sd">        Indices in the parameter vector corresponding to lengthscale parameters.</span>
<span class="sd">        Only used if regularize=True. If None, attempts to infer from kernel.</span>
<span class="sd">        </span>
<span class="sd">    :returns: *george.GP or None*</span>
<span class="sd">        GP object with optimized hyperparameters, or None if optimization failed.</span>
<span class="sd">        The returned GP is recomputed with optimal hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">RuntimeWarning</span><span class="p">)</span>
    
    <span class="c1"># Collapse arrays if 1D</span>
    <span class="n">_theta</span> <span class="o">=</span> <span class="n">_theta</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">_y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    
    <span class="c1"># Get dimensionality for regularization</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">_theta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="c1"># initial hyperparameters</span>
    <span class="n">init_hp</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
    <span class="n">nhparam</span> <span class="o">=</span> <span class="n">init_hp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Infer lengthscale indices if regularization is enabled</span>
    <span class="k">if</span> <span class="n">regularize</span> <span class="ow">and</span> <span class="n">lengthscale_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_names</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">()</span>
        <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">other_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_names</span><span class="p">):</span>
            <span class="c1"># Common patterns for lengthscale parameters in george kernels</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">pattern</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;metric:log_m&#39;</span><span class="p">]):</span>
                <span class="n">lengthscale_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">other_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengthscale_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Could not infer lengthscale indices from kernel. &quot;</span>
                        <span class="s2">&quot;Regularization gradient will not be applied. Please specify &quot;</span>
                        <span class="s2">&quot;lengthscale_indices explicitly.&quot;</span><span class="p">)</span>
        
    <span class="n">valid_methods</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">,</span> <span class="s2">&quot;bfgs&quot;</span><span class="p">,</span> <span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span> <span class="s2">&quot;powell&quot;</span><span class="p">,</span> <span class="s2">&quot;nelder-mead&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_methods</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> not in valid methods </span><span class="si">{</span><span class="n">valid_methods</span><span class="si">}</span><span class="s2">. Using &#39;l-bfgs-b&#39; optimizer instead.&quot;</span><span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;l-bfgs-b&quot;</span>
        
    <span class="c1"># methods without bounds arg </span>
    <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bfgs&quot;</span><span class="p">]:</span>
        <span class="n">bounds</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="c1"># configure objective function </span>
    <span class="k">if</span> <span class="n">regularize</span><span class="p">:</span>
        <span class="n">obj_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">gp_hyper_prior</span><span class="p">)</span> <span class="o">+</span> <span class="n">regularization_term</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lengthscale_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">obj_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">gp_hyper_prior</span><span class="p">)</span>

    <span class="c1"># configure gradient function</span>
    <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">,</span> <span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">regularize</span><span class="p">:</span>
            <span class="c1"># Use custom gradient function that includes regularization</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">_grad_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_y</span><span class="p">)</span> <span class="o">+</span> <span class="n">regularization_gradient</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lengthscale_indices</span><span class="p">,</span> <span class="n">amp_0</span><span class="o">=</span><span class="n">amp_0</span><span class="p">,</span> <span class="n">mu_0</span><span class="o">=</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">sigma_0</span><span class="o">=</span><span class="n">sigma_0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">_grad_nll</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jac</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="c1"># # Set improved default optimizer_kwargs for faster convergence</span>
    <span class="c1"># if optimizer_kwargs is None:</span>
    <span class="c1">#     default_optimizer_kwargs = {</span>
    <span class="c1">#         &#39;newton-cg&#39;: {&#39;maxiter&#39;: 200},</span>
    <span class="c1">#         &#39;bfgs&#39;: {&#39;maxiter&#39;: 200},</span>
    <span class="c1">#         &#39;l-bfgs-b&#39;: {&#39;maxiter&#39;: 200, &#39;factr&#39;: 1e12},</span>
    <span class="c1">#         &#39;powell&#39;: {&#39;maxiter&#39;: 200},</span>
    <span class="c1">#         &#39;nelder-mead&#39;: {&#39;maxiter&#39;: 200},</span>
    <span class="c1">#     }</span>
    <span class="c1">#     optimizer_kwargs = default_optimizer_kwargs.get(method.lower(), {})</span>
    
    <span class="n">nopt</span> <span class="o">=</span> <span class="n">p0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">p0</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">nopt</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Run the optimization routine nopt times</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">mll</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">x0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p0</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">obj_fn</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;opt iterations:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">nit</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">success</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">gp_hyper_prior</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)):</span>
                    <span class="c1"># Compute marginal log likelihood for this set of kernel hyperparameters</span>
                    <span class="n">test_gp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">gp</span><span class="p">)</span>
                    <span class="n">test_gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">test_gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
                    <span class="n">current_mll</span> <span class="o">=</span> <span class="n">test_gp</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    
                    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">mll</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_mll</span><span class="p">)</span>
                            
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: GP hyperparameter optimization restart </span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2"> failed. Solution failed prior bounds.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
                    <span class="n">mll</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
                    
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: GP hyperparameter optimization restart </span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="s2"> failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
                <span class="n">mll</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="c1"># Pick result with largest marginal log likelihood</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mll</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">max</span><span class="p">(</span><span class="n">mll</span><span class="p">)</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">mll</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>   
                <span class="n">gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: Failed to set best hyperparameters. Using initial values.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: All hyperparameter optimizations failed. Using initial values.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
            
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Single optimization run</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">obj_fn</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">success</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">gp_hyper_prior</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)):</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: GP hyperparameter optimization failed. Using initial values.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
                <span class="n">gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
                
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warning: GP hyperparameter optimization failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Using initial values.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">init_hp</span><span class="p">)</span>
            <span class="n">gp</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">gp</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">weighted_mse_by_probability</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">weight_method</span><span class="o">=</span><span class="s1">&#39;exponential&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute weighted MSE where higher probability values get larger weights.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    y_true : array-like</span>
<span class="sd">        True log likelihood values (more negative = lower probability)</span>
<span class="sd">    y_pred : array-like  </span>
<span class="sd">        Predicted log likelihood values</span>
<span class="sd">    weight_method : str</span>
<span class="sd">        Method for computing weights from log likelihood:</span>
<span class="sd">        - &#39;exponential&#39;: w = exp(y_true / temperature) </span>
<span class="sd">        - &#39;linear&#39;: w = y_true - min(y_true) + epsilon</span>
<span class="sd">        - &#39;softmax&#39;: w = softmax(y_true / temperature)</span>
<span class="sd">        - &#39;rank&#39;: w based on rank order of y_true</span>
<span class="sd">    temperature : float</span>
<span class="sd">        Temperature parameter for exponential/softmax weighting</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    weighted_mse : float</span>
<span class="sd">        Weighted mean squared error</span>
<span class="sd">    weights : array</span>
<span class="sd">        The weights used for each point</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">weight_method</span> <span class="o">==</span> <span class="s1">&#39;exponential&#39;</span><span class="p">:</span>
        <span class="c1"># Higher log likelihood = higher probability = larger weight</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_true</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">weight_method</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="c1"># Shift to make all weights positive, higher y_true gets higher weight</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
        
    <span class="k">elif</span> <span class="n">weight_method</span> <span class="o">==</span> <span class="s1">&#39;softmax&#39;</span><span class="p">:</span>
        <span class="c1"># Softmax weighting - probabilistic approach</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_true</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>  <span class="c1"># Normalize to maintain scale</span>
        
    <span class="k">elif</span> <span class="n">weight_method</span> <span class="o">==</span> <span class="s1">&#39;rank&#39;</span><span class="p">:</span>
        <span class="c1"># Rank-based weighting - highest log likelihood gets highest weight</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span>  <span class="c1"># Get ranks (0 to n-1)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">ranks</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Make weights 1 to n</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown weight_method: </span><span class="si">{</span><span class="n">weight_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Normalize weights to have mean = 1 (maintains MSE scale)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="c1"># Compute weighted MSE</span>
    <span class="n">mse_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">weighted_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">mse_values</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">weighted_mse</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_candidate_worker</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Worker function for parallel evaluation of a single hyperparameter candidate.</span>
<span class="sd">    </span>
<span class="sd">    :param args: tuple containing (cand_idx, hyperparams, gp, theta, y,  </span>
<span class="sd">                                   k_folds, scoring, weighted_mse_method, weighted_mse_factor)</span>
<span class="sd">    :returns: tuple (cand_idx, fold_scores, success_flag)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">r2_score</span>

    <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span> <span class="o">=</span> <span class="n">args</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Check for basic hyperparameter validity</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Invalid hyperparameters (NaN/Inf)&quot;</span><span class="p">)</span>
        
        <span class="c1"># Set up cross-validation</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">fold_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">fold_errors</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Track errors from each fold</span>
        <span class="n">fold_idx</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># Perform k-fold cross validation</span>
        <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">_theta</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Split data</span>
                <span class="n">_theta_train</span><span class="p">,</span> <span class="n">_theta_val</span> <span class="o">=</span> <span class="n">_theta</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">_theta</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
                <span class="n">_y_train</span><span class="p">,</span> <span class="n">_y_val</span> <span class="o">=</span> <span class="n">_y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">_y</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
                
                <span class="c1"># Check training data validity</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_y_train</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_y_train</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training targets contain NaN/Inf values&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_theta_train</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_theta_train</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training inputs contain NaN/Inf values&quot;</span><span class="p">)</span>
                
                <span class="c1"># Ensure proper shapes for george GP</span>
                <span class="n">_theta_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">_theta_train</span><span class="p">)</span>
                <span class="n">_theta_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">_theta_val</span><span class="p">)</span>
                
                <span class="c1"># Handle case where we have single samples</span>
                <span class="k">if</span> <span class="n">_theta_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">_theta_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">_theta_train</span> <span class="o">=</span> <span class="n">_theta_train</span><span class="o">.</span><span class="n">T</span>
                <span class="k">if</span> <span class="n">_theta_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">_theta_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">_theta_val</span> <span class="o">=</span> <span class="n">_theta_val</span><span class="o">.</span><span class="n">T</span>
                
                <span class="c1"># Create and configure GP for this fold</span>
                <span class="n">gp_fold</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">gp</span><span class="p">)</span>
                
                <span class="c1"># Check for invalid hyperparameters before setting</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid hyperparameters contain NaN/Inf: </span><span class="si">{</span><span class="n">hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="n">gp_fold</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)</span>
                <span class="n">gp_fold</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta_train</span><span class="p">)</span>
                
                <span class="c1"># Check if GP computation was successful</span>
                <span class="n">current_hp</span> <span class="o">=</span> <span class="n">gp_fold</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">current_hp</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">current_hp</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP parameters became invalid after compute: </span><span class="si">{</span><span class="n">current_hp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Check GP log-likelihood for numerical stability</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">log_like</span> <span class="o">=</span> <span class="n">gp_fold</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">_y_train</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">log_like</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">log_like</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP log-likelihood is invalid: </span><span class="si">{</span><span class="n">log_like</span><span class="si">}</span><span class="s2"> with hyperparams: </span><span class="si">{</span><span class="n">hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to compute GP log-likelihood: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2"> with hyperparams: </span><span class="si">{</span><span class="n">hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Make predictions on validation set</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">_y_pred</span> <span class="o">=</span> <span class="n">gp_fold</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">_y_train</span><span class="p">,</span> <span class="n">_theta_val</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP prediction failed: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2"> with hyperparams: </span><span class="si">{</span><span class="n">hyperparams</span><span class="si">}</span><span class="s2"> &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;train_shape: </span><span class="si">{</span><span class="n">_theta_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, val_shape: </span><span class="si">{</span><span class="n">_theta_val</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Check for NaN or infinite values</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP predictions are empty (no predictions returned)&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)):</span>
                    <span class="n">mean_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
                    <span class="n">std_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanstd</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP predictions contain NaN or Inf values: _y_pred stats: &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;mean=</span><span class="si">{</span><span class="n">mean_val</span><span class="si">}</span><span class="s2">, std=</span><span class="si">{</span><span class="n">std_val</span><span class="si">}</span><span class="s2">, &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;n_nan=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">))</span><span class="si">}</span><span class="s2">, n_inf=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_y_pred</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_y_val</span><span class="p">))</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">_y_val</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation targets contain NaN or Inf values&quot;</span><span class="p">)</span>
                
                <span class="c1"># Compute error using unscaled y values </span>
                <span class="n">y_val</span> <span class="o">=</span> <span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_y_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">_y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                
                <span class="c1"># Calculate fold score</span>
                <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s1">&#39;mse&#39;</span><span class="p">:</span>
                    <span class="n">fold_score</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s1">&#39;mae&#39;</span><span class="p">:</span>
                    <span class="n">fold_score</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s1">&#39;r2&#39;</span><span class="p">:</span>
                    <span class="n">fold_score</span> <span class="o">=</span> <span class="o">-</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># Negative because we minimize</span>
                <span class="k">elif</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s1">&#39;weighted_mse&#39;</span><span class="p">:</span>
                    <span class="n">fold_score</span> <span class="o">=</span> <span class="n">weighted_mse_by_probability</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> 
                                                           <span class="n">weight_method</span><span class="o">=</span><span class="n">weighted_mse_method</span><span class="p">,</span>
                                                           <span class="n">temperature</span><span class="o">=</span><span class="n">weighted_mse_factor</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported scoring method: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="n">fold_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fold_score</span><span class="p">)</span>
                
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">fold_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
                <span class="n">fold_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fold </span><span class="si">{</span><span class="n">fold_idx</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="n">fold_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># If all folds failed, return error message with details</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">)):</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;All folds failed. First error: </span><span class="si">{</span><span class="n">fold_errors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">fold_errors</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">error_msg</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="s2">&quot;success&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>


<div class="viewcode-block" id="optimize_gp_kfold_cv">
<a class="viewcode-back" href="../../alabi.html#alabi.gp_utils.optimize_gp_kfold_cv">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_gp_kfold_cv</span><span class="p">(</span><span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">hyperparameter_candidates</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span>
                         <span class="n">k_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                         <span class="n">stage2_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stage2_width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                         <span class="n">stage3_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stage3_width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                         <span class="n">weighted_mse_method</span><span class="o">=</span><span class="s2">&quot;exponential&quot;</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimize Gaussian Process hyperparameters using k-fold cross-validation.</span>
<span class="sd">    </span>
<span class="sd">    This function evaluates different hyperparameter configurations using k-fold </span>
<span class="sd">    cross-validation to select the configuration that generalizes best to unseen data.</span>
<span class="sd">    This approach helps prevent overfitting compared to standard marginal likelihood</span>
<span class="sd">    maximization.</span>
<span class="sd">    </span>
<span class="sd">    :param gp: (*george.GP*)</span>
<span class="sd">        Configured Gaussian Process object template. Will be copied for each CV fold.</span>
<span class="sd">        </span>
<span class="sd">    :param _theta: (*array-like, shape (n_samples, n_features)*)</span>
<span class="sd">        Training input locations (parameters). Must have at least k_folds samples.</span>
<span class="sd">        </span>
<span class="sd">    :param _y: (*array-like, shape (n_samples,)*)</span>
<span class="sd">        Training target values (function evaluations). Must match _theta length.</span>
<span class="sd">        </span>
<span class="sd">    :param gp_hyper_prior: (*callable*)</span>
<span class="sd">        Prior function for hyperparameters. Should return log-probability density</span>
<span class="sd">        for given hyperparameter vector. Used to constrain search space.</span>
<span class="sd">        </span>
<span class="sd">    :param hyperparameter_candidates: (*array-like, shape (n_candidates, n_hyperparams)*)</span>
<span class="sd">        Array of hyperparameter vectors to evaluate via cross-validation.</span>
<span class="sd">        Each row represents one hyperparameter configuration to test.</span>
<span class="sd">        </span>
<span class="sd">    :param k_folds: (*int, optional, default=5*)</span>
<span class="sd">        Number of folds for cross-validation. Must be &gt;= 2 and &lt;= n_samples.</span>
<span class="sd">        Common choices: 5 or 10 for good bias-variance tradeoff.</span>
<span class="sd">        </span>
<span class="sd">    :param scoring: (*str, optional, default=&#39;mse&#39;*)</span>
<span class="sd">        Scoring metric for cross-validation. Supported options:</span>
<span class="sd">        </span>
<span class="sd">        - &#39;mse&#39;: Mean Squared Error (lower is better)</span>
<span class="sd">        - &#39;mae&#39;: Mean Absolute Error (lower is better)  </span>
<span class="sd">        - &#39;r2&#39;: R-squared coefficient (higher is better)</span>
<span class="sd">        - &#39;weighted_mse&#39;: Weighted MSE giving higher weight to high-probability regions</span>
<span class="sd">        </span>
<span class="sd">    :param pool: (*multiprocessing.Pool, optional, default=None*)</span>
<span class="sd">        Multiprocessing pool for parallel evaluation of hyperparameter candidates.</span>
<span class="sd">        If None, evaluation runs sequentially. If provided, candidates are </span>
<span class="sd">        evaluated in parallel using the pool&#39;s worker processes.</span>
<span class="sd">        </span>
<span class="sd">    :param stage2_candidates: (*int, optional, default=None*)</span>
<span class="sd">        Number of candidates for stage 2 grid search. If None, uses </span>
<span class="sd">        len(hyperparameter_candidates) // 2.</span>
<span class="sd">        </span>
<span class="sd">    :param stage2_width: (*float, optional, default=0.5*)</span>
<span class="sd">        Width factor for stage 2 search around best parameters.</span>
<span class="sd">        Smaller values = tighter search, larger values = wider search.</span>
<span class="sd">        </span>
<span class="sd">    :param stage3_candidates: (*int, optional, default=None*)</span>
<span class="sd">        Number of candidates for stage 3 ultra-fine search. If None, uses </span>
<span class="sd">        max(stage2_candidates // 2, 3).</span>
<span class="sd">        </span>
<span class="sd">    :param stage3_width: (*float, optional, default=0.2*)</span>
<span class="sd">        Width factor for stage 3 search around stage 2 best parameters.</span>
<span class="sd">        Should be smaller than stage2_width for finer refinement.</span>
<span class="sd">        </span>
<span class="sd">    :param weighted_mse_method: (*str, optional, default=&#39;exponential&#39;*)</span>
<span class="sd">        Weighting method for weighted_mse scoring. Options:</span>
<span class="sd">        - &#39;exponential&#39;: w = exp(y_true / temperature)</span>
<span class="sd">        - &#39;linear&#39;: w = y_true - min(y_true) + epsilon  </span>
<span class="sd">        - &#39;softmax&#39;: w = softmax(y_true / temperature)</span>
<span class="sd">        - &#39;rank&#39;: w based on rank order of y_true</span>
<span class="sd">        </span>
<span class="sd">    :param weighted_mse_factor: (*float, optional, default=1.0*)</span>
<span class="sd">        Temperature parameter for exponential/softmax weighting.</span>
<span class="sd">        Lower values emphasize high-probability regions more strongly.</span>
<span class="sd">        </span>
<span class="sd">    :returns:</span>
<span class="sd">        - **best_gp** (*george.GP*) -- GP with optimal hyperparameters set</span>
<span class="sd">        - **best_hyperparams** (*array*) -- Best hyperparameter vector</span>
<span class="sd">        - **cv_results** (*dict*) -- Detailed CV results with scores and statistics</span>
<span class="sd">        </span>
<span class="sd">    :raises:</span>
<span class="sd">        - **ValueError** -- If insufficient data, invalid parameters, or no valid hyperparameters</span>
<span class="sd">        - **RuntimeError** -- If all hyperparameter evaluations fail</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">stage2_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">two_stage</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">two_stage</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">stage3_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">three_stage</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">three_stage</span> <span class="o">=</span> <span class="kc">False</span>
        
    <span class="c1"># Input validation</span>
    <span class="n">_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span>
    <span class="n">hyperparameter_candidates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">hyperparameter_candidates</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_theta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_theta</span> <span class="o">=</span> <span class="n">_theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="n">_y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_theta and _y must have same length, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_samples</span> <span class="o">&lt;</span> <span class="n">k_folds</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of samples (</span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2">) must be &gt;= k_folds (</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">k_folds</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;k_folds must be &gt;= 2, got </span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">hyperparameter_candidates</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">hyperparameter_candidates</span> <span class="o">=</span> <span class="n">hyperparameter_candidates</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="n">n_candidates</span><span class="p">,</span> <span class="n">n_hyperparams</span> <span class="o">=</span> <span class="n">hyperparameter_candidates</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="c1"># Set up cross-validation</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    
    <span class="c1"># Storage for results</span>
    <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_candidates</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">))</span>
    <span class="n">cv_scores</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Initialize with worst possible score</span>
    
    <span class="c1"># Two-stage optimization: explore then exploit</span>
    <span class="k">if</span> <span class="n">two_stage</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== STAGE 1: EXPLORATION (Random Search) ===&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating </span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2"> hyperparameter candidates using </span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">-fold CV...&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Parallel evaluation using multiprocessing pool</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using multiprocessing pool with </span><span class="si">{</span><span class="n">pool</span><span class="o">.</span><span class="n">_processes</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>
        
        <span class="c1"># Prepare arguments for all candidates</span>
        <span class="n">worker_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hyperparameter_candidates</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> 
                   <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
            <span class="n">worker_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        
        <span class="c1"># Evaluate all candidates in parallel with progress bar</span>
        <span class="n">failed_candidate_errors</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Track errors for diagnostics</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">worker_args</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating candidates&quot;</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;candidate&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="c1"># Use pool.imap for progress tracking</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="n">_evaluate_candidate_worker</span><span class="p">,</span> <span class="n">worker_args</span><span class="p">):</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Process results</span>
        <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Store fold scores</span>
                <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                    <span class="n">cv_scores</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                
                <span class="c1"># Calculate and print statistics</span>
                <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                    <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                    <span class="n">failed_candidate_errors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparameter_candidates</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">],</span> <span class="s2">&quot;All folds failed&quot;</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Candidate failed completely</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;hyperparams&quot;</span> <span class="ow">in</span> <span class="n">status</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">or</span> <span class="s2">&quot;prior&quot;</span> <span class="ow">in</span> <span class="n">status</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Hyperparams: </span><span class="si">{</span><span class="n">hyperparameter_candidates</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">failed_candidate_errors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparameter_candidates</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">],</span> <span class="n">status</span><span class="p">))</span>
    
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Sequential evaluation using the worker function</span>
        <span class="n">failed_candidate_errors</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Track errors for diagnostics</span>
        <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hyperparameter_candidates</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">_evaluate_candidate_worker</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">cand_idx_result</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">results</span>
            
            <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Store fold scores</span>
                <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                    <span class="n">cv_scores</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                
                <span class="c1"># Calculate and print statistics</span>
                <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                    <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                    <span class="n">failed_candidate_errors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="s2">&quot;All folds failed&quot;</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Candidate failed completely</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;hyperparams&quot;</span> <span class="ow">in</span> <span class="n">status</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">or</span> <span class="s2">&quot;prior&quot;</span> <span class="ow">in</span> <span class="n">status</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Hyperparams: </span><span class="si">{</span><span class="n">hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">failed_candidate_errors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">status</span><span class="p">))</span>
    
    <span class="c1"># Find best hyperparameters</span>
    <span class="c1"># Calculate mean CV score for each candidate (only over successful folds)</span>
    <span class="n">mean_cv_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">std_cv_scores</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">cand_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_candidates</span><span class="p">):</span>
        <span class="n">fold_scores</span> <span class="o">=</span> <span class="n">cv_scores</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">valid_scores</span> <span class="o">=</span> <span class="n">fold_scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">)]</span>
        
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">mean_cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
            <span class="n">std_cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mean_cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            <span class="n">std_cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    
    <span class="n">mean_cv_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mean_cv_scores</span><span class="p">)</span>
    <span class="n">std_cv_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">std_cv_scores</span><span class="p">)</span>
    
    <span class="c1"># Find best candidate (lowest score for most metrics, highest for R²)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">mean_cv_scores</span><span class="p">)):</span>
        <span class="n">best_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mean_cv_scores</span><span class="p">)</span>
        <span class="n">best_hyperparams</span> <span class="o">=</span> <span class="n">hyperparameter_candidates</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="n">mean_cv_scores</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
        <span class="n">best_score_std</span> <span class="o">=</span> <span class="n">std_cv_scores</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Stage 1 best hyperparameters (candidate </span><span class="si">{</span><span class="n">best_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">best_hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stage 1 best CV </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">best_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">best_score_std</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Store stage 1 results</span>
        <span class="n">stage1_results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;best_score&#39;</span><span class="p">:</span> <span class="n">best_score</span><span class="p">,</span>
            <span class="s1">&#39;best_score_std&#39;</span><span class="p">:</span> <span class="n">best_score_std</span><span class="p">,</span>
            <span class="s1">&#39;best_hyperparams&#39;</span><span class="p">:</span> <span class="n">best_hyperparams</span><span class="p">,</span>
            <span class="s1">&#39;best_candidate_idx&#39;</span><span class="p">:</span> <span class="n">best_idx</span><span class="p">,</span>
            <span class="s1">&#39;all_scores&#39;</span><span class="p">:</span> <span class="n">mean_cv_scores</span><span class="p">,</span>
            <span class="s1">&#39;all_scores_std&#39;</span><span class="p">:</span> <span class="n">std_cv_scores</span><span class="p">,</span>
            <span class="s1">&#39;cv_scores_matrix&#39;</span><span class="p">:</span> <span class="n">cv_scores</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># All candidates failed</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ERROR: All hyperparameter candidates failed CV evaluation&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Diagnostics:&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Number of candidates: </span><span class="si">{</span><span class="n">n_candidates</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Number of folds: </span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Training data shape: </span><span class="si">{</span><span class="n">_theta</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Training targets shape: </span><span class="si">{</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Training targets stats: min=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, mean=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># Report errors from failed candidates</span>
            <span class="k">if</span> <span class="s1">&#39;failed_candidate_errors&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">failed_candidate_errors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Showing first 5 candidate errors:&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hp</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">failed_candidate_errors</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Hyperparams: </span><span class="si">{</span><span class="n">hp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Possible causes:&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Hyperparameters produce numerically unstable GP&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Training data has issues (NaN, inf, extreme values)&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - GP kernel is incompatible with data&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Insufficient training data for cross-validation&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="c1"># === STAGE 2: EXPLOITATION (Grid Search around best) ===</span>
    <span class="k">if</span> <span class="n">two_stage</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== STAGE 2: EXPLOITATION (Grid Search around best) ===&quot;</span><span class="p">)</span>
            
            <span class="c1"># Determine number of stage 2 candidates</span>
            <span class="k">if</span> <span class="n">stage2_candidates</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">stage2_candidates</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_candidates</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
            
            <span class="c1"># Generate grid around best hyperparameters</span>
            <span class="n">stage2_hyperparam_candidates</span> <span class="o">=</span> <span class="n">_generate_stage2_candidates</span><span class="p">(</span>
                <span class="n">best_hyperparams</span><span class="p">,</span> <span class="n">stage2_candidates</span><span class="p">,</span> <span class="n">stage2_width</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="n">gp</span>
            <span class="p">)</span>
            
            <span class="c1"># Evaluate stage 2 candidates</span>
            <span class="n">n_candidates_s2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stage2_hyperparam_candidates</span><span class="p">)</span>
            <span class="n">cv_scores_s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_candidates_s2</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">))</span>
            <span class="n">cv_scores_s2</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating </span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2"> stage 2 candidates using </span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">-fold CV...&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Parallel evaluation for stage 2</span>
                <span class="n">worker_args_s2</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stage2_hyperparam_candidates</span><span class="p">):</span>
                    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span>
                           <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
                    <span class="n">worker_args_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                
                <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">worker_args_s2</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Stage 2 candidates&quot;</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;candidate&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
                    <span class="n">results_s2</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="n">_evaluate_candidate_worker</span><span class="p">,</span> <span class="n">worker_args_s2</span><span class="p">):</span>
                        <span class="n">results_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                        <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="c1"># Process stage 2 results</span>
                <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">results_s2</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                            <span class="n">cv_scores_s2</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                        
                        <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                            <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Sequential evaluation for stage 2</span>
                <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stage2_hyperparam_candidates</span><span class="p">):</span>
                    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
                    <span class="n">results_s2</span> <span class="o">=</span> <span class="n">_evaluate_candidate_worker</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                    <span class="n">cand_idx_result</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">results_s2</span>
                    
                    <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                            <span class="n">cv_scores_s2</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                        
                        <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                            <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 2 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># Find best from stage 2</span>
            <span class="n">mean_cv_scores_s2</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">std_cv_scores_s2</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="k">for</span> <span class="n">cand_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_candidates_s2</span><span class="p">):</span>
                <span class="n">fold_scores</span> <span class="o">=</span> <span class="n">cv_scores_s2</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">valid_scores</span> <span class="o">=</span> <span class="n">fold_scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">)]</span>
                
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">mean_cv_scores_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
                    <span class="n">std_cv_scores_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mean_cv_scores_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
                    <span class="n">std_cv_scores_s2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            
            <span class="n">mean_cv_scores_s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mean_cv_scores_s2</span><span class="p">)</span>
            <span class="n">std_cv_scores_s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">std_cv_scores_s2</span><span class="p">)</span>
            
            <span class="c1"># Compare stage 2 best with stage 1 best</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">mean_cv_scores_s2</span><span class="p">)):</span>
                <span class="n">best_idx_s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mean_cv_scores_s2</span><span class="p">)</span>
                <span class="n">best_hyperparams_s2</span> <span class="o">=</span> <span class="n">stage2_hyperparam_candidates</span><span class="p">[</span><span class="n">best_idx_s2</span><span class="p">]</span>
                <span class="n">best_score_s2</span> <span class="o">=</span> <span class="n">mean_cv_scores_s2</span><span class="p">[</span><span class="n">best_idx_s2</span><span class="p">]</span>
                <span class="n">best_score_std_s2</span> <span class="o">=</span> <span class="n">std_cv_scores_s2</span><span class="p">[</span><span class="n">best_idx_s2</span><span class="p">]</span>
                
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Stage 2 best hyperparameters (candidate </span><span class="si">{</span><span class="n">best_idx_s2</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">best_hyperparams_s2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stage 2 best CV </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">best_score_s2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">best_score_std_s2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Use stage 2 results if better</span>
                <span class="k">if</span> <span class="n">best_score_s2</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">:</span>  <span class="c1"># Assuming lower is better for most metrics</span>
                    <span class="n">best_hyperparams</span> <span class="o">=</span> <span class="n">best_hyperparams_s2</span>
                    <span class="n">best_score</span> <span class="o">=</span> <span class="n">best_score_s2</span>
                    <span class="n">best_score_std</span> <span class="o">=</span> <span class="n">best_score_std_s2</span>
                    <span class="n">best_idx</span> <span class="o">=</span> <span class="n">best_idx_s2</span>  <span class="c1"># Note: this is index within stage 2 candidates</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Stage 2 improved results!&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Stage 1 results remain best.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stage 2 failed - using Stage 1 results&quot;</span><span class="p">)</span>
    
    <span class="c1"># === STAGE 3: REFINEMENT (Ultra-fine search around stage 2 best) ===</span>
    <span class="k">if</span> <span class="n">three_stage</span> <span class="ow">and</span> <span class="n">two_stage</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== STAGE 3: REFINEMENT (Ultra-fine search around best) ===&quot;</span><span class="p">)</span>
        
        <span class="c1"># Determine number of stage 3 candidates</span>
        <span class="k">if</span> <span class="n">stage3_candidates</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">stage3_candidates</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="n">stage2_candidates</span> <span class="k">if</span> <span class="n">stage2_candidates</span> <span class="k">else</span> <span class="n">n_candidates</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="c1"># Generate ultra-fine grid around current best hyperparameters</span>
        <span class="n">stage3_hyperparam_candidates</span> <span class="o">=</span> <span class="n">_generate_stage3_candidates</span><span class="p">(</span>
            <span class="n">best_hyperparams</span><span class="p">,</span> <span class="n">stage3_candidates</span><span class="p">,</span> <span class="n">stage3_width</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="n">gp</span>
        <span class="p">)</span>
        
        <span class="c1"># Evaluate stage 3 candidates</span>
        <span class="n">n_candidates_s3</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stage3_hyperparam_candidates</span><span class="p">)</span>
        <span class="n">cv_scores_s3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_candidates_s3</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">))</span>
        <span class="n">cv_scores_s3</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating </span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2"> stage 3 candidates using </span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2">-fold CV...&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Parallel evaluation for stage 3</span>
            <span class="n">worker_args_s3</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stage3_hyperparam_candidates</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span>
                       <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
                <span class="n">worker_args_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            
            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">worker_args_s3</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Stage 3 candidates&quot;</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;candidate&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
                <span class="n">results_s3</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">pool</span><span class="o">.</span><span class="n">imap</span><span class="p">(</span><span class="n">_evaluate_candidate_worker</span><span class="p">,</span> <span class="n">worker_args_s3</span><span class="p">):</span>
                    <span class="n">results_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Process stage 3 results</span>
            <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">results_s3</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                        <span class="n">cv_scores_s3</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                    
                    <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                        <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Sequential evaluation for stage 3</span>
            <span class="k">for</span> <span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stage3_hyperparam_candidates</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">_theta</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">y_scaler</span><span class="p">,</span> <span class="n">k_folds</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">weighted_mse_method</span><span class="p">,</span> <span class="n">weighted_mse_factor</span><span class="p">)</span>
                <span class="n">results_s3</span> <span class="o">=</span> <span class="n">_evaluate_candidate_worker</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                <span class="n">cand_idx_result</span><span class="p">,</span> <span class="n">fold_scores</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">results_s3</span>
                
                <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span> <span class="ow">and</span> <span class="n">fold_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">):</span>
                        <span class="n">cv_scores_s3</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">fold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                    
                    <span class="n">valid_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">fold_scores</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                        <span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">±</span><span class="si">{</span><span class="n">std_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">k_folds</span><span class="si">}</span><span class="s2"> folds successful)&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: All folds failed&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stage 3 Candidate </span><span class="si">{</span><span class="n">cand_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_candidates_s3</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Find best from stage 3</span>
        <span class="n">mean_cv_scores_s3</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">std_cv_scores_s3</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">cand_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_candidates_s3</span><span class="p">):</span>
            <span class="n">fold_scores</span> <span class="o">=</span> <span class="n">cv_scores_s3</span><span class="p">[</span><span class="n">cand_idx</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">valid_scores</span> <span class="o">=</span> <span class="n">fold_scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">fold_scores</span><span class="p">)]</span>
            
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_cv_scores_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
                <span class="n">std_cv_scores_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">valid_scores</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mean_cv_scores_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
                <span class="n">std_cv_scores_s3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        
        <span class="n">mean_cv_scores_s3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mean_cv_scores_s3</span><span class="p">)</span>
        <span class="n">std_cv_scores_s3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">std_cv_scores_s3</span><span class="p">)</span>
        
        <span class="c1"># Compare stage 3 best with previous best</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">mean_cv_scores_s3</span><span class="p">)):</span>
            <span class="n">best_idx_s3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mean_cv_scores_s3</span><span class="p">)</span>
            <span class="n">best_hyperparams_s3</span> <span class="o">=</span> <span class="n">stage3_hyperparam_candidates</span><span class="p">[</span><span class="n">best_idx_s3</span><span class="p">]</span>
            <span class="n">best_score_s3</span> <span class="o">=</span> <span class="n">mean_cv_scores_s3</span><span class="p">[</span><span class="n">best_idx_s3</span><span class="p">]</span>
            <span class="n">best_score_std_s3</span> <span class="o">=</span> <span class="n">std_cv_scores_s3</span><span class="p">[</span><span class="n">best_idx_s3</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Stage 3 best hyperparameters (candidate </span><span class="si">{</span><span class="n">best_idx_s3</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">best_hyperparams_s3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stage 3 best CV </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">best_score_s3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">best_score_std_s3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># Use stage 3 results if better</span>
            <span class="k">if</span> <span class="n">best_score_s3</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">:</span>  <span class="c1"># Assuming lower is better for most metrics</span>
                <span class="n">best_hyperparams</span> <span class="o">=</span> <span class="n">best_hyperparams_s3</span>
                <span class="n">best_score</span> <span class="o">=</span> <span class="n">best_score_s3</span>
                <span class="n">best_score_std</span> <span class="o">=</span> <span class="n">best_score_std_s3</span>
                <span class="n">best_idx</span> <span class="o">=</span> <span class="n">best_idx_s3</span>  <span class="c1"># Note: this is index within stage 3 candidates</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Stage 3 improved results!&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Previous results remain best.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stage 3 failed - using previous results&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">three_stage</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">two_stage</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: three_stage=True requires two_stage=True. Ignoring three_stage option.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Configure final GP with best hyperparameters</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Setting best hyperparameters on GP...&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best hyperparams shape: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">best_hyperparams</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GP current params shape: </span><span class="si">{</span><span class="n">gp</span><span class="o">.</span><span class="n">get_parameter_vector</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">gp</span><span class="o">.</span><span class="n">set_parameter_vector</span><span class="p">(</span><span class="n">best_hyperparams</span><span class="p">)</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">_theta</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully configured GP with best hyperparameters&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to set best hyperparameters: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># raise RuntimeError(f&quot;Failed to set best hyperparameters: {str(e)}&quot;)</span>
        <span class="c1"># If optimization fails, return original GP without changes</span>
        <span class="k">return</span> <span class="n">gp</span>
    
    <span class="c1"># Compile detailed results</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;best_score&quot;</span><span class="p">:</span> <span class="n">best_score</span><span class="p">,</span>
        <span class="s2">&quot;best_score_std&quot;</span><span class="p">:</span> <span class="n">best_score_std</span><span class="p">,</span>
        <span class="s2">&quot;best_hyperparams&quot;</span><span class="p">:</span> <span class="n">best_hyperparams</span><span class="p">,</span>
        <span class="s2">&quot;best_candidate_idx&quot;</span><span class="p">:</span> <span class="n">best_idx</span><span class="p">,</span>
        <span class="s2">&quot;all_scores&quot;</span><span class="p">:</span> <span class="n">mean_cv_scores</span><span class="p">,</span>
        <span class="s2">&quot;all_scores_std&quot;</span><span class="p">:</span> <span class="n">std_cv_scores</span><span class="p">,</span>
        <span class="s2">&quot;cv_scores_matrix&quot;</span><span class="p">:</span> <span class="n">cv_scores</span><span class="p">,</span>
        <span class="s2">&quot;scoring_method&quot;</span><span class="p">:</span> <span class="n">scoring</span><span class="p">,</span>
        <span class="s2">&quot;n_folds&quot;</span><span class="p">:</span> <span class="n">k_folds</span><span class="p">,</span>
        <span class="s2">&quot;n_candidates&quot;</span><span class="p">:</span> <span class="n">n_candidates</span><span class="p">,</span>
        <span class="s2">&quot;two_stage&quot;</span><span class="p">:</span> <span class="n">two_stage</span><span class="p">,</span>
        <span class="s2">&quot;three_stage&quot;</span><span class="p">:</span> <span class="n">three_stage</span>
    <span class="p">}</span>
    
    <span class="c1"># Add stage-specific results if multi-stage was used</span>
    <span class="k">if</span> <span class="n">two_stage</span><span class="p">:</span>
        <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;stage1_results&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stage1_results</span>
        <span class="k">if</span> <span class="s2">&quot;mean_cv_scores_s2&quot;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
            <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;stage2_results&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;all_scores&quot;</span><span class="p">:</span> <span class="n">mean_cv_scores_s2</span><span class="p">,</span>
                <span class="s2">&quot;all_scores_std&quot;</span><span class="p">:</span> <span class="n">std_cv_scores_s2</span><span class="p">,</span>
                <span class="s2">&quot;cv_scores_matrix&quot;</span><span class="p">:</span> <span class="n">cv_scores_s2</span><span class="p">,</span>
                <span class="s2">&quot;candidates&quot;</span><span class="p">:</span> <span class="n">stage2_hyperparam_candidates</span>
            <span class="p">}</span>
        
        <span class="c1"># Add stage 3 results if three-stage was used</span>
        <span class="k">if</span> <span class="n">three_stage</span> <span class="ow">and</span> <span class="s2">&quot;mean_cv_scores_s3&quot;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
            <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;stage3_results&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;all_scores&quot;</span><span class="p">:</span> <span class="n">mean_cv_scores_s3</span><span class="p">,</span>
                <span class="s2">&quot;all_scores_std&quot;</span><span class="p">:</span> <span class="n">std_cv_scores_s3</span><span class="p">,</span>
                <span class="s2">&quot;cv_scores_matrix&quot;</span><span class="p">:</span> <span class="n">cv_scores_s3</span><span class="p">,</span>
                <span class="s2">&quot;candidates&quot;</span><span class="p">:</span> <span class="n">stage3_hyperparam_candidates</span>
            <span class="p">}</span>
            
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CV optimization completed. Best </span><span class="si">{</span><span class="n">scoring</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;best_score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;best_score_std&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">gp</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_generate_stage2_candidates</span><span class="p">(</span><span class="n">best_params</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate stage 2 candidates around best parameters from stage 1.</span>
<span class="sd">    </span>
<span class="sd">    :param best_params: Best hyperparameters from stage 1</span>
<span class="sd">    :param n_candidates: Number of candidates to generate</span>
<span class="sd">    :param width_factor: Width of search around best parameters</span>
<span class="sd">    :param gp: Optional GP object to infer parameter structure from kernel</span>
<span class="sd">    :returns: Array of stage 2 hyperparameter candidates</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">best_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">best_params</span><span class="p">)</span>
    <span class="n">n_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_params</span><span class="p">)</span>
    
    <span class="c1"># Infer lengthscale indices from GP kernel if available</span>
    <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">gp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_names</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">()</span>
        <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> 
                              <span class="k">if</span> <span class="s1">&#39;metric:log_m&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
    
    <span class="c1"># Check if we have uniform length scales</span>
    <span class="k">if</span> <span class="n">lengthscale_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengthscale_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Check if all lengthscale values are identical</span>
        <span class="n">length_scales</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="n">lengthscale_indices</span><span class="p">]</span>
        <span class="n">uniform_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">length_scales</span><span class="p">,</span> <span class="n">length_scales</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Fallback: assume indices 2+ are length scales if n_params &gt; 2</span>
        <span class="k">if</span> <span class="n">n_params</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">length_scales</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">uniform_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">length_scales</span><span class="p">,</span> <span class="n">length_scales</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_params</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">uniform_scales</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Generate candidates using normal distribution around best parameters</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Always include the best parameters as first candidate</span>
    <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    
    <span class="c1"># Generate remaining candidates</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_candidates</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">uniform_scales</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengthscale_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># For uniform scales: perturb each non-lengthscale parameter independently</span>
            <span class="c1"># and apply single perturbation to all lengthscales</span>
            <span class="n">candidate</span> <span class="o">=</span> <span class="n">best_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            
            <span class="c1"># Perturb non-lengthscale parameters independently</span>
            <span class="n">non_lengthscale_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_params</span><span class="p">)</span> <span class="k">if</span> <span class="n">j</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lengthscale_indices</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">non_lengthscale_indices</span><span class="p">:</span>
                <span class="n">candidate</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">)</span>
            
            <span class="c1"># Apply same perturbation to all length scales</span>
            <span class="n">lengthscale_perturbation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">lengthscale_indices</span><span class="p">:</span>
                <span class="n">candidate</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lengthscale_perturbation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For independent scales: perturb all parameters independently</span>
            <span class="n">perturbations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>
            <span class="n">candidate</span> <span class="o">=</span> <span class="n">best_params</span> <span class="o">+</span> <span class="n">perturbations</span>
        
        <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_generate_stage3_candidates</span><span class="p">(</span><span class="n">best_params</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate stage 3 candidates around best parameters from stage 2.</span>
<span class="sd">    Uses tighter search with smaller perturbations for ultra-fine optimization.</span>
<span class="sd">    </span>
<span class="sd">    :param best_params: Best hyperparameters from stage 2</span>
<span class="sd">    :param n_candidates: Number of candidates to generate</span>
<span class="sd">    :param width_factor: Width of search around best parameters (should be smaller than stage2)</span>
<span class="sd">    :param gp: Optional GP object to infer parameter structure from kernel</span>
<span class="sd">    :returns: Array of stage 3 hyperparameter candidates</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">best_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">best_params</span><span class="p">)</span>
    <span class="n">n_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_params</span><span class="p">)</span>
    
    <span class="c1"># Infer lengthscale indices from GP kernel if available</span>
    <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">gp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_names</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">()</span>
        <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> 
                              <span class="k">if</span> <span class="s1">&#39;metric:log_m&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
    
    <span class="c1"># Check if we have uniform length scales</span>
    <span class="k">if</span> <span class="n">lengthscale_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengthscale_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Check if all lengthscale values are identical</span>
        <span class="n">length_scales</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="n">lengthscale_indices</span><span class="p">]</span>
        <span class="n">uniform_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">length_scales</span><span class="p">,</span> <span class="n">length_scales</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Fallback: assume indices 2+ are length scales if n_params &gt; 2</span>
        <span class="k">if</span> <span class="n">n_params</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">length_scales</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">uniform_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">length_scales</span><span class="p">,</span> <span class="n">length_scales</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_params</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">uniform_scales</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">lengthscale_indices</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Generate candidates using tighter normal distribution around best parameters</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Always include the best parameters as first candidate</span>
    <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    
    <span class="c1"># Generate remaining candidates with smaller perturbations for fine-tuning</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_candidates</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">uniform_scales</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengthscale_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># For uniform scales: perturb each non-lengthscale parameter independently</span>
            <span class="c1"># and apply single perturbation to all lengthscales</span>
            <span class="n">candidate</span> <span class="o">=</span> <span class="n">best_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            
            <span class="c1"># Perturb non-lengthscale parameters independently</span>
            <span class="n">non_lengthscale_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_params</span><span class="p">)</span> <span class="k">if</span> <span class="n">j</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lengthscale_indices</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">non_lengthscale_indices</span><span class="p">:</span>
                <span class="n">candidate</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">)</span>
            
            <span class="c1"># Apply same perturbation to all length scales</span>
            <span class="n">lengthscale_perturbation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">lengthscale_indices</span><span class="p">:</span>
                <span class="n">candidate</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lengthscale_perturbation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For independent scales: perturb all parameters independently</span>
            <span class="n">perturbations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_factor</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>
            <span class="n">candidate</span> <span class="o">=</span> <span class="n">best_params</span> <span class="o">+</span> <span class="n">perturbations</span>
        
        <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span>
</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Jess Birky
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/notebook-outputs.js?v=9bb603c3"></script>
    </body>
</html>